{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"inference4j","text":"<p>Run AI models in Java. Three lines of code, zero setup.</p> <p>inference4j is an inference-only AI library for Java built on ONNX Runtime. It provides ergonomic, type-safe APIs for running model inference locally \u2014 no API keys, no network calls, no third-party services. Pass a <code>String</code>, <code>BufferedImage</code>, or <code>Path</code>, get Java objects back.</p>"},{"location":"#what-can-you-do-with-inference4j","title":"What can you do with inference4j?","text":"<p>Want to see it in action? Check out inference4j-showcase \u2014 a local demo app you can run to explore every capability the library provides.</p>"},{"location":"#sentiment-analysis","title":"Sentiment Analysis","text":"<pre><code>try (var classifier = DistilBertTextClassifier.builder().build()) {\n    System.out.println(classifier.classify(\"This movie was fantastic!\"));\n    // [TextClassification[label=POSITIVE, confidence=0.9998]]\n}\n</code></pre>"},{"location":"#text-embeddings-semantic-search","title":"Text Embeddings &amp; Semantic Search","text":"<pre><code>try (var embedder = SentenceTransformerEmbedder.builder()\n        .modelId(\"inference4j/all-MiniLM-L6-v2\").build()) {\n    float[] embedding = embedder.encode(\"Hello, world!\");\n}\n</code></pre>"},{"location":"#image-classification","title":"Image Classification","text":"<pre><code>try (var classifier = ResNetClassifier.builder().build()) {\n    List&lt;Classification&gt; results = classifier.classify(Path.of(\"cat.jpg\"));\n    // [Classification[label=tabby cat, confidence=0.87], ...]\n}\n</code></pre>"},{"location":"#object-detection","title":"Object Detection","text":"<pre><code>try (var detector = YoloV8Detector.builder().build()) {\n    List&lt;Detection&gt; detections = detector.detect(Path.of(\"street.jpg\"));\n    // [Detection[label=car, confidence=0.94, box=BoundingBox[...]], ...]\n}\n</code></pre>"},{"location":"#speech-to-text","title":"Speech-to-Text","text":"<pre><code>try (var recognizer = Wav2Vec2Recognizer.builder().build()) {\n    System.out.println(recognizer.transcribe(Path.of(\"audio.wav\")).text());\n}\n</code></pre>"},{"location":"#voice-activity-detection","title":"Voice Activity Detection","text":"<pre><code>try (var vad = SileroVadDetector.builder().build()) {\n    List&lt;VoiceSegment&gt; segments = vad.detect(Path.of(\"meeting.wav\"));\n    // [VoiceSegment[start=0.50, end=3.20], VoiceSegment[start=5.10, end=8.75]]\n}\n</code></pre>"},{"location":"#text-detection","title":"Text Detection","text":"<pre><code>try (var detector = CraftTextDetector.builder().build()) {\n    List&lt;TextRegion&gt; regions = detector.detect(Path.of(\"document.jpg\"));\n}\n</code></pre>"},{"location":"#zero-shot-image-classification","title":"Zero-Shot Image Classification","text":"<pre><code>try (var classifier = ClipClassifier.builder().build()) {\n    List&lt;Classification&gt; results = classifier.classify(\n            Path.of(\"photo.jpg\"), List.of(\"cat\", \"dog\", \"bird\", \"car\"));\n    // [Classification[label=cat, confidence=0.82], ...]\n}\n</code></pre>"},{"location":"#search-reranking","title":"Search Reranking","text":"<pre><code>try (var reranker = MiniLMSearchReranker.builder().build()) {\n    float score = reranker.score(\"What is Java?\", \"Java is a programming language.\");\n}\n</code></pre>"},{"location":"#summarization","title":"Summarization","text":"<pre><code>try (var summarizer = BartSummarizer.distilBartCnn().build()) {\n    System.out.println(summarizer.summarize(\"Long article text here...\"));\n}\n</code></pre>"},{"location":"#translation","title":"Translation","text":"<pre><code>try (var translator = FlanT5TextGenerator.flanT5Base().build()) {\n    System.out.println(translator.translate(\"Hello!\", Language.EN, Language.FR));\n}\n</code></pre>"},{"location":"#grammar-correction","title":"Grammar Correction","text":"<pre><code>try (var corrector = CoeditGrammarCorrector.coeditBase().build()) {\n    System.out.println(corrector.correct(\"She don't likes swimming.\"));\n}\n</code></pre>"},{"location":"#text-to-sql","title":"Text-to-SQL","text":"<pre><code>try (var sqlGen = T5SqlGenerator.t5SmallAwesome().build()) {\n    System.out.println(sqlGen.generateSql(\"How many users?\",\n            \"CREATE TABLE users (id INT, name VARCHAR, email VARCHAR)\"));\n}\n</code></pre>"},{"location":"#text-generation","title":"Text Generation","text":"<pre><code>try (var generator = TextGenerator.builder()\n        .model(ModelSources.phi3Mini())\n        .build()) {\n    generator.generate(\"Explain recursion.\", token -&gt; System.out.print(token));\n}\n</code></pre>"},{"location":"#what-you-dont-have-to-do","title":"What you don't have to do","text":"<ul> <li>No tokenization \u2014 WordPiece tokenizers are built in and handled automatically</li> <li>No tensor handling \u2014 pass a <code>String</code>, <code>BufferedImage</code>, or <code>Path</code>; get Java objects back</li> <li>No ONNX session setup \u2014 <code>builder().build()</code> handles everything</li> <li>No model downloads \u2014 auto-downloaded from HuggingFace and cached on first use</li> <li>No Python sidecar \u2014 pure Java, runs anywhere Java runs</li> </ul>"},{"location":"#why-inference4j","title":"Why inference4j?","text":""},{"location":"#the-problem","title":"The problem","text":"<p>Running a trained ML model in Java sounds simple \u2014 load the model, pass some data, get a result. In practice, the inference call itself is the easy part. The hard part is everything around it: preprocessing and postprocessing.</p> <p>Before a model can process an image, someone has to resize it, normalize the pixel values, rearrange the channels into the right layout (NCHW? NHWC?), and pack the result into a multi-dimensional float array called a tensor. After the model runs, someone has to interpret the raw output tensor \u2014 apply softmax to get probabilities, decode token IDs back into text, map class indices to human-readable labels, run non-maximum suppression to filter overlapping bounding boxes.</p> <p>This work requires understanding the model's internals: its expected input shape, normalization constants, output format, and decoding strategy. ML engineers deal with this routinely. Java developers who need to ship a model to production shouldn't have to.</p>"},{"location":"#built-on-onnx","title":"Built on ONNX","text":"<p>inference4j embraces ONNX (Open Neural Network Exchange) as its runtime platform. ONNX is an open standard for representing ML models \u2014 a model trained in PyTorch, TensorFlow, or JAX can be exported to a single <code>.onnx</code> file and run anywhere via ONNX Runtime. This means inference4j can run models from any training framework without depending on that framework at runtime. No Python, no TensorFlow, no PyTorch \u2014 just a <code>.onnx</code> file and the ONNX Runtime native library.</p>"},{"location":"#what-inference4j-does","title":"What inference4j does","text":"<p>The vast majority of inference tasks follow the same three-stage pattern: preprocess \u2192 infer \u2192 postprocess. inference4j provides curated wrappers that handle all three stages, so you work with standard Java types instead of tensors:</p> <pre><code>%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '18px'}}}%%\nflowchart TD\n    Input[\"&lt;b&gt;Java Object&lt;/b&gt;&lt;br&gt;String, BufferedImage, Path\"]\n\n    subgraph inference4j[\"inference4j wrapper\"]\n        Pre[\"&lt;b&gt;Preprocess&lt;/b&gt;&lt;br&gt;tokenize text, resize/normalize image, load audio\"]\n        Infer[\"&lt;b&gt;Infer&lt;/b&gt;&lt;br&gt;ONNX Runtime forward pass\"]\n        Post[\"&lt;b&gt;Postprocess&lt;/b&gt;&lt;br&gt;softmax, decode, NMS, label mapping\"]\n        Pre --&gt; Infer --&gt; Post\n    end\n\n    Output[\"&lt;b&gt;Java Object&lt;/b&gt;&lt;br&gt;Classification, Transcription, Detection\"]\n\n    Input --&gt; Pre\n    Post --&gt; Output</code></pre> <p>Each wrapper encapsulates the model-specific knowledge \u2014 the normalization constants, the tokenizer, the output decoding \u2014 so you don't have to.</p>"},{"location":"#where-it-fits","title":"Where it fits","text":"<p>Java has great tools for building AI-powered applications. Spring AI provides an excellent abstraction layer for LLM orchestration. DJL offers engine-agnostic model training and inference. LangChain4j simplifies LLM-powered workflows.</p> <p>inference4j doesn't compete with any of them. It fills a different gap: taking a specific ONNX model and making it trivial to call from Java, with all the preprocessing and postprocessing handled for you.</p> <ul> <li>3-line integration for popular models \u2014 <code>builder().build()</code>, call a method, get Java objects back</li> <li>Standard Java types in, standard Java types out \u2014 no tensor abstractions leak into your code</li> <li>Inference only \u2014 optimized for production serving, not training</li> <li>Lightweight \u2014 each wrapper is a thin layer over ONNX Runtime, not a framework</li> <li>Complements the ecosystem \u2014 use inference4j to run your embedding model, Spring AI to orchestrate your LLM chain, both in the same application</li> </ul> <p>Learn more about the pipeline architecture in How It Works.</p> <p>Get started Browse use cases</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#completed","title":"Completed","text":""},{"location":"roadmap/#phase-1-foundation-core-nlp","title":"Phase 1: Foundation (Core &amp; NLP)","text":"<ul> <li>[x] <code>inference4j-core</code> \u2014 <code>InferenceSession</code>, <code>Tensor</code>, <code>ModelSource</code>, <code>MathOps</code> (softmax, sigmoid, logSoftmax, topK, NMS, cxcywh2xyxy)</li> <li>[x] Tokenizers \u2014 <code>WordPieceTokenizer</code>, <code>BpeTokenizer</code>, <code>DecodingBpeTokenizer</code>, <code>EncodedInput</code>, <code>Tokenizer</code> interface</li> <li>[x] <code>SentenceTransformer</code> wrapper \u2014 sentence embeddings with CLS/MEAN/MAX pooling</li> <li>[x] <code>EmbeddingModelRouter</code> \u2014 A/B testing with round-robin routing</li> </ul>"},{"location":"roadmap/#phase-2-vision","title":"Phase 2: Vision","text":"<ul> <li>[x] Image preprocessing \u2014 <code>ImageTransformPipeline</code>, <code>ResizeTransform</code>, <code>CenterCropTransform</code>, <code>ImageLayout</code> (NCHW/NHWC), <code>Labels</code> (ImageNet/COCO presets)</li> <li>[x] <code>ResNet</code> wrapper \u2014 image classification with ImageNet defaults</li> <li>[x] <code>EfficientNet</code> wrapper \u2014 image classification with TensorFlow defaults</li> </ul>"},{"location":"roadmap/#phase-25-object-detection","title":"Phase 2.5: Object Detection","text":"<ul> <li>[x] <code>YoloV8</code> wrapper \u2014 NMS-based detection (also compatible with YOLO11)</li> <li>[x] <code>Yolo26</code> wrapper \u2014 NMS-free detection</li> <li>[x] <code>ObjectDetector</code> interface, <code>Detection</code>, <code>BoundingBox</code> result types</li> </ul>"},{"location":"roadmap/#phase-3-audio","title":"Phase 3: Audio","text":"<ul> <li>[x] <code>Wav2Vec2</code> wrapper \u2014 CTC speech-to-text (single-pass, non-autoregressive)</li> <li>[x] Audio preprocessing \u2014 <code>AudioLoader</code> (WAV loading), <code>AudioProcessor</code> (resample, normalize), <code>Vocabulary</code> (vocab.json)</li> <li>[x] <code>MathOps.ctcGreedyDecode()</code> \u2014 CTC greedy decoding</li> <li>[x] <code>SpeechRecognizer</code> interface, <code>Transcription</code> result type</li> <li>[x] Silero VAD wrapper \u2014 voice activity detection</li> <li>[x] Hardware acceleration benchmarks (CoreML: ResNet 3.7x, CRAFT 5.4x)</li> </ul>"},{"location":"roadmap/#phase-35-nlp","title":"Phase 3.5: NLP","text":"<ul> <li>[x] <code>DistilBertTextClassifier</code> wrapper \u2014 text classification with auto-detection of softmax/sigmoid from <code>config.json</code></li> <li>[x] <code>MiniLMSearchReranker</code> wrapper \u2014 cross-encoder query-document relevance scoring</li> <li>[x] <code>TextClassifier</code> and <code>SearchReranker</code> interfaces, <code>TextClassification</code> result type</li> <li>[x] <code>ModelConfig</code> \u2014 parses HuggingFace <code>config.json</code> for <code>id2label</code> and <code>problem_type</code></li> <li>[x] Sentence pair encoding in <code>Tokenizer</code>/<code>WordPieceTokenizer</code></li> </ul>"},{"location":"roadmap/#phase-4-clip-visual-search-zero-shot-classification","title":"Phase 4: CLIP \u2014 Visual Search &amp; Zero-Shot Classification","text":"<ul> <li>[x] CLIP image encoder and text encoder</li> <li>[x] <code>ClipClassifier</code> \u2014 zero-shot image classification against arbitrary text labels</li> <li>[x] <code>ClipModel</code> with <code>similarity(image, texts)</code> API</li> <li>[x] <code>BpeTokenizer</code> \u2014 byte-level BPE for CLIP/GPT-2 family</li> <li>[x] Runnable examples in <code>inference4j-examples</code></li> </ul>"},{"location":"roadmap/#phase-5-autoregressive-generation","title":"Phase 5: Autoregressive Generation","text":"<ul> <li>[x] <code>GenerationEngine</code> \u2014 pure ONNX Runtime autoregressive loop with KV cache</li> <li>[x] <code>GenerativeTask</code> / <code>GenerativeSession</code> \u2014 generation contracts in core</li> <li>[x] Sampling pipeline \u2014 <code>LogitsProcessor</code>, <code>GreedySampler</code>, <code>CategoricalSampler</code>, temperature/topK/topP</li> <li>[x] <code>TokenStreamer</code> \u2014 streaming token delivery with stop sequence support</li> <li>[x] <code>OnnxTextGenerator</code> \u2014 native text generation with GPT-2, SmolLM2, Qwen2.5 (pure ONNX Runtime, no genai dependency)</li> <li>[x] <code>DecodingBpeTokenizer</code> / <code>TokenDecoder</code> \u2014 BPE tokenizer with decoding support</li> <li>[x] <code>inference4j-genai</code> \u2014 onnxruntime-genai backed generation for larger models (Phi-3, DeepSeek-R1, Phi-3.5 Vision)</li> <li>[x] Streaming generation API \u2014 token-by-token callbacks via <code>Consumer&lt;String&gt;</code></li> </ul>"},{"location":"roadmap/#phase-6-encoder-decoder-generation","title":"Phase 6: Encoder-Decoder Generation","text":"<ul> <li>[x] <code>EncoderDecoderSession</code> \u2014 encoder-decoder autoregressive loop with cross-attention and self-attention KV caches</li> <li>[x] <code>AbstractEncoderDecoderBuilder</code> \u2014 shared builder for encoder-decoder wrappers</li> <li>[x] <code>FlanT5TextGenerator</code> \u2014 multi-task text generation (summarization, translation, SQL, grammar) with Flan-T5</li> <li>[x] <code>BartSummarizer</code> \u2014 text summarization with BART / DistilBART</li> <li>[x] <code>MarianTranslator</code> \u2014 machine translation with MarianMT (Helsinki-NLP opus-mt models)</li> <li>[x] <code>CoeditGrammarCorrector</code> \u2014 grammar correction with CoEdIT</li> <li>[x] Task interfaces \u2014 <code>TextGenerator</code>, <code>Summarizer</code>, <code>Translator</code>, <code>GrammarCorrector</code>, <code>SqlGenerator</code></li> <li>[x] <code>Language</code> enum \u2014 24 languages for typed translation APIs</li> </ul>"},{"location":"roadmap/#architecture-ecosystem","title":"Architecture &amp; Ecosystem","text":"<ul> <li>[x] <code>AbstractInferenceTask</code> \u2014 enforced preprocess \u2192 infer \u2192 postprocess pipeline with <code>final run()</code></li> <li>[x] <code>Preprocessor</code>/<code>Postprocessor</code> functional interfaces</li> <li>[x] <code>InferenceContext</code> \u2014 cross-stage data carrier</li> <li>[x] Task-oriented architecture \u2014 <code>InferenceTask</code> \u2192 <code>Classifier</code>/<code>Detector</code> \u2192 domain interfaces</li> <li>[x] Builder API \u2014 <code>.session()</code> package-private, public API uses <code>modelId</code> + <code>modelSource</code> + <code>sessionOptions(SessionConfigurer)</code></li> <li>[x] Spring Boot starter \u2014 auto-configuration, health indicators</li> <li>[x] Documentation site (MkDocs Material)</li> <li>[x] CRAFT text detection wrapper \u2014 <code>TextDetector</code> interface, <code>TextRegion</code>, <code>CraftTextDetector</code></li> <li>[x] Model test suite \u2014 <code>./gradlew modelTest</code> with real model downloads and inference verification</li> <li>[x] Module consolidation \u2014 <code>inference4j-tasks</code> and <code>inference4j-preprocessing</code> merged into <code>inference4j-core</code></li> </ul>"},{"location":"roadmap/#next-up","title":"Next Up","text":""},{"location":"roadmap/#v0100-ner-embeddings","title":"v0.10.0 \u2014 NER &amp; Embeddings","text":"<ul> <li>[ ] Named Entity Recognition \u2014 NER via BERT-based token classification models (e.g., <code>bert-base-NER</code>, <code>dslim/bert-base-NER</code>)</li> <li>[ ] <code>TokenClassifier</code> interface, <code>NamedEntity</code> result type (entity text, label, span, confidence)</li> <li>[ ] Improved embeddings \u2014 support for larger, more capable embedding models (e.g., <code>bge-base</code>, <code>gte-base</code>, <code>e5-base-v2</code>) beyond MiniLM</li> </ul>"},{"location":"roadmap/#v0110-tiktoken-llm-support","title":"v0.11.0 \u2014 Tiktoken &amp; LLM Support","text":"<ul> <li>[ ] Tiktoken tokenizer \u2014 <code>cl100k_base</code> / <code>o200k_base</code> encoding for OpenAI-family models</li> <li>[ ] At least one LLM that uses Tiktoken (e.g., Llama 3, Phi-4)</li> </ul>"},{"location":"roadmap/#v0120-text-to-speech","title":"v0.12.0 \u2014 Text-to-Speech","text":"<ul> <li>[ ] Piper TTS \u2014 text-to-speech via Piper ONNX models</li> <li>[ ] <code>SpeechSynthesizer</code> interface, audio output generation</li> </ul>"},{"location":"roadmap/#beyond","title":"Beyond","text":"<ul> <li>[ ] OCR Pipeline \u2014 CRAFT detection + TrOCR recognition composed end-to-end; study viability of full TrOCR models</li> <li>[ ] CRAFT improvements \u2014 test and improve support for vertical text and mixed orientation</li> <li>[ ] Whisper \u2014 study cost of mel spectrogram / FFT preprocessing; native autoregressive speech-to-text</li> <li>[ ] Stable Diffusion \u2014 study feasibility of text-to-image models on ONNX Runtime, including lightweight variants with GPU acceleration</li> <li>[ ] More ViT models \u2014 additional Vision Transformer variants (low effort, reuse existing image classification infrastructure)</li> </ul>"},{"location":"roadmap/#dropped","title":"Dropped","text":"<ul> <li>~~Generic Pipeline API~~ \u2014 <code>Pipeline.builder().stage().stage().build()</code> adds abstraction without value. Models are too different for a generic composition framework. Named pipelines (e.g., <code>OcrPipeline</code>) as concrete classes instead.</li> <li>~~Codegen plugin~~ \u2014 generates type-safe wrappers from <code>.onnx</code> metadata but doesn't solve preprocessing/postprocessing, which is where the real complexity lives. Handcrafted wrappers deliver more value.</li> </ul>"},{"location":"roadmap/#target-models","title":"Target models","text":"Domain Model Status Text SentenceTransformer (all-MiniLM, all-mpnet, BERT) Done Text Cross-encoder reranker (ms-marco-MiniLM) Done Text Text classification (DistilBERT, sentiment, moderation) Done Text CRAFT (text detection) Done Text GPT-2 (text generation) Done Text SmolLM2-360M-Instruct (text generation) Done Text Qwen2.5-1.5B-Instruct (text generation) Done Text Flan-T5 (summarization, translation, SQL, grammar) Done Text BART / DistilBART (summarization) Done Text MarianMT (translation) Done Text CoEdIT (grammar correction) Done Text BERT NER (named entity recognition) v0.10.0 Text BGE / GTE / E5 (improved embeddings) v0.10.0 Text Tiktoken LLM v0.11.0 Vision ResNet Done Vision EfficientNet Done Vision YOLOv8 / YOLO11 Done Vision YOLO26 Done Vision CLIP (visual search, zero-shot classification) Done Vision Phi-3.5 Vision (captioning, VQA) Done Vision Additional ViT models Beyond Vision TrOCR + OCR Pipeline Beyond Vision Stable Diffusion (text-to-image) Beyond \u2014 feasibility study Audio Wav2Vec2-CTC (speech-to-text) Done Audio Silero VAD (voice activity detection) Done Audio Piper TTS (text-to-speech) v0.12.0 Audio Whisper (autoregressive speech-to-text) Beyond \u2014 feasibility study"},{"location":"contributing/adding-a-wrapper/","title":"Adding a Wrapper","text":"<p>This guide explains the internals of inference4j for contributors who want to add support for a new model.</p>"},{"location":"contributing/adding-a-wrapper/#architecture-overview","title":"Architecture overview","text":"<p>Every wrapper follows the same three-stage pattern enforced by <code>AbstractInferenceTask</code>:</p> <pre><code>%%{init: {'theme': 'base', 'themeVariables': {'fontSize': '18px'}}}%%\nflowchart TD\n    Input[\"&lt;b&gt;I \u2014 domain input&lt;/b&gt;&lt;br&gt;String, BufferedImage, Path\"]\n\n    subgraph AbstractInferenceTask[\"AbstractInferenceTask (final run)\"]\n        Pre[\"&lt;b&gt;Preprocessor&lt;/b&gt;&lt;br&gt;domain input \u2192 Map of String, Tensor\"]\n        Session[\"&lt;b&gt;InferenceSession.run()&lt;/b&gt;&lt;br&gt;ONNX Runtime forward pass\"]\n        Post[\"&lt;b&gt;Postprocessor&lt;/b&gt;&lt;br&gt;Map of String, Tensor \u2192 domain output\"]\n        Pre --&gt; Session --&gt; Post\n    end\n\n    Output[\"&lt;b&gt;O \u2014 domain output&lt;/b&gt;&lt;br&gt;Classification, Transcription, Detection\"]\n\n    Input --&gt; Pre\n    Post --&gt; Output</code></pre> <p>The <code>run()</code> method is <code>final</code> \u2014 you can't override it. Instead, you provide a <code>Preprocessor</code> and a <code>Postprocessor</code> to the constructor, and the base class wires the pipeline together.</p>"},{"location":"contributing/adding-a-wrapper/#core-types","title":"Core types","text":""},{"location":"contributing/adding-a-wrapper/#abstractinferencetask","title":"AbstractInferenceTask","text":"<pre><code>public abstract class AbstractInferenceTask&lt;I, O&gt; implements InferenceTask&lt;I, O&gt; {\n\n    protected final InferenceSession session;\n    protected final Preprocessor&lt;I, Map&lt;String, Tensor&gt;&gt; preprocessor;\n    protected final Postprocessor&lt;InferenceContext&lt;I&gt;, O&gt; postprocessor;\n\n    protected AbstractInferenceTask(\n            InferenceSession session,\n            Preprocessor&lt;I, Map&lt;String, Tensor&gt;&gt; preprocessor,\n            Postprocessor&lt;InferenceContext&lt;I&gt;, O&gt; postprocessor) {\n        // ...\n    }\n\n    @Override\n    public final O run(I input) {\n        Map&lt;String, Tensor&gt; inputs = preprocessor.process(input);\n        Map&lt;String, Tensor&gt; outputs = session.run(inputs);\n        return postprocessor.process(new InferenceContext&lt;&gt;(input, inputs, outputs));\n    }\n}\n</code></pre> <p>Key points:</p> <ul> <li><code>I</code> is the domain input type (<code>String</code>, <code>BufferedImage</code>, <code>Path</code>)</li> <li><code>O</code> is the domain output type (<code>List&lt;Classification&gt;</code>, <code>Transcription</code>, etc.)</li> <li><code>run()</code> is <code>final</code> \u2014 the pipeline sequence is not negotiable</li> <li>You wire preprocessing and postprocessing in your constructor or builder</li> </ul>"},{"location":"contributing/adding-a-wrapper/#inferencecontext","title":"InferenceContext","text":"<pre><code>public record InferenceContext&lt;I&gt;(\n    I input,\n    Map&lt;String, Tensor&gt; preprocessed,\n    Map&lt;String, Tensor&gt; outputs\n) {}\n</code></pre> <p>The postprocessor receives the full context \u2014 not just the output tensors. This is essential when postprocessing needs information from the original input (e.g., original image dimensions for rescaling bounding boxes back to pixel coordinates).</p>"},{"location":"contributing/adding-a-wrapper/#preprocessor-and-postprocessor","title":"Preprocessor and Postprocessor","text":"<p>Both are <code>@FunctionalInterface</code>:</p> <pre><code>@FunctionalInterface\npublic interface Preprocessor&lt;I, O&gt; {\n    O process(I input);\n}\n\n@FunctionalInterface\npublic interface Postprocessor&lt;I, O&gt; {\n    O process(I input);\n}\n</code></pre> <p>They support composition via <code>andThen()</code> and provide a static <code>identity()</code> factory for no-op stages.</p>"},{"location":"contributing/adding-a-wrapper/#vision-wrappers-imagetransformpipeline","title":"Vision wrappers: ImageTransformPipeline","text":"<p>For image-based models, <code>ImageTransformPipeline</code> handles the <code>BufferedImage \u2192 Tensor</code> conversion. It composes image transforms (resize, center crop, normalize) and produces a correctly shaped float tensor.</p> <pre><code>flowchart TD\n    A[\"BufferedImage\"]\n\n    subgraph pipeline [\"ImageTransformPipeline\"]\n        B[\"Resize (e.g. 224x224)\"]\n        C[\"Center crop (optional)\"]\n        D[\"Extract RGB pixels\"]\n        E[\"Normalize&lt;br&gt;(pixel/255 - mean) / std\"]\n        F[\"Arrange into layout&lt;br&gt;NCHW or NHWC\"]\n        B --&gt; C --&gt; D --&gt; E --&gt; F\n    end\n\n    G[\"Tensor \u2014 shape: 1, 3, H, W\"]\n\n    A --&gt; B\n    F --&gt; G</code></pre>"},{"location":"contributing/adding-a-wrapper/#imagenet-preset","title":"ImageNet preset","text":"<p>For models trained on ImageNet (ResNet, VGG, etc.):</p> <pre><code>ImageTransformPipeline pipeline = ImageTransformPipeline.imagenet(224);\n// mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]\n// NCHW layout, size 224\u00d7224\n</code></pre>"},{"location":"contributing/adding-a-wrapper/#custom-pipeline","title":"Custom pipeline","text":"<p>For models with different normalization (EfficientNet, YOLO, etc.), use the builder:</p> <pre><code>ImageTransformPipeline pipeline = ImageTransformPipeline.builder()\n    .resize(280)\n    .mean(new float[]{127f/255, 127f/255, 127f/255})\n    .std(new float[]{128f/255, 128f/255, 128f/255})\n    .layout(ImageLayout.NHWC)\n    .build();\n</code></pre>"},{"location":"contributing/adding-a-wrapper/#auto-detection-from-model","title":"Auto-detection from model","text":"<p>The <code>AbstractImageClassifier.detectPipeline()</code> helper reads the model's input shape from the ONNX session and auto-configures the pipeline:</p> <ul> <li>Detects input dimensions \u2192 sets resize target</li> <li>Detects channel position \u2192 sets NCHW or NHWC layout</li> <li>Applies the normalization constants you provide</li> </ul> <p>This means adding a new image classifier often requires only specifying the correct mean/std values \u2014 the rest is auto-detected.</p>"},{"location":"contributing/adding-a-wrapper/#anatomy-of-a-wrapper","title":"Anatomy of a wrapper","text":"<p>Here's the typical structure, using <code>ResNetClassifier</code> as a reference:</p>"},{"location":"contributing/adding-a-wrapper/#1-extend-the-right-base-class","title":"1. Extend the right base class","text":"<pre><code>public class ResNetClassifier extends AbstractImageClassifier {\n    // AbstractImageClassifier extends AbstractInferenceTask&lt;BufferedImage, List&lt;Classification&gt;&gt;\n    // and implements ImageClassifier (which extends Classifier&lt;BufferedImage, Classification&gt;)\n}\n</code></pre> <p>Choose your base:</p> Base class When to use <code>AbstractInferenceTask&lt;I, O&gt;</code> Any task \u2014 most flexible <code>AbstractImageClassifier</code> Image classification (provides <code>classify()</code> methods, top-K, label mapping) Direct interface implementation Stateful models (e.g., <code>SileroVadDetector</code> manages hidden state across frames)"},{"location":"contributing/adding-a-wrapper/#2-define-the-builder","title":"2. Define the builder","text":"<pre><code>public static Builder builder() {\n    return new Builder();\n}\n\npublic static class Builder {\n    private String modelId = \"inference4j/resnet50-v1-7\";  // sensible default\n    private ModelSource modelSource;\n    private SessionConfigurer sessionConfigurer;\n    // ... task-specific fields ...\n\n    public Builder modelId(String modelId) { this.modelId = modelId; return this; }\n    public Builder modelSource(ModelSource modelSource) { this.modelSource = modelSource; return this; }\n    public Builder sessionOptions(SessionConfigurer sc) { this.sessionConfigurer = sc; return this; }\n\n    // package-private \u2014 not part of the public API\n    Builder session(InferenceSession session) { this.session = session; return this; }\n\n    public ResNetClassifier build() {\n        // 1. Resolve model files\n        if (session == null) {\n            ModelSource source = modelSource != null ? modelSource\n                : HuggingFaceModelSource.defaultInstance();\n            Path modelDir = source.resolve(modelId);\n            session = InferenceSession.of(modelDir.resolve(\"model.onnx\"), sessionConfigurer);\n        }\n        // 2. Load task-specific resources (labels, vocab, config)\n        // 3. Build preprocessor and postprocessor\n        // 4. Return new instance\n        return new ResNetClassifier(session, preprocessor, labels, ...);\n    }\n}\n</code></pre> <p>Key conventions:</p> <ul> <li>Default model ID \u2014 every wrapper has a sensible default pointing to the <code>inference4j</code> HuggingFace org</li> <li><code>.session()</code> is package-private \u2014 the public API uses <code>modelId</code> + <code>modelSource</code> + <code>sessionOptions</code></li> <li>Auto-load resources \u2014 labels, vocabulary, config are loaded from the model directory in <code>build()</code></li> </ul>"},{"location":"contributing/adding-a-wrapper/#3-wire-preprocessing","title":"3. Wire preprocessing","text":"<p>For vision, the preprocessor converts a <code>BufferedImage</code> into a named tensor map:</p> <pre><code>// In the constructor or builder\nPreprocessor&lt;BufferedImage, Map&lt;String, Tensor&gt;&gt; preprocessor =\n    image -&gt; Map.of(inputName, imageTransformPipeline.process(image));\n</code></pre> <p>For NLP, tokenize text into input ID and attention mask tensors:</p> <pre><code>Preprocessor&lt;String, Map&lt;String, Tensor&gt;&gt; preprocessor = text -&gt; {\n    EncodedInput encoded = tokenizer.encode(text, maxLength);\n    return Map.of(\n        \"input_ids\", Tensor.of(encoded.inputIds()),\n        \"attention_mask\", Tensor.of(encoded.attentionMask())\n    );\n};\n</code></pre>"},{"location":"contributing/adding-a-wrapper/#4-wire-postprocessing","title":"4. Wire postprocessing","text":"<p>The postprocessor receives <code>InferenceContext&lt;I&gt;</code> \u2014 giving access to both the raw output tensors and the original input:</p> <pre><code>Postprocessor&lt;InferenceContext&lt;BufferedImage&gt;, List&lt;Classification&gt;&gt; postprocessor =\n    ctx -&gt; {\n        float[] logits = ctx.outputs().get(\"output\").toFloats();\n        float[] probs = outputOperator.apply(logits);  // softmax\n        int[] topIndices = MathOps.topK(probs, topK);\n        return Arrays.stream(topIndices)\n            .mapToObj(i -&gt; new Classification(labels.get(i), i, probs[i]))\n            .toList();\n    };\n</code></pre>"},{"location":"contributing/adding-a-wrapper/#utilities-available","title":"Utilities available","text":"Utility Package Purpose <code>ImageTransformPipeline</code> <code>io.github.inference4j.preprocessing.image</code> BufferedImage \u2192 Tensor with resize, normalize, layout <code>WordPieceTokenizer</code> <code>io.github.inference4j.tokenizer</code> Text \u2192 token IDs (BERT-family models) <code>BpeTokenizer</code> <code>io.github.inference4j.tokenizer</code> Byte-level BPE for CLIP/GPT-2 family <code>MathOps</code> <code>io.github.inference4j.processing</code> <code>softmax</code>, <code>sigmoid</code>, <code>topK</code>, <code>nms</code>, <code>ctcGreedyDecode</code>, <code>cxcywh2xyxy</code> <code>OutputOperator</code> <code>io.github.inference4j.processing</code> Pre-built activations: <code>softmax()</code>, <code>sigmoid()</code>, <code>identity()</code> <code>Labels</code> <code>io.github.inference4j.preprocessing.image</code> Label sets with presets: <code>Labels.imagenet()</code>, <code>Labels.coco()</code> <code>AudioLoader</code> <code>io.github.inference4j.preprocessing.audio</code> WAV file loading <code>AudioProcessor</code> <code>io.github.inference4j.preprocessing.audio</code> Resample, normalize audio <code>Vocabulary</code> <code>io.github.inference4j.preprocessing.audio</code> Load vocab.json for CTC models <code>ModelConfig</code> <code>io.github.inference4j.preprocessing.text</code> Parse HuggingFace config.json (<code>id2label</code>, <code>problem_type</code>)"},{"location":"contributing/adding-a-wrapper/#checklist-for-a-new-wrapper","title":"Checklist for a new wrapper","text":"<ol> <li>Identify the model's contract \u2014 input tensor names, shapes, normalization, output format</li> <li>Choose a base class \u2014 <code>AbstractInferenceTask</code>, <code>AbstractImageClassifier</code>, or direct interface implementation</li> <li>Implement preprocessing \u2014 convert the domain input to the tensors the model expects</li> <li>Implement postprocessing \u2014 convert output tensors to a meaningful Java result type</li> <li>Add a builder with <code>modelId</code>, <code>modelSource</code>, <code>sessionOptions</code>, and task-specific options</li> <li>Host the ONNX model under the <code>inference4j</code> HuggingFace org with required companion files</li> <li>Write tests \u2014 unit tests with mocked session, integration tests with the real model</li> <li>Add Spring Boot auto-configuration if the wrapper should be available as a managed bean</li> </ol> <p>Sharing your wrapper</p> <p>If you intend to share your wrapper or model with the community, please follow the Contributing Guide for instructions on how to submit a pull request and host the model on the inference4j HuggingFace org.</p>"},{"location":"generative-ai/chat-templates/","title":"Chat Templates","text":"<p>Every instruction-tuned language model expects prompts in a specific format. The model was trained on conversations structured with special marker tokens, and it will only follow instructions reliably if the prompt matches that format. A <code>ChatTemplate</code> tells inference4j how to wrap a user message in the correct markers before passing it to the tokenizer.</p>"},{"location":"generative-ai/chat-templates/#why-models-need-different-templates","title":"Why models need different templates","text":"<p>During training, each model family learns to associate specific token sequences with roles in a conversation. These markers vary across model families:</p> Model User marker End marker Assistant marker Phi-3 <code>&lt;\\|user\\|&gt;</code> <code>&lt;\\|end\\|&gt;</code> <code>&lt;\\|assistant\\|&gt;</code> DeepSeek-R1 (Qwen) <code>&lt;\\|User\\|&gt;</code> \u2014 <code>&lt;\\|Assistant\\|&gt;</code> Llama / ChatML <code>&lt;\\|im_start\\|&gt;user</code> <code>&lt;\\|im_end\\|&gt;</code> <code>&lt;\\|im_start\\|&gt;assistant</code> <p>If you send a Phi-3 prompt to a DeepSeek model (or vice versa), the model won't recognize the role markers. It may ignore the instruction entirely, echo the markers back, or produce incoherent output.</p>"},{"location":"generative-ai/chat-templates/#how-it-works","title":"How it works","text":"<p><code>ChatTemplate</code> is a functional interface with a single method:</p> <pre><code>@FunctionalInterface\npublic interface ChatTemplate {\n    String format(String userMessage);\n}\n</code></pre> <p>It takes the raw user message and returns the fully formatted prompt string. For Phi-3, the implementation looks like:</p> <pre><code>message -&gt; \"&lt;|user|&gt;\\n\" + message + \"&lt;|end|&gt;\\n&lt;|assistant|&gt;\\n\"\n</code></pre> <p>For a prompt like \"What is Java?\", this produces:</p> <pre><code>&lt;|user|&gt;\nWhat is Java?&lt;|end|&gt;\n&lt;|assistant|&gt;\n</code></pre> <p>The tokenizer then encodes this formatted string into token IDs that the model recognizes.</p>"},{"location":"generative-ai/chat-templates/#preconfigured-templates","title":"Preconfigured templates","text":""},{"location":"generative-ai/chat-templates/#onnxruntime-genai-models","title":"onnxruntime-genai models","text":"<p>When you use <code>ModelSources</code> factory methods, the chat template is already configured:</p> <pre><code>// Chat template is bundled \u2014 nothing extra to configure\nTextGenerator.builder()\n        .model(ModelSources.phi3Mini())\n        .build();\n</code></pre> <p><code>ModelSources.phi3Mini()</code> returns a <code>GenerativeModel</code> that pairs the model source with the correct Phi-3 chat template. <code>ModelSources.deepSeekR1_1_5B()</code> does the same for DeepSeek's format.</p>"},{"location":"generative-ai/chat-templates/#native-generation-models","title":"Native generation models","text":"<p><code>OnnxTextGenerator</code> presets for instruct models (SmolLM2, Qwen2.5) come with a ChatML template preconfigured. GPT-2 is a base model (not instruction-tuned) so it has no default template, but you can provide one:</p> <pre><code>OnnxTextGenerator.gpt2()\n        .chatTemplate(msg -&gt; \"Q: \" + msg + \"\\nA:\")\n        .maxNewTokens(100)\n        .build();\n</code></pre>"},{"location":"generative-ai/chat-templates/#custom-templates","title":"Custom templates","text":"<p>To use a model that isn't preconfigured in <code>ModelSources</code>, provide both a <code>ModelSource</code> and a <code>ChatTemplate</code>:</p> <pre><code>TextGenerator.builder()\n        .modelSource(myModelSource)\n        .chatTemplate(msg -&gt; \"&lt;|im_start|&gt;user\\n\" + msg + \"&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\")\n        .build();\n</code></pre> <p>The template format for a given model is defined in its <code>tokenizer_config.json</code> file on HuggingFace, under the <code>chat_template</code> field. It's typically a Jinja2 template \u2014 you only need to translate the user-message portion into a Java lambda.</p>"},{"location":"generative-ai/chat-templates/#adding-support-for-a-new-model","title":"Adding support for a new model","text":"<p>When a new model family uses a chat template that isn't covered by <code>ModelSources</code>, you need two things:</p> <ol> <li>The model files \u2014 a <code>ModelSource</code> that resolves the ONNX weights, tokenizer,    and config files</li> <li>The chat template \u2014 a <code>ChatTemplate</code> lambda that formats prompts with the    correct markers</li> </ol> <p>Look at the model's <code>tokenizer_config.json</code> for the <code>chat_template</code> field to find the expected format. For a simple user message (no system prompt, no tools), the relevant portion is usually a few lines of Jinja that map to a straightforward string concatenation in Java.</p>"},{"location":"generative-ai/introduction/","title":"Generative AI","text":"<p>Run autoregressive models \u2014 text generation, speech-to-text, and vision-language \u2014 directly in Java with token-by-token generation and streaming.</p>"},{"location":"generative-ai/introduction/#two-approaches-to-generation","title":"Two approaches to generation","text":"<p>All models in inference4j outside this section are single-pass \u2014 one forward pass, one result. Generative models are fundamentally different. They produce output one token at a time in a loop, feeding each token back into the model to produce the next one. This loop requires managing a KV cache, token sampling, and stop conditions.</p> <p>inference4j supports two approaches to running this loop:</p>"},{"location":"generative-ai/introduction/#native-generation-inference4j-core","title":"Native generation (inference4j-core)","text":"<p>inference4j implements the full autoregressive loop in Java on top of standard ONNX Runtime. This includes KV cache management, token sampling (temperature, top-K, top-P), BPE tokenization with decoding, and streaming \u2014 all built into <code>inference4j-core</code> with zero additional dependencies.</p> <p>Pros:</p> <ul> <li>Works with any ONNX model that exports KV cache inputs/outputs \u2014 the standard HuggingFace ONNX export format</li> <li>No extra native libraries beyond ONNX Runtime</li> <li>Full control over the generation pipeline (sampling, stop sequences, token streaming)</li> </ul> <p>Cons:</p> <ul> <li>The generation loop runs in Java rather than optimized C++, so it's slightly slower per token</li> </ul>"},{"location":"generative-ai/introduction/#onnxruntime-genai-inference4j-genai","title":"onnxruntime-genai (inference4j-genai)","text":"<p>onnxruntime-genai is a native library by Microsoft that handles the entire generation pipeline in optimized C++ \u2014 tokenization, the generation loop, KV cache, and sampling. inference4j wraps this library via the <code>inference4j-genai</code> module.</p> <p>Pros:</p> <ul> <li>The entire loop (including tokenization and KV cache) runs in native C++, maximizing throughput</li> <li>Supports multimodal models (Phi-3.5 Vision) where image preprocessing is handled natively</li> </ul> <p>Cons:</p> <ul> <li>Models must be exported in onnxruntime-genai's specific format \u2014 few are available today</li> <li>The library is in preview and community support is limited; Microsoft's investment appears to have slowed</li> <li>Requires a separate native dependency (<code>onnxruntime-genai</code>) that we build and publish ourselves since Microsoft does not currently publish Java bindings to Maven Central</li> <li>GPU support is not available in the Java bindings</li> </ul> <p>onnxruntime-genai is experimental</p> <p>The <code>inference4j-genai</code> module wraps a library in preview. We maintain the onnxruntime-genai Java build ourselves. The API may change between releases.</p>"},{"location":"generative-ai/introduction/#where-were-heading","title":"Where we're heading","text":"<p>The native generation approach is the future. It unlocks any ONNX model on HuggingFace that exports with KV cache support \u2014 hundreds of models \u2014 without depending on a third-party native library. It supports both decoder-only models (GPT-2, SmolLM2, Qwen2.5, Gemma 2, TinyLlama) and encoder-decoder models (Flan-T5, BART, MarianMT, CoEdIT).</p> <p>The onnxruntime-genai path remains valuable for models that need native multimodal preprocessing (like Phi-3.5 Vision) and for users who prefer the optimized C++ loop.</p>"},{"location":"generative-ai/introduction/#the-autoregressive-loop","title":"The autoregressive loop","text":"<p>A generative model doesn't produce its entire output in a single forward pass. It produces one token at a time. Each token is fed back into the model to produce the next one, forming a loop that continues until the model emits a stop token or reaches a maximum length.</p> <pre><code>flowchart TD\n    A[\"Prompt tokens\"] --&gt; B[\"Forward pass\"]\n    B --&gt; C[\"Next token\"]\n    C --&gt; D{\"Stop token?\"}\n    D -- No --&gt; B\n    D -- Yes --&gt; E[\"Complete text\"]</code></pre> <p>If you ask a model \"What is Java?\" and it generates a 50-token answer, the model runs 50 forward passes \u2014 one for each token in the response. This is why generation is orders of magnitude slower than classification or embedding.</p>"},{"location":"generative-ai/introduction/#the-kv-cache","title":"The KV cache","text":"<p>There's a problem with the naive loop above. Each forward pass computes attention over all previous tokens. Without optimization, generating token 50 would recompute attention over all 49 previous tokens from scratch \u2014 the same work done for tokens 1 through 49, repeated entirely.</p> <p>The KV cache (key-value cache) solves this. During each forward pass, the model caches the intermediate key and value tensors from the attention layers. On the next pass, only the new token's attention needs to be computed \u2014 everything from previous tokens is read from the cache. This turns generation from O(n^2^) to O(n) in sequence length.</p>"},{"location":"generative-ai/introduction/#encoder-decoder-models","title":"Encoder-decoder models","text":"<p>The models described above (GPT-2, SmolLM2, Qwen2.5) are decoder-only \u2014 they process the entire input and output as a single sequence. Encoder-decoder models split the work into two parts:</p> <ol> <li>Encoder: processes the full input in a single forward pass, producing a rich representation of the input text</li> <li>Decoder: generates the output one token at a time, attending to the encoder's representation via cross-attention</li> </ol> <p>This architecture is a natural fit for tasks where the input and output are structurally different \u2014 summarization (long article \u2192 short summary), translation (English \u2192 French), and grammar correction (broken text \u2192 fixed text).</p> <pre><code>flowchart TD\n    A[\"Input text\"] --&gt; B[\"Encoder&lt;br&gt;&lt;small&gt;single forward pass&lt;/small&gt;\"]\n    B --&gt; C[\"Encoder output&lt;br&gt;&lt;small&gt;frozen representation&lt;/small&gt;\"]\n    C --&gt; D[\"Decoder step 1&lt;br&gt;&lt;small&gt;cross-attention to encoder&lt;/small&gt;\"]\n    D --&gt; E[\"Token 1\"]\n    E --&gt; F[\"Decoder step 2\"]\n    F --&gt; G[\"Token 2\"]\n    G --&gt; H[\"...\"]\n    H --&gt; I{\"Stop token?\"}\n    I -- No --&gt; J[\"Decoder step N\"]\n    I -- Yes --&gt; K[\"Complete output\"]</code></pre>"},{"location":"generative-ai/introduction/#two-types-of-kv-cache","title":"Two types of KV cache","text":"<p>Encoder-decoder models maintain two separate caches:</p> <ul> <li>Cross-attention cache \u2014 computed once from the encoder output after the first decoder step, then frozen for the rest of generation. This is what lets the decoder \"look at\" the input without recomputing it.</li> <li>Self-attention cache \u2014 grows with each decoder step, just like in decoder-only models. This cache stores the decoder's own previous states.</li> </ul> <p>This split is the key architectural difference from decoder-only models, where there is only one KV cache that grows throughout generation.</p>"},{"location":"generative-ai/introduction/#how-the-two-approaches-differ","title":"How the two approaches differ","text":"<pre><code>flowchart LR\n    subgraph native[\"Native generation (inference4j-core)\"]\n        direction LR\n        N1[\"BPE tokenize&lt;br&gt;&lt;small&gt;inference4j&lt;/small&gt;\"]\n        N2[\"Forward pass + KV cache&lt;br&gt;&lt;small&gt;ONNX Runtime&lt;/small&gt;\"]\n        N3[\"Sample + decode&lt;br&gt;&lt;small&gt;inference4j&lt;/small&gt;\"]\n        N1 --&gt; N2 --&gt; N3\n    end\n\n    subgraph genai[\"onnxruntime-genai (inference4j-genai)\"]\n        direction LR\n        G1[\"Tokenize + Generate loop + KV cache + Sampling + Decode&lt;br&gt;&lt;small&gt;onnxruntime-genai (C++)&lt;/small&gt;\"]\n    end</code></pre> <p>In native generation, inference4j handles tokenization, sampling, and decoding in Java while ONNX Runtime does the forward passes. In the genai path, the entire pipeline runs in onnxruntime-genai's native C++ layer.</p>"},{"location":"generative-ai/introduction/#supported-models","title":"Supported models","text":""},{"location":"generative-ai/introduction/#native-generation-inference4j-core_1","title":"Native generation (inference4j-core)","text":"Model Preset Model ID Parameters Size GPT-2 <code>OnnxTextGenerator.gpt2()</code> <code>inference4j/gpt2</code> 124M ~500 MB SmolLM2-360M-Instruct <code>OnnxTextGenerator.smolLM2()</code> <code>inference4j/smollm2-360m-instruct</code> 360M ~700 MB Qwen2.5-1.5B-Instruct <code>OnnxTextGenerator.qwen2()</code> <code>inference4j/qwen2.5-1.5b-instruct</code> 1.5B ~3 GB"},{"location":"generative-ai/introduction/#native-encoder-decoder-inference4j-core","title":"Native encoder-decoder (inference4j-core)","text":"Model Wrapper Default Model ID Parameters Size Flan-T5 Small <code>FlanT5TextGenerator</code> <code>inference4j/flan-t5-small</code> 77M ~300 MB Flan-T5 Base <code>FlanT5TextGenerator</code> <code>inference4j/flan-t5-base</code> 250M ~900 MB Flan-T5 Large <code>FlanT5TextGenerator</code> <code>inference4j/flan-t5-large</code> 780M ~3 GB DistilBART CNN 12-6 <code>BartSummarizer</code> <code>inference4j/distilbart-cnn-12-6</code> 306M ~1.2 GB BART Large CNN <code>BartSummarizer</code> <code>inference4j/bart-large-cnn</code> 406M ~1.6 GB MarianMT <code>MarianTranslator</code> User-specified (<code>inference4j/opus-mt-*</code>) varies varies CoEdIT Base <code>CoeditGrammarCorrector</code> <code>inference4j/coedit-base</code> 250M ~900 MB CoEdIT Large <code>CoeditGrammarCorrector</code> <code>inference4j/coedit-large</code> 780M ~3 GB T5-small-awesome-text-to-sql <code>T5SqlGenerator</code> <code>inference4j/t5-small-awesome-text-to-sql</code> 60M ~240 MB T5-LM-Large-text2sql-spider <code>T5SqlGenerator</code> <code>inference4j/T5-LM-Large-text2sql-spider</code> 0.8B ~4.6 GB"},{"location":"generative-ai/introduction/#onnxruntime-genai-inference4j-genai_1","title":"onnxruntime-genai (inference4j-genai)","text":"Model Wrapper Model ID Parameters Size Phi-3 Mini 4K Instruct <code>TextGenerator</code> <code>inference4j/phi-3-mini-4k-instruct</code> 3.8B ~2.7 GB DeepSeek-R1-Distill-Qwen-1.5B <code>TextGenerator</code> <code>inference4j/deepseek-r1-distill-qwen-1.5b</code> 1.5B ~1 GB Whisper Small <code>WhisperSpeechModel</code> <code>inference4j/whisper-small-genai</code> \u2014 ~500 MB Phi-3.5 Vision Instruct <code>VisionLanguageModel</code> <code>inference4j/phi-3.5-vision-instruct</code> 4.2B ~3.3 GB <p>All models are hosted on the inference4j HuggingFace org and downloaded automatically on first use.</p>"},{"location":"generative-ai/introduction/#next-steps","title":"Next steps","text":"<ul> <li>Native Text Generation \u2014 GPT-2, SmolLM2, Qwen2.5 via OnnxTextGenerator (decoder-only)</li> <li>Summarization \u2014 BartSummarizer, FlanT5TextGenerator (encoder-decoder)</li> <li>Translation \u2014 MarianTranslator, FlanT5TextGenerator (encoder-decoder)</li> <li>Grammar Correction \u2014 CoeditGrammarCorrector, FlanT5TextGenerator (encoder-decoder)</li> <li>Text-to-SQL \u2014 FlanT5TextGenerator (encoder-decoder)</li> <li>Chat Templates \u2014 how prompt formatting works across models</li> <li>Text Generation (onnxruntime-genai) \u2014 Phi-3, DeepSeek-R1 via onnxruntime-genai</li> <li>Whisper Speech-to-Text \u2014 transcription and translation via onnxruntime-genai</li> <li>Phi-3.5 Vision \u2014 image description and visual Q&amp;A via onnxruntime-genai</li> </ul>"},{"location":"generative-ai/native-text-generation/","title":"Native Text Generation","text":"<p>Generate text with GPT-2, SmolLM2, TinyLlama, Qwen2.5, Gemma 2, and other models using inference4j's native generation loop \u2014 no additional dependencies beyond ONNX Runtime.</p> <p><code>OnnxTextGenerator</code> is the single entry point for all natively-supported text generation models. Named presets provide one-liner access to popular models, and the generic builder supports custom models.</p>"},{"location":"generative-ai/native-text-generation/#quick-example","title":"Quick example","text":"<pre><code>// GPT-2 \u2014 completion model\ntry (var gen = OnnxTextGenerator.gpt2().maxNewTokens(50).build()) {\n    System.out.println(gen.generate(\"Once upon a time\").text());\n}\n\n// SmolLM2-360M \u2014 ChatML instruct model\ntry (var gen = OnnxTextGenerator.smolLM2().maxNewTokens(50).build()) {\n    System.out.println(gen.generate(\"What is the capital of France?\").text());\n}\n\n// TinyLlama-1.1B-Chat \u2014 Zephyr-style instruct model\ntry (var gen = OnnxTextGenerator.tinyLlama().maxNewTokens(100).build()) {\n    System.out.println(gen.generate(\"Explain gravity\").text());\n}\n\n// Qwen2.5-1.5B \u2014 ChatML instruct model\ntry (var gen = OnnxTextGenerator.qwen2().maxNewTokens(100).build()) {\n    System.out.println(gen.generate(\"Explain gravity\").text());\n}\n</code></pre>"},{"location":"generative-ai/native-text-generation/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.nlp.OnnxTextGenerator;\n\npublic class TextGeneration {\n    public static void main(String[] args) {\n        try (var gen = OnnxTextGenerator.qwen2()\n                .maxNewTokens(100)\n                .temperature(0.8f)\n                .topK(50)\n                .topP(0.9f)\n                .build()) {\n\n            GenerationResult result = gen.generate(\"The meaning of life is\");\n\n            System.out.println(result.text());\n            System.out.printf(\"%d tokens in %,d ms%n\",\n                    result.generatedTokens(), result.duration().toMillis());\n        }\n    }\n}\n</code></pre> <p>Enable sampling for better output</p> <p>GPT-2 defaults to greedy decoding (<code>temperature=0</code>), which produces repetitive text. Set <code>temperature</code>, <code>topK</code>, and <code>topP</code> for more coherent output. Instruct models (SmolLM2, Qwen2.5) also benefit from sampling.</p>"},{"location":"generative-ai/native-text-generation/#streaming","title":"Streaming","text":"<p>Pass a <code>Consumer&lt;String&gt;</code> to receive tokens as they are generated:</p> <pre><code>try (var gen = OnnxTextGenerator.smolLM2()\n        .maxNewTokens(100)\n        .temperature(0.8f)\n        .topK(50)\n        .build()) {\n    gen.generate(\"Tell me a joke\", token -&gt; System.out.print(token));\n}\n</code></pre> <p>The final <code>GenerationResult</code> is still returned after generation completes, containing the full text and timing information.</p>"},{"location":"generative-ai/native-text-generation/#model-presets","title":"Model presets","text":"Preset Model Parameters Size Chat Template <code>OnnxTextGenerator.gpt2()</code> GPT-2 124M ~500 MB None (completion) <code>OnnxTextGenerator.smolLM2()</code> SmolLM2-360M-Instruct 360M ~700 MB ChatML <code>OnnxTextGenerator.tinyLlama()</code> TinyLlama-1.1B-Chat 1.1B ~2.2 GB Zephyr (<code>&lt;\\|user\\|&gt;</code> / <code>&lt;/s&gt;</code>) <code>OnnxTextGenerator.qwen2()</code> Qwen2.5-1.5B-Instruct 1.5B ~3 GB ChatML <code>OnnxTextGenerator.gemma2()</code> Gemma 2-2B-IT 2.6B ~5 GB <code>&lt;start_of_turn&gt;</code> / <code>&lt;end_of_turn&gt;</code> <p>Gated models</p> <p>Gemma 2 is a gated model \u2014 you must accept Google's license terms on HuggingFace before downloading. The <code>gemma2()</code> preset does not set a model ID for auto-download. Provide the model directory yourself via <code>modelSource()</code>:</p> <pre><code>try (var gen = OnnxTextGenerator.gemma2()\n        .modelSource(id -&gt; Path.of(\"/path/to/gemma-2-2b-it\"))\n        .maxNewTokens(100)\n        .build()) {\n    gen.generate(\"What is Java?\", token -&gt; System.out.print(token));\n}\n</code></pre>"},{"location":"generative-ai/native-text-generation/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> Preset-dependent HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> \u2014 ONNX Runtime session options (e.g., thread count) <code>.chatTemplate(ChatTemplate)</code> <code>ChatTemplate</code> Preset-dependent Prompt formatting <code>.addedToken(String)</code> <code>String</code> Preset-dependent Register a special token for atomic encoding <code>.tokenizerProvider(TokenizerProvider)</code> <code>TokenizerProvider</code> GPT-2 BPE Tokenizer construction strategy (e.g., SentencePiece for Gemma) <code>.maxNewTokens(int)</code> <code>int</code> <code>256</code> Maximum number of tokens to generate <code>.temperature(float)</code> <code>float</code> <code>0.0</code> Sampling temperature (higher = more random) <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling (keep K most probable tokens) <code>.topP(float)</code> <code>float</code> <code>0.0</code> (disabled) Nucleus sampling (keep tokens summing to P probability) <code>.eosTokenId(int)</code> <code>int</code> Auto-detected End-of-sequence token ID (loaded from <code>config.json</code>) <code>.stopSequence(String)</code> <code>String</code> \u2014 Stop sequence (can be called multiple times)"},{"location":"generative-ai/native-text-generation/#result-type","title":"Result type","text":"<p><code>GenerationResult</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The generated text <code>promptTokens()</code> <code>int</code> Number of tokens in the input prompt <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time"},{"location":"generative-ai/native-text-generation/#how-it-works","title":"How it works","text":"<p><code>OnnxTextGenerator</code> uses inference4j's native generation engine. The entire autoregressive loop \u2014 tokenization, KV cache management, sampling, and decoding \u2014 runs in Java, with only the forward passes delegated to ONNX Runtime.</p> <pre><code>flowchart TD\n    A[\"User prompt\"] --&gt; B[\"Tokenize&lt;br&gt;&lt;small&gt;inference4j&lt;/small&gt;\"]\n    B --&gt; C[\"Forward pass + KV cache&lt;br&gt;&lt;small&gt;ONNX Runtime&lt;/small&gt;\"]\n    C --&gt; D[\"Sample next token&lt;br&gt;&lt;small&gt;inference4j&lt;/small&gt;\"]\n    D --&gt; E{\"Stop?\"}\n    E -- No --&gt; C\n    E -- Yes --&gt; F[\"Decode tokens&lt;br&gt;&lt;small&gt;inference4j&lt;/small&gt;\"]\n    F --&gt; G[\"GenerationResult\"]</code></pre> <p>See the introduction for a detailed explanation of the autoregressive loop, KV cache, and how native generation compares to onnxruntime-genai.</p>"},{"location":"generative-ai/native-text-generation/#custom-models","title":"Custom models","text":"<p>Use <code>OnnxTextGenerator.builder()</code> for any BPE-based causal LM exported to ONNX with KV cache:</p> <pre><code>try (var gen = OnnxTextGenerator.builder()\n        .modelId(\"my-org/my-model\")\n        .addedToken(\"&lt;|special_start|&gt;\")\n        .addedToken(\"&lt;|special_end|&gt;\")\n        .chatTemplate(msg -&gt; \"&lt;|user|&gt;\" + msg + \"&lt;|assistant|&gt;\")\n        .temperature(0.7f)\n        .maxNewTokens(100)\n        .build()) {\n    gen.generate(\"Hello\", token -&gt; System.out.print(token));\n}\n</code></pre> <p>By default, the builder uses GPT-2-style BPE (<code>vocab.json</code> + <code>merges.txt</code>). For SentencePiece models (Gemma, LLaMA, TinyLlama), use <code>.tokenizerProvider(SentencePieceBpeTokenizer.provider())</code> which reads <code>tokenizer.json</code> instead.</p> <p>The model directory must contain <code>model.onnx</code> and <code>config.json</code>, plus the tokenizer files required by the provider.</p>"},{"location":"generative-ai/native-text-generation/#tips","title":"Tips","text":"<ul> <li>Use <code>temperature(0.8f)</code>, <code>topK(50)</code>, <code>topP(0.9f)</code> to avoid degenerate repetition from greedy decoding.</li> <li>Lower <code>maxNewTokens</code> for demos or quick tests \u2014 it directly controls how many forward passes run.</li> <li>Reuse <code>OnnxTextGenerator</code> instances across prompts \u2014 each one holds the model and tokenizer in memory.</li> <li>Models download on first use and are cached in <code>~/.cache/inference4j/</code>.</li> </ul>"},{"location":"generative-ai/phi-vision/","title":"Phi-3.5 Vision","text":"<p>Ask questions about images and get text answers using Microsoft's Phi-3.5 Vision model.</p> <p>See the overview for background on how autoregressive generation differs from single-pass inference.</p>"},{"location":"generative-ai/phi-vision/#quick-example","title":"Quick example","text":"<pre><code>try (var vision = VisionLanguageModel.builder()\n        .model(ModelSources.phi3Vision())\n        .build()) {\n    GenerationResult result = vision.generate(\n            new VisionInput(Path.of(\"cat.jpg\"), \"Describe this image.\"));\n    System.out.println(result.text());\n}\n</code></pre> <pre><code>The image shows a close-up of an orange tabby cat with a white collar.\nThe cat's eyes are green, and it has a neutral expression. The background\nis blurred, with hints of a red object and a green plant.\n</code></pre>"},{"location":"generative-ai/phi-vision/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.genai.ModelSources;\nimport io.github.inference4j.genai.vision.VisionInput;\nimport io.github.inference4j.genai.vision.VisionLanguageModel;\nimport java.nio.file.Path;\n\npublic class ImageDescription {\n    public static void main(String[] args) {\n        Path image = Path.of(\"cat.jpg\");\n\n        try (var vision = VisionLanguageModel.builder()\n                .model(ModelSources.phi3Vision())\n                .build()) {\n\n            // Describe an image\n            GenerationResult description = vision.generate(\n                    new VisionInput(image, \"Describe this image.\"));\n            System.out.println(description.text());\n\n            // Ask a question about it\n            GenerationResult answer = vision.generate(\n                    new VisionInput(image, \"What colors are prominent in this image?\"));\n            System.out.println(answer.text());\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"generative-ai/phi-vision/#streaming","title":"Streaming","text":"<p>Pass a <code>Consumer&lt;String&gt;</code> to receive tokens as they are generated:</p> <pre><code>try (var vision = VisionLanguageModel.builder()\n        .model(ModelSources.phi3Vision())\n        .build()) {\n    vision.generate(\n            new VisionInput(Path.of(\"cat.jpg\"), \"Describe this image.\"),\n            token -&gt; System.out.print(token));\n}\n</code></pre>"},{"location":"generative-ai/phi-vision/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.model(GenerativeModel)</code> <code>GenerativeModel</code> \u2014 Preconfigured model from <code>ModelSources</code> <code>.modelId(String)</code> <code>String</code> \u2014 HuggingFace model ID (requires <code>.chatTemplate()</code>) <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.chatTemplate(ChatTemplate)</code> <code>ChatTemplate</code> \u2014 Prompt formatting (must include <code>&lt;\\|image_1\\|&gt;</code> placeholder) <code>.maxLength(int)</code> <code>int</code> <code>4096</code> Maximum total sequence length (input image tokens + output tokens) <code>.temperature(double)</code> <code>double</code> <code>0.0</code> Sampling temperature (0 = greedy) <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(double)</code> <code>double</code> <code>0.0</code> (disabled) Nucleus sampling"},{"location":"generative-ai/phi-vision/#methods","title":"Methods","text":"Method Description <code>generate(VisionInput)</code> Generate text from an image and prompt <code>generate(VisionInput, Consumer)</code> Generate with token streaming"},{"location":"generative-ai/phi-vision/#how-it-works","title":"How it works","text":"<p>Phi-3.5 Vision is a decoder-only vision-language model. Unlike Whisper (encoder-decoder), it interleaves image tokens with text tokens in a single sequence:</p> <ol> <li>The image goes through a CLIP ViT encoder to produce visual embeddings</li> <li>An MLP projector maps visual embeddings into the language model's token space</li> <li>The Phi-3 Mini decoder processes interleaved image + text tokens autoregressively</li> </ol> <pre><code>flowchart LR\n    A[\"Image file\"] --&gt; B[\"onnxruntime-genai&lt;br&gt;CLIP encoder \u2192 projector \u2192 Phi-3 decoder\"]\n    C[\"Text prompt\"] --&gt; B\n    B --&gt; D[\"GenerationResult\"]</code></pre> <p>All heavy lifting \u2014 image preprocessing, vision encoding, embedding projection, autoregressive decoding, KV cache \u2014 is handled natively by onnxruntime-genai.</p>"},{"location":"generative-ai/phi-vision/#custom-models","title":"Custom models","text":"<p>For a custom vision-language model, provide a model source and chat template:</p> <pre><code>try (var vision = VisionLanguageModel.builder()\n        .modelId(\"my-org/my-vision-model\")\n        .chatTemplate(msg -&gt;\n                \"&lt;|user|&gt;\\n&lt;|image_1|&gt;\\n\" + msg + \"\\n&lt;|end|&gt;\\n&lt;|assistant|&gt;\\n\")\n        .build()) {\n    vision.generate(new VisionInput(Path.of(\"photo.jpg\"), \"Describe this image.\"));\n}\n</code></pre> <p>The chat template must include <code>&lt;|image_1|&gt;</code> where the image embeddings should be inserted. The <code>MultiModalProcessor</code> handles the substitution internally.</p>"},{"location":"generative-ai/phi-vision/#tips","title":"Tips","text":"<ul> <li><code>temperature(0.0)</code> (default) gives deterministic, greedy decoding \u2014 best for factual descriptions.</li> <li>Use higher <code>maxLength</code> for detailed descriptions and lower values for short answers.</li> <li>The model downloads ~3.3 GB on first use (INT4 quantized).</li> <li>Reuse <code>VisionLanguageModel</code> instances \u2014 each one holds three ONNX models in memory (vision encoder, embedding projector, text decoder).</li> </ul>"},{"location":"generative-ai/text-generation/","title":"Text Generation","text":"<p>Generate text with decoder-only language models like Phi-3 and DeepSeek-R1, with built-in streaming support.</p> <p>See the overview for background on how autoregressive generation differs from single-pass inference.</p>"},{"location":"generative-ai/text-generation/#quick-example","title":"Quick example","text":"<pre><code>try (var generator = TextGenerator.builder()\n        .model(ModelSources.phi3Mini())\n        .build()) {\n    System.out.println(generator.generate(\"What is Java?\").text());\n}\n</code></pre>"},{"location":"generative-ai/text-generation/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.genai.ModelSources;\nimport io.github.inference4j.genai.nlp.TextGenerator;\n\npublic class TextGeneration {\n    public static void main(String[] args) {\n        try (var generator = TextGenerator.builder()\n                .model(ModelSources.phi3Mini())\n                .maxLength(200)\n                .temperature(0.7)\n                .build()) {\n\n            GenerationResult result = generator.generate(\"What is Java in one sentence?\");\n\n            System.out.println(result.text());\n            System.out.printf(\"%d tokens in %,d ms%n\",\n                    result.generatedTokens(), result.duration().toMillis());\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"generative-ai/text-generation/#streaming","title":"Streaming","text":"<p>Pass a <code>Consumer&lt;String&gt;</code> to receive tokens as they are generated:</p> <pre><code>try (var generator = TextGenerator.builder()\n        .model(ModelSources.deepSeekR1_1_5B())\n        .maxLength(200)\n        .build()) {\n    generator.generate(\"Explain recursion in simple terms.\", token -&gt; System.out.print(token));\n}\n</code></pre> <p>The final <code>GenerationResult</code> is still returned after generation completes, containing the full text and timing information.</p>"},{"location":"generative-ai/text-generation/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.model(GenerativeModel)</code> <code>GenerativeModel</code> \u2014 Preconfigured model from <code>ModelSources</code> <code>.modelSource(ModelSource)</code> <code>ModelSource</code> \u2014 Custom model source (requires <code>.chatTemplate()</code>) <code>.chatTemplate(ChatTemplate)</code> <code>ChatTemplate</code> \u2014 Prompt formatting for custom models <code>.maxLength(int)</code> <code>int</code> <code>1024</code> Maximum number of tokens to generate <code>.temperature(double)</code> <code>double</code> <code>1.0</code> Sampling temperature (higher = more random) <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling (keep K most probable tokens) <code>.topP(double)</code> <code>double</code> <code>0.0</code> (disabled) Nucleus sampling (keep tokens summing to P probability)"},{"location":"generative-ai/text-generation/#result-type","title":"Result type","text":"<p><code>GenerationResult</code> is a record with three fields:</p> Field Type Description <code>text()</code> <code>String</code> The generated text <code>promptTokens()</code> <code>int</code> Number of tokens in the input prompt (0 if unknown) <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time"},{"location":"generative-ai/text-generation/#how-it-works","title":"How it works","text":"<p><code>TextGenerator</code> formats the prompt using the model's chat template, then delegates to onnxruntime-genai for tokenization, the autoregressive generation loop, and decoding.</p> <pre><code>flowchart LR\n    A[\"User prompt\"] --&gt; B[\"ChatTemplate&lt;br&gt;format prompt\"]\n    B --&gt; C[\"onnxruntime-genai&lt;br&gt;tokenize \u2192 generate \u2192 decode\"]\n    C --&gt; D[\"GenerationResult\"]</code></pre> <p>See the overview for a detailed explanation of the generation loop, KV cache, and why this architecture differs from single-pass wrappers.</p>"},{"location":"generative-ai/text-generation/#tips","title":"Tips","text":"<ul> <li>Generation speed scales with model size. DeepSeek-R1 (1.5B) is noticeably faster   than Phi-3 (3.8B) on the same hardware.</li> <li>Lower <code>maxLength</code> for short answers \u2014 it bounds the generation loop.</li> <li><code>temperature</code> below 1.0 gives more focused output; above 1.0 gives more varied output.</li> <li>Reuse <code>TextGenerator</code> instances across prompts \u2014 each one holds the model in memory.</li> </ul>"},{"location":"generative-ai/whisper/","title":"Whisper Speech-to-Text","text":"<p>Work in Progress</p> <p><code>WhisperSpeechModel</code> is implemented but the pre-exported model artifact (<code>inference4j/whisper-small-genai</code>) is not yet available on HuggingFace. The onnxruntime Whisper converter has compatibility issues with current Python package versions. This page documents the target API.</p> <p>Transcribe and translate speech using OpenAI's Whisper models, with automatic chunking for long audio.</p> <p>See the overview for background on how autoregressive generation differs from single-pass inference.</p>"},{"location":"generative-ai/whisper/#quick-example","title":"Quick example","text":"<pre><code>try (var whisper = WhisperSpeechModel.builder()\n        .modelId(\"inference4j/whisper-small-genai\")\n        .build()) {\n    System.out.println(whisper.transcribe(Path.of(\"meeting.wav\")).text());\n}\n</code></pre>"},{"location":"generative-ai/whisper/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.genai.audio.WhisperSpeechModel;\nimport io.github.inference4j.audio.Transcription;\nimport java.nio.file.Path;\n\npublic class WhisperTranscription {\n    public static void main(String[] args) {\n        try (var whisper = WhisperSpeechModel.builder()\n                .modelId(\"inference4j/whisper-small-genai\")\n                .build()) {\n\n            Transcription result = whisper.transcribe(Path.of(\"meeting.wav\"));\n            System.out.println(result.text());\n        }\n    }\n}\n</code></pre>"},{"location":"generative-ai/whisper/#translation","title":"Translation","text":"<p>Whisper can translate speech from any supported language into English:</p> <pre><code>try (var whisper = WhisperSpeechModel.builder()\n        .modelId(\"inference4j/whisper-small-genai\")\n        .language(\"fr\")\n        .task(WhisperTask.TRANSLATE)\n        .build()) {\n    Transcription result = whisper.transcribe(Path.of(\"french-audio.wav\"));\n    System.out.println(result.text());  // English translation\n}\n</code></pre>"},{"location":"generative-ai/whisper/#from-raw-audio-data","title":"From raw audio data","text":"<p>If you already have audio samples as a float array:</p> <pre><code>try (var whisper = WhisperSpeechModel.builder()\n        .modelId(\"inference4j/whisper-small-genai\")\n        .build()) {\n    float[] samples = loadAudioSamples();\n    Transcription result = whisper.transcribe(samples, 16000);\n    System.out.println(result.text());\n}\n</code></pre>"},{"location":"generative-ai/whisper/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> \u2014 HuggingFace model ID (required) <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.language(String)</code> <code>String</code> <code>\"en\"</code> Source language code (e.g., <code>\"fr\"</code>, <code>\"de\"</code>, <code>\"ja\"</code>) <code>.task(WhisperTask)</code> <code>WhisperTask</code> <code>TRANSCRIBE</code> <code>TRANSCRIBE</code> or <code>TRANSLATE</code> (to English) <code>.maxLength(int)</code> <code>int</code> <code>448</code> Maximum number of tokens to generate per chunk <code>.temperature(double)</code> <code>double</code> <code>0.0</code> Sampling temperature (0 = greedy) <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(double)</code> <code>double</code> <code>0.0</code> (disabled) Nucleus sampling"},{"location":"generative-ai/whisper/#result-type","title":"Result type","text":"<p><code>Transcription</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The transcribed or translated text <code>segments()</code> <code>List&lt;Segment&gt;</code> Timed segments (when available) <p>Each <code>Segment</code> contains:</p> Field Type Description <code>text()</code> <code>String</code> Segment text <code>startTime()</code> <code>float</code> Start time in seconds <code>endTime()</code> <code>float</code> End time in seconds"},{"location":"generative-ai/whisper/#auto-chunking","title":"Auto-chunking","text":"<p>Whisper processes audio in 30-second windows. For audio longer than 30 seconds, <code>WhisperSpeechModel</code> automatically:</p> <ol> <li>Splits the audio into 30-second chunks</li> <li>Transcribes each chunk independently</li> <li>Concatenates the results</li> </ol> <p>This happens transparently \u2014 just call <code>transcribe()</code> with any length audio.</p>"},{"location":"generative-ai/whisper/#how-it-works","title":"How it works","text":"<p>Unlike Wav2Vec2 (single-pass CTC), Whisper is an autoregressive encoder-decoder model. The audio is encoded into a mel spectrogram, then the decoder generates text tokens one at a time.</p> <pre><code>flowchart LR\n    A[\"Audio file\"] --&gt; B[\"onnxruntime-genai&lt;br&gt;mel spectrogram \u2192 encoder \u2192 decoder loop\"]\n    B --&gt; C[\"Transcription\"]</code></pre> <p>All heavy lifting \u2014 mel spectrogram computation, encoder forward pass, autoregressive decoding, KV cache, beam search \u2014 is handled natively by onnxruntime-genai's C++ layer.</p>"},{"location":"generative-ai/whisper/#whisper-vs-wav2vec2","title":"Whisper vs Wav2Vec2","text":"Wav2Vec2 Whisper Architecture CTC (single-pass) Encoder-decoder (autoregressive) Speed Fast Slower (token-by-token) Accuracy Good on clean speech Better on diverse audio Languages English (default model) 99 languages Translation No Yes (to English) Module <code>inference4j-core</code> <code>inference4j-genai</code>"},{"location":"generative-ai/whisper/#tips","title":"Tips","text":"<ul> <li>Use <code>WhisperTask.TRANSLATE</code> to translate any language to English in a single step.</li> <li>Smaller models (tiny, base) are faster but less accurate. The <code>small</code> model is a good balance.</li> <li><code>temperature(0.0)</code> (default) gives deterministic, greedy decoding \u2014 best for transcription accuracy.</li> <li>Reuse <code>WhisperSpeechModel</code> instances \u2014 each one holds the model in memory.</li> <li>For short, clean English audio where speed matters, Wav2Vec2 may be a better fit.</li> </ul>"},{"location":"getting-started/how-it-works/","title":"How It Works","text":""},{"location":"getting-started/how-it-works/#what-is-onnx","title":"What is ONNX?","text":"<p>ONNX (Open Neural Network Exchange) is an open standard for representing machine learning models. A model trained in any framework \u2014 PyTorch, TensorFlow, JAX \u2014 can be exported to a single <code>.onnx</code> file that captures the model's architecture and weights in a framework-independent format.</p> <p>ONNX Runtime is a high-performance engine that runs these <code>.onnx</code> files. It provides hardware acceleration (CPU, GPU, CoreML, CUDA) and has native bindings for many languages, including Java.</p> <p>inference4j builds on top of the ONNX Runtime Java API, adding preprocessing, postprocessing, and model management so you don't have to work with tensors directly.</p> <p>Useful links:</p> <ul> <li>ONNX specification</li> <li>ONNX Runtime documentation</li> <li>ONNX Runtime Java API</li> <li>HuggingFace ONNX models</li> </ul>"},{"location":"getting-started/how-it-works/#the-three-stage-pipeline","title":"The three-stage pipeline","text":"<p>Almost every inference task follows the same pattern:</p> <pre><code>flowchart LR\n    Input[\"Java Object&lt;br&gt;String, BufferedImage, Path\"]\n\n    subgraph inference4j[\"inference4j wrapper\"]\n        Pre[\"Preprocess\"]\n        Infer[\"Infer\"]\n        Post[\"Postprocess\"]\n        Pre --&gt; Infer --&gt; Post\n    end\n\n    Output[\"Java Object&lt;br&gt;List of Classification, Transcription, ...\"]\n\n    Input --&gt; Pre\n    Post --&gt; Output</code></pre>"},{"location":"getting-started/how-it-works/#1-preprocess","title":"1. Preprocess","text":"<p>Converts Java objects into tensors \u2014 the multi-dimensional arrays that models consume.</p> <p>What this looks like depends on the domain:</p> Domain Input Preprocessing NLP <code>String</code> Tokenize text \u2192 <code>input_ids</code> + <code>attention_mask</code> tensors Vision <code>BufferedImage</code> Resize, normalize pixels, rearrange channels \u2192 float tensor Audio <code>Path</code> (WAV file) Load samples, resample to 16kHz, normalize \u2192 float tensor <p>This is where most of the model-specific knowledge lives: normalization constants, tokenizer vocabulary, expected input dimensions, channel layout (NCHW vs NHWC).</p>"},{"location":"getting-started/how-it-works/#2-infer","title":"2. Infer","text":"<p>A single forward pass through the ONNX model via ONNX Runtime. Tensors in, tensors out. This stage is model-agnostic \u2014 ONNX Runtime handles it regardless of which framework trained the model.</p>"},{"location":"getting-started/how-it-works/#3-postprocess","title":"3. Postprocess","text":"<p>Converts raw output tensors back into meaningful Java objects.</p> Task Raw output Postprocessing Result Classification Float array (logits) Softmax \u2192 top-K \u2192 label mapping <code>List&lt;Classification&gt;</code> Object detection Float array (boxes + scores) NMS \u2192 coordinate rescaling \u2192 label mapping <code>List&lt;Detection&gt;</code> Speech-to-text Float array (per-frame logits) CTC greedy decode \u2192 character joining <code>Transcription</code> Text detection Pixel-level heatmap Thresholding \u2192 connected components \u2192 bounding boxes <code>List&lt;TextRegion&gt;</code>"},{"location":"getting-started/how-it-works/#what-is-a-tensor","title":"What is a Tensor?","text":"<p>A tensor is a multi-dimensional array of numbers. If you're familiar with Java arrays:</p> <ul> <li>A <code>float</code> is a 0-dimensional tensor (a scalar)</li> <li>A <code>float[]</code> is a 1-dimensional tensor (a vector)</li> <li>A <code>float[][]</code> is a 2-dimensional tensor (a matrix)</li> <li>A <code>float[][][]</code> is a 3-dimensional tensor</li> </ul> <p>ML models consume and produce tensors with specific shapes. For example, a ResNet image classification model expects an input tensor with shape <code>[1, 3, 224, 224]</code> \u2014 that's 1 image, 3 color channels (RGB), 224 pixels tall, 224 pixels wide. The shape convention (<code>[batch, channels, height, width]</code> vs <code>[batch, height, width, channels]</code>) depends on the model's origin framework.</p> <p>inference4j handles all tensor creation and interpretation internally. You never need to construct a tensor yourself.</p>"},{"location":"getting-started/how-it-works/#example-what-happens-when-you-classify-an-image","title":"Example: what happens when you classify an image","text":"<pre><code>try (var classifier = ResNetClassifier.builder().build()) {\n    List&lt;Classification&gt; results = classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre> <p>Behind that single <code>classify()</code> call:</p> <pre><code>flowchart TD\n    A[\"cat.jpg (Path)\"]\n\n    subgraph preprocess [\"Preprocess\"]\n        B[\"Load BufferedImage\"]\n        C[\"Resize to 224x224\"]\n        D[\"Normalize pixels&lt;br&gt;mean: 0.485, 0.456, 0.406&lt;br&gt;std: 0.229, 0.224, 0.225\"]\n        E[\"Arrange into NCHW layout&lt;br&gt;shape: 1, 3, 224, 224\"]\n        B --&gt; C --&gt; D --&gt; E\n    end\n\n    subgraph infer [\"Infer\"]\n        F[\"ONNX Runtime forward pass\"]\n        G[\"Raw logits \u2014 shape: 1, 1000\"]\n        F --&gt; G\n    end\n\n    subgraph postprocess [\"Postprocess\"]\n        H[\"Softmax to probabilities\"]\n        I[\"Top-K selection\"]\n        J[\"Map indices to ImageNet labels\"]\n        H --&gt; I --&gt; J\n    end\n\n    K[\"List of Classification&lt;br&gt;tabby cat: 0.87, ...\"]\n\n    A --&gt; B\n    E --&gt; F\n    G --&gt; H\n    J --&gt; K</code></pre> <p>All of this \u2014 the resizing, normalization, channel layout, softmax, label mapping \u2014 is encoded in the <code>ResNetClassifier</code> wrapper. A different model (say, EfficientNet) has different normalization constants, a different input layout (NHWC instead of NCHW), and may or may not need softmax applied. Each wrapper knows its model's requirements.</p>"},{"location":"getting-started/how-it-works/#beyond-single-pass-autoregressive-generation","title":"Beyond single-pass: autoregressive generation","text":"<p>The three-stage pipeline above covers single-pass models \u2014 those that produce their entire output in one forward pass. Classification, embedding, detection, and speech-to-text all follow this pattern.</p> <p>Text generation works differently. A generative model produces output one token at a time, feeding each token back into the model to produce the next one. This autoregressive loop, along with KV cache management and token sampling, requires a fundamentally different architecture that doesn't fit the preprocess-infer-postprocess pipeline.</p> <p>inference4j handles this in two ways:</p> <ul> <li><code>OnnxTextGenerator</code> (in <code>inference4j-core</code>) \u2014 a pure ONNX Runtime generation loop with KV cache management, sampling, and streaming. Supports GPT-2, SmolLM2, Qwen2.5, and custom BPE-based models. No additional dependencies.</li> <li><code>inference4j-genai</code> \u2014 backed by onnxruntime-genai for larger models (Phi-3, DeepSeek-R1, Phi-3.5 Vision). See the Generative AI section for details.</li> </ul>"},{"location":"getting-started/how-it-works/#next-steps","title":"Next steps","text":"<ul> <li>Browse use cases to see the single-pass wrappers in action</li> <li>Generative AI for autoregressive text generation</li> <li>Adding a Wrapper explains the internals for contributors</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Java 17 or higher</li> <li>ONNX Runtime (included transitively)</li> </ul>"},{"location":"getting-started/installation/#add-the-dependency","title":"Add the dependency","text":"<p><code>inference4j-core</code> is the only dependency you need \u2014 it includes all task wrappers, preprocessing, and tokenizers.</p> GradleMaven <pre><code>implementation 'io.github.inference4j:inference4j-core:${inference4jVersion}'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-core&lt;/artifactId&gt;\n    &lt;version&gt;${inference4jVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/installation/#generative-ai","title":"Generative AI","text":"<p>For text generation (Phi-3, DeepSeek-R1, etc.), add <code>inference4j-genai</code> instead:</p> GradleMaven <pre><code>implementation 'io.github.inference4j:inference4j-genai'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-genai&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>This is a separate module backed by onnxruntime-genai. See the Generative AI guide for details.</p>"},{"location":"getting-started/installation/#jvm-flags","title":"JVM flags","text":"<p>ONNX Runtime requires native access. Add this flag to your JVM arguments:</p> <pre><code>--enable-native-access=ALL-UNNAMED\n</code></pre> <p>Or, if you're on the module path:</p> <pre><code>--enable-native-access=com.microsoft.onnxruntime\n</code></pre>"},{"location":"getting-started/installation/#setting-jvm-flags-in-gradle","title":"Setting JVM flags in Gradle","text":"<pre><code>tasks.withType(JavaExec).configureEach {\n    jvmArgs '--enable-native-access=ALL-UNNAMED'\n}\n\ntasks.withType(Test).configureEach {\n    jvmArgs '--enable-native-access=ALL-UNNAMED'\n}\n</code></pre>"},{"location":"getting-started/installation/#spring-boot","title":"Spring Boot","text":"<p>For Spring Boot applications, use the starter instead:</p> GradleMaven <pre><code>implementation 'io.github.inference4j:inference4j-spring-boot-starter:${inference4jVersion}'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-spring-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;${inference4jVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>See the Spring Boot guide for configuration details.</p>"},{"location":"getting-started/installation/#gpu-support","title":"GPU support","text":"<p>The default dependency includes CPU and CoreML (macOS) support. For CUDA (Linux/Windows), swap the ONNX Runtime dependency:</p> GradleMaven <pre><code>implementation('io.github.inference4j:inference4j-core:${inference4jVersion}') {\n    exclude group: 'com.microsoft.onnxruntime', module: 'onnxruntime'\n}\nimplementation 'com.microsoft.onnxruntime:onnxruntime_gpu:${onnxruntimeVersion}'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-core&lt;/artifactId&gt;\n    &lt;version&gt;${inference4jVersion}&lt;/version&gt;\n    &lt;exclusions&gt;\n        &lt;exclusion&gt;\n            &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n            &lt;artifactId&gt;onnxruntime&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n    &lt;/exclusions&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n    &lt;artifactId&gt;onnxruntime_gpu&lt;/artifactId&gt;\n    &lt;version&gt;${onnxruntimeVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>See the Hardware Acceleration guide for usage details.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#your-first-model-in-3-lines","title":"Your first model in 3 lines","text":"<pre><code>import io.github.inference4j.nlp.DistilBertTextClassifier;\n\npublic class QuickStart {\n    public static void main(String[] args) {\n        try (var classifier = DistilBertTextClassifier.builder().build()) {\n            System.out.println(classifier.classify(\"inference4j makes AI in Java easy!\"));\n            // [TextClassification[label=POSITIVE, confidence=0.9998]]\n        }\n    }\n}\n</code></pre> <p>That's it. The model downloads automatically on first run (~260MB, cached in <code>~/.cache/inference4j/</code>). No Python, no manual downloads, no tensor wrangling.</p>"},{"location":"getting-started/quickstart/#what-happens-under-the-hood","title":"What happens under the hood","text":"<p>When you call <code>builder().build()</code>:</p> <ol> <li>Model resolution \u2014 The builder resolves the default model ID (<code>inference4j/distilbert-base-uncased-finetuned-sst-2-english</code>) using <code>HuggingFaceModelSource</code>, which downloads the ONNX model, vocabulary, and config files to <code>~/.cache/inference4j/</code></li> <li>Session creation \u2014 An ONNX Runtime <code>InferenceSession</code> is created with default session options</li> <li>Tokenizer setup \u2014 A <code>WordPieceTokenizer</code> is loaded from the downloaded <code>vocab.txt</code></li> </ol> <p>When you call <code>classify(text)</code>:</p> <ol> <li>Preprocessing \u2014 The text is tokenized into input IDs and attention mask tensors</li> <li>Inference \u2014 Tensors are passed to the ONNX Runtime session for a single forward pass</li> <li>Postprocessing \u2014 Raw logits are transformed via softmax into labeled classifications</li> </ol>"},{"location":"getting-started/quickstart/#try-another-domain","title":"Try another domain","text":""},{"location":"getting-started/quickstart/#vision","title":"Vision","text":"<pre><code>import io.github.inference4j.vision.ResNetClassifier;\n\ntry (var classifier = ResNetClassifier.builder().build()) {\n    var results = classifier.classify(Path.of(\"cat.jpg\"));\n    results.forEach(c -&gt;\n        System.out.printf(\"%s: %.2f%%%n\", c.label(), c.confidence() * 100));\n}\n</code></pre>"},{"location":"getting-started/quickstart/#audio","title":"Audio","text":"<pre><code>import io.github.inference4j.audio.Wav2Vec2Recognizer;\n\ntry (var recognizer = Wav2Vec2Recognizer.builder().build()) {\n    System.out.println(recognizer.transcribe(Path.of(\"speech.wav\")).text());\n}\n</code></pre>"},{"location":"getting-started/quickstart/#customizing-the-builder","title":"Customizing the builder","text":"<p>Every model wrapper follows the same builder pattern:</p> <pre><code>try (var classifier = ResNetClassifier.builder()\n        .modelId(\"inference4j/resnet50-v1-7\")     // override default model\n        .sessionOptions(opts -&gt; opts.addCoreML())  // hardware acceleration\n        .defaultTopK(3)                            // return top 3 predictions\n        .build()) {\n    classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre> <p>Common builder methods available on all wrappers:</p> Method Description <code>.modelId(String)</code> Override the default HuggingFace model ID <code>.modelSource(ModelSource)</code> Use a custom model source (e.g., <code>LocalModelSource</code>) <code>.sessionOptions(SessionConfigurer)</code> Configure ONNX Runtime options (GPU, threads)"},{"location":"getting-started/quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Browse the Use Cases for detailed examples of each capability</li> <li>See Hardware Acceleration for GPU and CoreML setup</li> <li>Check Supported Models for all available models</li> </ul>"},{"location":"guides/hardware-acceleration/","title":"Hardware Acceleration","text":"<p>inference4j supports GPU and hardware acceleration via ONNX Runtime execution providers. The <code>.sessionOptions()</code> API is available on every model wrapper.</p>"},{"location":"guides/hardware-acceleration/#coreml-macos","title":"CoreML (macOS)","text":"<p>CoreML is bundled in the standard ONNX Runtime dependency on macOS \u2014 no additional setup needed.</p> <pre><code>try (var classifier = ResNetClassifier.builder()\n        .sessionOptions(opts -&gt; opts.addCoreML())\n        .build()) {\n    classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre>"},{"location":"guides/hardware-acceleration/#cuda-linuxwindows","title":"CUDA (Linux/Windows)","text":"<p>For NVIDIA GPU acceleration, swap the ONNX Runtime dependency:</p> GradleMaven <pre><code>implementation('io.github.inference4j:inference4j-core:${inference4jVersion}') {\n    exclude group: 'com.microsoft.onnxruntime', module: 'onnxruntime'\n}\nimplementation 'com.microsoft.onnxruntime:onnxruntime_gpu:${onnxruntimeVersion}'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-core&lt;/artifactId&gt;\n    &lt;version&gt;${inference4jVersion}&lt;/version&gt;\n    &lt;exclusions&gt;\n        &lt;exclusion&gt;\n            &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n            &lt;artifactId&gt;onnxruntime&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n    &lt;/exclusions&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;\n    &lt;artifactId&gt;onnxruntime_gpu&lt;/artifactId&gt;\n    &lt;version&gt;${onnxruntimeVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then enable CUDA in the builder:</p> <pre><code>try (var classifier = ResNetClassifier.builder()\n        .sessionOptions(opts -&gt; opts.addCUDA(0))  // device ID 0\n        .build()) {\n    classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre>"},{"location":"guides/hardware-acceleration/#the-sessionoptions-api","title":"The <code>sessionOptions</code> API","text":"<p>Every model wrapper exposes <code>.sessionOptions(SessionConfigurer)</code> in its builder. <code>SessionConfigurer</code> is a <code>@FunctionalInterface</code> that receives the ONNX Runtime <code>SessionOptions</code>:</p> <pre><code>@FunctionalInterface\npublic interface SessionConfigurer {\n    void configure(OrtSession.SessionOptions options) throws OrtException;\n}\n</code></pre> <p>This gives you full access to ONNX Runtime configuration:</p> <pre><code>.sessionOptions(opts -&gt; {\n    opts.addCoreML();\n    opts.setIntraOpNumThreads(4);\n})\n</code></pre>"},{"location":"guides/hardware-acceleration/#common-options","title":"Common options","text":"Method Description <code>opts.addCoreML()</code> Enable CoreML (macOS) <code>opts.addCUDA(deviceId)</code> Enable CUDA (Linux/Windows) <code>opts.setIntraOpNumThreads(n)</code> Set number of threads for intra-op parallelism <code>opts.setInterOpNumThreads(n)</code> Set number of threads for inter-op parallelism <code>opts.setOptimizationLevel(level)</code> Set graph optimization level"},{"location":"guides/hardware-acceleration/#benchmarks-on-apple-silicon-m-series","title":"Benchmarks on Apple Silicon (M-series)","text":"Model Capability CPU CoreML Speedup ResNet-50 Image Classification 37 ms 10 ms 3.7x CRAFT Text Detection 831 ms 153 ms 5.4x <p>Measured with 3 warmup runs + 10 timed runs.</p>"},{"location":"guides/hardware-acceleration/#tips","title":"Tips","text":"<ul> <li>CoreML is available on macOS only. The <code>addCoreML()</code> call will fail on other platforms.</li> <li>CUDA requires the <code>onnxruntime_gpu</code> artifact and a compatible NVIDIA driver + CUDA toolkit.</li> <li>If the execution provider fails to initialize, ONNX Runtime silently falls back to CPU. Check logs for warnings.</li> <li>For production workloads, benchmark both CPU and GPU \u2014 small models (like MiniLM) may be faster on CPU due to GPU data transfer overhead.</li> <li><code>.sessionOptions()</code> is composable \u2014 you can set multiple options in a single lambda.</li> </ul>"},{"location":"guides/model-loading/","title":"Model Loading","text":"<p>inference4j resolves models through the <code>ModelSource</code> interface. By default, models are downloaded from HuggingFace and cached locally. You can also load models from disk or implement your own resolution strategy.</p>"},{"location":"guides/model-loading/#modelsource-interface","title":"ModelSource interface","text":"<pre><code>@FunctionalInterface\npublic interface ModelSource {\n    Path resolve(String modelId);\n}\n</code></pre> <p><code>ModelSource</code> takes a model ID and returns the local directory path where the model files live. Every builder accepts <code>.modelSource(ModelSource)</code>.</p>"},{"location":"guides/model-loading/#localmodelsource","title":"LocalModelSource","text":"<p>Load models from a directory on disk:</p> <pre><code>var source = new LocalModelSource(Path.of(\"/models\"));\n\ntry (var classifier = DistilBertTextClassifier.builder()\n        .modelId(\"my-sentiment-model\")\n        .modelSource(source)\n        .build()) {\n    classifier.classify(\"Great product!\");\n}\n</code></pre> <p>This resolves to <code>/models/my-sentiment-model/</code>, which should contain the model files (<code>model.onnx</code>, <code>vocab.txt</code>, <code>config.json</code>, etc.).</p>"},{"location":"guides/model-loading/#huggingfacemodelsource","title":"HuggingFaceModelSource","text":"<p>The default model source. Downloads models from the HuggingFace Hub and caches them locally.</p> <pre><code>// Uses default cache directory (~/.cache/inference4j/)\nvar source = HuggingFaceModelSource.defaultInstance();\n\n// Or specify a custom cache directory\nvar source = new HuggingFaceModelSource(Path.of(\"/custom/cache\"));\n</code></pre>"},{"location":"guides/model-loading/#cache-directory-resolution","title":"Cache directory resolution","text":"<p>The cache directory is resolved in this order:</p> <ol> <li>Constructor parameter (<code>new HuggingFaceModelSource(path)</code>)</li> <li>System property: <code>-Dinference4j.cache.dir=/path/to/cache</code></li> <li>Environment variable: <code>INFERENCE4J_CACHE_DIR=/path/to/cache</code></li> <li>Default: <code>~/.cache/inference4j/</code></li> </ol>"},{"location":"guides/model-loading/#downloaded-files","title":"Downloaded files","text":"<p><code>HuggingFaceModelSource</code> attempts to download these files (skipping any that don't exist):</p> <ul> <li><code>model.onnx</code> \u2014 the ONNX model</li> <li><code>vocab.txt</code> \u2014 WordPiece vocabulary (NLP models)</li> <li><code>vocab.json</code> \u2014 JSON vocabulary (audio models)</li> <li><code>config.json</code> \u2014 HuggingFace model config</li> <li><code>labels.txt</code> \u2014 class labels (vision models)</li> <li><code>silero_vad.onnx</code> \u2014 alternate model filename (Silero VAD)</li> </ul>"},{"location":"guides/model-loading/#lambda-shorthand","title":"Lambda shorthand","text":"<p>Since <code>ModelSource</code> is a <code>@FunctionalInterface</code>, you can use a lambda:</p> <pre><code>try (var classifier = ResNetClassifier.builder()\n        .modelId(\"resnet50\")\n        .modelSource(id -&gt; Path.of(\"/models\").resolve(id))\n        .build()) {\n    classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre>"},{"location":"guides/model-loading/#required-model-files-by-task","title":"Required model files by task","text":"Task Required files Text classification <code>model.onnx</code>, <code>vocab.txt</code>, <code>config.json</code> (with <code>id2label</code>) Text embeddings <code>model.onnx</code>, <code>vocab.txt</code> Search reranking <code>model.onnx</code>, <code>vocab.txt</code> Image classification <code>model.onnx</code>, <code>labels.txt</code> Object detection <code>model.onnx</code>, <code>labels.txt</code> Text detection <code>model.onnx</code> Speech-to-text <code>model.onnx</code>, <code>vocab.json</code> Voice activity detection <code>silero_vad.onnx</code>"},{"location":"guides/model-loading/#using-a-custom-huggingface-model","title":"Using a custom HuggingFace model","text":"<p>Any ONNX-exported model hosted on HuggingFace works \u2014 just override the model ID:</p> <pre><code>try (var classifier = DistilBertTextClassifier.builder()\n        .modelId(\"your-org/your-fine-tuned-model\")\n        .build()) {\n    classifier.classify(\"Some text\");\n}\n</code></pre> <p>The model repository must contain the required files listed above.</p>"},{"location":"guides/model-loading/#tips","title":"Tips","text":"<ul> <li><code>LocalModelSource</code> does no downloading \u2014 it expects files to already exist at the resolved path.</li> <li><code>HuggingFaceModelSource</code> is thread-safe. Concurrent requests for the same model wait for the first download to complete.</li> <li>Models are cached permanently. To re-download, delete the cached directory.</li> <li>The model ID is just a string \u2014 it can be any identifier meaningful to your <code>ModelSource</code> implementation.</li> </ul>"},{"location":"guides/spring-boot/","title":"Spring Boot","text":"<p>inference4j provides a Spring Boot starter with opt-in auto-configuration for all supported models.</p>"},{"location":"guides/spring-boot/#setup","title":"Setup","text":"<p>Add the starter dependency:</p> GradleMaven <pre><code>implementation 'io.github.inference4j:inference4j-spring-boot-starter:${inference4jVersion}'\n</code></pre> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.github.inference4j&lt;/groupId&gt;\n    &lt;artifactId&gt;inference4j-spring-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;${inference4jVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"guides/spring-boot/#configuration","title":"Configuration","text":"<p>Every model is opt-in \u2014 nothing is downloaded until you set <code>enabled: true</code>.</p> <pre><code>inference4j:\n  nlp:\n    text-classifier:\n      enabled: true\n</code></pre>"},{"location":"guides/spring-boot/#all-properties","title":"All properties","text":"Property Type Default Description <code>inference4j.nlp.text-classifier.enabled</code> <code>boolean</code> <code>false</code> Enable DistilBERT text classifier <code>inference4j.nlp.text-classifier.model-id</code> <code>String</code> <code>inference4j/distilbert-base-uncased-finetuned-sst-2-english</code> Model ID <code>inference4j.nlp.text-embedder.enabled</code> <code>boolean</code> <code>false</code> Enable SentenceTransformer embedder <code>inference4j.nlp.text-embedder.model-id</code> <code>String</code> (required) Model ID (no default \u2014 must be specified) <code>inference4j.nlp.search-reranker.enabled</code> <code>boolean</code> <code>false</code> Enable MiniLM cross-encoder reranker <code>inference4j.nlp.search-reranker.model-id</code> <code>String</code> <code>inference4j/ms-marco-MiniLM-L-6-v2</code> Model ID <code>inference4j.vision.image-classifier.enabled</code> <code>boolean</code> <code>false</code> Enable ResNet image classifier <code>inference4j.vision.image-classifier.model-id</code> <code>String</code> <code>inference4j/resnet50-v1-7</code> Model ID <code>inference4j.vision.object-detector.enabled</code> <code>boolean</code> <code>false</code> Enable YOLOv8 object detector <code>inference4j.vision.object-detector.model-id</code> <code>String</code> <code>inference4j/yolov8n</code> Model ID <code>inference4j.vision.text-detector.enabled</code> <code>boolean</code> <code>false</code> Enable CRAFT text detector <code>inference4j.vision.text-detector.model-id</code> <code>String</code> <code>inference4j/craft-mlt-25k</code> Model ID <code>inference4j.audio.speech-recognizer.enabled</code> <code>boolean</code> <code>false</code> Enable Wav2Vec2 speech recognizer <code>inference4j.audio.speech-recognizer.model-id</code> <code>String</code> <code>inference4j/wav2vec2-base-960h</code> Model ID <code>inference4j.audio.vad.enabled</code> <code>boolean</code> <code>false</code> Enable Silero VAD <code>inference4j.audio.vad.model-id</code> <code>String</code> <code>inference4j/silero-vad</code> Model ID"},{"location":"guides/spring-boot/#usage","title":"Usage","text":"<p>Beans are registered by their interface type, so you inject the interface \u2014 not the concrete implementation:</p> <pre><code>@RestController\npublic class SentimentController {\n    private final TextClassifier classifier;\n\n    public SentimentController(TextClassifier classifier) {\n        this.classifier = classifier;\n    }\n\n    @PostMapping(\"/analyze\")\n    public List&lt;TextClassification&gt; analyze(@RequestBody String text) {\n        return classifier.classify(text);\n    }\n}\n</code></pre>"},{"location":"guides/spring-boot/#overriding-beans","title":"Overriding beans","text":"<p>All auto-configured beans use <code>@ConditionalOnMissingBean</code>, so you can replace any model with your own implementation:</p> <pre><code>@Configuration\npublic class CustomModelConfig {\n\n    @Bean\n    public TextClassifier textClassifier() {\n        return DistilBertTextClassifier.builder()\n            .modelId(\"your-org/your-model\")\n            .sessionOptions(opts -&gt; opts.addCoreML())\n            .build();\n    }\n}\n</code></pre> <p>When you define your own bean, the auto-configured one is skipped.</p>"},{"location":"guides/spring-boot/#health-indicator","title":"Health indicator","text":"<p>An actuator health indicator is included automatically when Spring Boot Actuator is on the classpath. It reports the number of registered <code>InferenceTask</code> beans.</p> <pre><code>GET /actuator/health\n</code></pre> <pre><code>{\n  \"status\": \"UP\",\n  \"components\": {\n    \"inference4j\": {\n      \"status\": \"UP\",\n      \"details\": {\n        \"tasks\": 3\n      }\n    }\n  }\n}\n</code></pre> <p>The health indicator is enabled by default. Disable it with:</p> <pre><code>inference4j:\n  health:\n    enabled: false\n</code></pre>"},{"location":"guides/spring-boot/#example-configuration","title":"Example configuration","text":"<p>A typical production setup enabling sentiment analysis and semantic search:</p> <pre><code>inference4j:\n  nlp:\n    text-classifier:\n      enabled: true\n    text-embedder:\n      enabled: true\n      model-id: inference4j/all-MiniLM-L6-v2\n    search-reranker:\n      enabled: true\n</code></pre> <pre><code>@Service\npublic class SearchService {\n    private final TextEmbedder embedder;\n    private final SearchReranker reranker;\n\n    public SearchService(TextEmbedder embedder, SearchReranker reranker) {\n        this.embedder = embedder;\n        this.reranker = reranker;\n    }\n\n    public List&lt;SearchResult&gt; search(String query, List&lt;String&gt; candidates) {\n        // Stage 1: embed and retrieve\n        float[] queryEmb = embedder.encode(query);\n        // ... cosine similarity ranking ...\n\n        // Stage 2: rerank top candidates\n        float[] scores = reranker.scoreBatch(query, topCandidates);\n        // ... sort by score ...\n        return results;\n    }\n}\n</code></pre>"},{"location":"guides/spring-boot/#lazy-loading","title":"Lazy loading","text":"<p>All inference4j beans are lazy by default \u2014 models are downloaded and ONNX sessions are created on first use, not at application startup. This keeps startup fast even when multiple models are enabled.</p> <p>The trade-off is that the first request to each model may be slower due to the model download and session initialization.</p>"},{"location":"guides/spring-boot/#warming-up-specific-models","title":"Warming up specific models","text":"<p>If you need a model ready immediately (e.g., for latency-sensitive endpoints), inject it eagerly at startup with an <code>ApplicationReadyEvent</code> listener:</p> <pre><code>@Component\npublic class ModelWarmup {\n    private final TextClassifier classifier;\n\n    public ModelWarmup(TextClassifier classifier) {\n        this.classifier = classifier;\n    }\n\n    @EventListener(ApplicationReadyEvent.class)\n    public void warmup() {\n        classifier.classify(\"warmup\");\n    }\n}\n</code></pre> <p>This triggers the lazy bean initialization during startup, so the model is ready when the first real request arrives.</p>"},{"location":"guides/spring-boot/#tips","title":"Tips","text":"<ul> <li>Use <code>model-id</code> to swap models without changing code (e.g., switch from ResNet to EfficientNet).</li> <li>The text embedder has no default model ID \u2014 you must specify one via <code>model-id</code>.</li> <li>All beans implement <code>AutoCloseable</code> and are properly cleaned up on application shutdown.</li> </ul>"},{"location":"guides/tokenizers/","title":"Tokenizers","text":"<p>Transformer models don't work with raw text \u2014 they expect sequences of integer token IDs mapped from a fixed vocabulary. A tokenizer bridges this gap: it splits text into tokens, maps each token to its vocabulary index, and produces the metadata tensors (<code>attention_mask</code>, <code>token_type_ids</code>) that the model expects as input.</p> <p>inference4j handles tokenization automatically. When you call <code>classifier.classify(\"some text\")</code>, the wrapper tokenizes the input, runs inference, and decodes the output \u2014 you never touch a token ID. But if you need to customize tokenization or use tokenizers directly, this guide covers how.</p>"},{"location":"guides/tokenizers/#built-in-tokenizers","title":"Built-in tokenizers","text":"<p>inference4j ships five tokenizer implementations, covering the most common algorithms in production transformer models:</p> Tokenizer Algorithm Models Vocabulary format <code>WordPieceTokenizer</code> WordPiece (greedy longest-match subword splitting) BERT, DistilBERT, MiniLM, SentenceTransformer <code>vocab.txt</code> (one token per line) <code>BpeTokenizer</code> Byte-level BPE (iterative pair merging) CLIP <code>vocab.json</code> + <code>merges.txt</code> <code>DecodingBpeTokenizer</code> Byte-level BPE with decode support GPT-2, SmolLM2, Qwen2.5, BART <code>vocab.json</code> + <code>merges.txt</code> <code>SentencePieceBpeTokenizer</code> SentencePiece BPE (Unicode-native, <code>\u2581</code> space prefix) Gemma, LLaMA, TinyLlama, MarianMT <code>tokenizer.json</code> <code>UnigramTokenizer</code> SentencePiece Unigram (Viterbi optimal segmentation) Flan-T5, CoEdIT, T5SqlGenerator <code>tokenizer.json</code>"},{"location":"guides/tokenizers/#wordpiece","title":"WordPiece","text":"<p>WordPiece breaks unknown words into known subword units using a <code>##</code> continuation prefix. For example, <code>\"unbelievable\"</code> becomes <code>[\"un\", \"##believ\", \"##able\"]</code> \u2014 preserving meaning even for out-of-vocabulary words.</p> <p>The encoding pipeline:</p> <ol> <li>Lowercase and split on whitespace/punctuation</li> <li>Split each word into subwords via greedy longest-match against the vocabulary</li> <li>Wrap with <code>[CLS]</code> and <code>[SEP]</code> special tokens</li> <li>Truncate to <code>maxLength</code> if needed</li> </ol> <pre><code>Tokenizer tokenizer = WordPieceTokenizer.fromVocabFile(Path.of(\"vocab.txt\"));\nEncodedInput encoded = tokenizer.encode(\"Hello world!\", 128);\n// encoded.inputIds()      \u2192 [101, 7592, 2088, 999, 102]\n// encoded.attentionMask() \u2192 [1, 1, 1, 1, 1]\n// encoded.tokenTypeIds()  \u2192 [0, 0, 0, 0, 0]\n</code></pre> <p>WordPiece also supports sentence pair encoding for cross-encoder models (e.g., rerankers):</p> <pre><code>EncodedInput encoded = tokenizer.encode(\"What is Java?\", \"Java is a programming language.\", 128);\n// Format: [CLS] textA [SEP] textB [SEP]\n// tokenTypeIds: 0 for textA tokens, 1 for textB tokens\n</code></pre> <p>Note</p> <p>The built-in <code>WordPieceTokenizer</code> applies unconditional lowercasing, matching <code>bert-base-uncased</code> and <code>distilbert-base-uncased</code>. It is not suitable for cased models.</p>"},{"location":"guides/tokenizers/#byte-pair-encoding-bpe","title":"Byte-Pair Encoding (BPE)","text":"<p>BPE starts from individual characters and iteratively merges the most frequent pairs into subwords. CLIP's variant adds byte-level encoding (handling any UTF-8 input) and <code>&lt;/w&gt;</code> end-of-word markers.</p> <p>The encoding pipeline:</p> <ol> <li>Lowercase and normalize whitespace</li> <li>Split via regex into words, contractions, digits, and punctuation</li> <li>Encode each byte via GPT-2's byte-to-unicode table</li> <li>Apply BPE merges according to the priority table</li> <li>Wrap with <code>&lt;|startoftext|&gt;</code> and <code>&lt;|endoftext|&gt;</code> special tokens</li> <li>Pad to <code>maxLength</code> (default: 77 for CLIP)</li> </ol> <pre><code>Tokenizer tokenizer = BpeTokenizer.fromFiles(\n        Path.of(\"vocab.json\"), Path.of(\"merges.txt\"));\nEncodedInput encoded = tokenizer.encode(\"a photo of a cat\");\n// encoded.inputIds()      \u2192 [49406, 320, 1125, 539, 320, 2368, 49407, 0, ...]\n// encoded.attentionMask() \u2192 [1, 1, 1, 1, 1, 1, 1, 0, ...]\n</code></pre>"},{"location":"guides/tokenizers/#decoding-bpe","title":"Decoding BPE","text":"<p><code>DecodingBpeTokenizer</code> extends <code>BpeTokenizer</code> with the ability to decode token IDs back to text. This is required by generative models that produce token IDs as output \u2014 autoregressive (GPT-2, SmolLM2, Qwen2.5) and encoder-decoder (BART).</p> <p>The encoding pipeline is the same as BPE. The decoding pipeline:</p> <ol> <li>Reverse vocabulary lookup (ID \u2192 token string)</li> <li>Concatenate tokens</li> <li>Decode GPT-2 byte-to-unicode mapping back to raw bytes</li> <li>Interpret bytes as UTF-8</li> </ol> <pre><code>DecodingBpeTokenizer tokenizer = DecodingBpeTokenizer.fromFiles(\n        Path.of(\"vocab.json\"), Path.of(\"merges.txt\"));\n\n// Encode\nEncodedInput encoded = tokenizer.encode(\"Hello world\");\n\n// Decode\nString text = tokenizer.decode(new int[]{15496, 995}); // \"Hello world\"\n\n// Single token (for streaming)\nString fragment = tokenizer.decode(15496); // \"Hello\"\n</code></pre>"},{"location":"guides/tokenizers/#sentencepiece-bpe","title":"SentencePiece BPE","text":"<p>SentencePiece BPE operates directly on Unicode text without a pre-tokenization regex. Word boundaries are encoded using the <code>\u2581</code> (U+2581) space prefix, and characters not in the vocabulary fall back to <code>&lt;0xNN&gt;</code> byte tokens.</p> <p>The encoding pipeline:</p> <ol> <li>Prepend <code>\u2581</code> and replace all spaces with <code>\u2581</code></li> <li>Split on special tokens (added tokens preserved atomically)</li> <li>Split into characters and apply BPE merges</li> <li>Characters not in vocab \u2192 UTF-8 bytes \u2192 <code>&lt;0xNN&gt;</code> token IDs</li> </ol> <p><code>SentencePieceBpeTokenizer</code> is used automatically by <code>OnnxTextGenerator.tinyLlama()</code>, <code>OnnxTextGenerator.gemma2()</code>, and <code>MarianTranslator</code>. For custom SentencePiece models, use the <code>TokenizerProvider</code>:</p> <pre><code>try (var gen = OnnxTextGenerator.builder()\n        .modelId(\"my-org/my-sentencepiece-model\")\n        .tokenizerProvider(SentencePieceBpeTokenizer.provider())\n        .chatTemplate(msg -&gt; \"&lt;start&gt;\" + msg + \"&lt;end&gt;\")\n        .build()) {\n    gen.generate(\"Hello\", token -&gt; System.out.print(token));\n}\n</code></pre>"},{"location":"guides/tokenizers/#unigram","title":"Unigram","text":"<p>The Unigram algorithm assigns a log-probability score to every token in the vocabulary and uses dynamic programming (Viterbi) to find the segmentation that maximizes the total score. This is used by T5-family models (Flan-T5, CoEdIT, T5SqlGenerator).</p> <p>The encoding pipeline:</p> <ol> <li>Prepend <code>\u2581</code> and replace all spaces with <code>\u2581</code></li> <li>Split on added tokens (special tokens preserved atomically)</li> <li>Run Viterbi to find the optimal segmentation</li> <li>Unmapped characters \u2192 UTF-8 bytes \u2192 <code>&lt;0xNN&gt;</code> token IDs</li> </ol> <p><code>UnigramTokenizer</code> is used automatically by <code>FlanT5TextGenerator</code>, <code>CoeditGrammarCorrector</code>, and <code>T5SqlGenerator</code>. It reads vocabulary and scores from <code>tokenizer.json</code>.</p>"},{"location":"guides/tokenizers/#default-behavior","title":"Default behavior","text":"<p>You don't need to configure tokenizers for standard use. Every NLP wrapper auto-loads the correct tokenizer from the model directory during <code>.build()</code>:</p> <pre><code>// WordPiece loaded automatically from vocab.txt\ntry (var classifier = DistilBertTextClassifier.builder()\n        .modelId(\"inference4j/distilbert-base-uncased-finetuned-sst-2-english\")\n        .build()) {\n    classifier.classify(\"This movie was fantastic!\");\n}\n\n// BPE loaded automatically from vocab.json + merges.txt\ntry (var classifier = ClipClassifier.builder().build()) {\n    classifier.classify(Path.of(\"photo.jpg\"), List.of(\"cat\", \"dog\", \"bird\"));\n}\n</code></pre> <p>The wrapper knows which tokenizer algorithm its model expects and which vocabulary files to look for.</p>"},{"location":"guides/tokenizers/#supplying-a-custom-tokenizer","title":"Supplying a custom tokenizer","text":"<p>All NLP builders expose a <code>.tokenizer()</code> method that lets you override the default:</p> <pre><code>Tokenizer myTokenizer = WordPieceTokenizer.fromVocabFile(Path.of(\"/path/to/my/vocab.txt\"));\n\ntry (var embedder = SentenceTransformerEmbedder.builder()\n        .modelId(\"my-custom-model\")\n        .tokenizer(myTokenizer)\n        .build()) {\n    float[] embedding = embedder.encode(\"Hello, world!\");\n}\n</code></pre> <p>When you provide a tokenizer, the wrapper skips auto-loading and uses yours directly.</p>"},{"location":"guides/tokenizers/#when-to-use-a-custom-tokenizer","title":"When to use a custom tokenizer","text":"<p>Shared instances \u2014 if you're running multiple wrappers against the same vocabulary, share a single tokenizer instance to avoid loading the vocabulary file multiple times:</p> <pre><code>Tokenizer shared = WordPieceTokenizer.fromVocabFile(Path.of(\"vocab.txt\"));\n\ntry (var embedder = SentenceTransformerEmbedder.builder()\n            .modelId(\"my-model\").tokenizer(shared).build();\n     var classifier = DistilBertTextClassifier.builder()\n            .modelId(\"my-model\").tokenizer(shared).build()) {\n    // Both use the same tokenizer instance\n}\n</code></pre> <p>Custom vocabulary \u2014 if you've fine-tuned a model with a modified vocabulary, point the tokenizer at your custom <code>vocab.txt</code>:</p> <pre><code>Tokenizer tokenizer = WordPieceTokenizer.fromVocabFile(Path.of(\"my-custom-vocab.txt\"));\ntry (var classifier = DistilBertTextClassifier.builder()\n        .tokenizer(tokenizer)\n        .modelSource(LocalModelSource.of(Path.of(\"my-finetuned-model\")))\n        .build()) {\n    classifier.classify(\"custom domain text\");\n}\n</code></pre> <p>Testing \u2014 supply a mock or stub tokenizer in unit tests to isolate inference logic from tokenization:</p> <pre><code>Tokenizer stub = text -&gt; new EncodedInput(\n    new long[]{101, 7592, 102},\n    new long[]{1, 1, 1},\n    new long[]{0, 0, 0}\n);\n</code></pre>"},{"location":"guides/tokenizers/#the-encodedinput-record","title":"The <code>EncodedInput</code> record","text":"<p>All tokenizers return an <code>EncodedInput</code> containing the three standard tensors that transformer models expect:</p> <pre><code>public record EncodedInput(\n    long[] inputIds,       // token IDs from the vocabulary\n    long[] attentionMask,  // 1 for real tokens, 0 for padding\n    long[] tokenTypeIds    // segment IDs (0 for first sentence, 1 for second)\n) {}\n</code></pre> Field Purpose Example <code>inputIds</code> Maps each token to its vocabulary index <code>[101, 7592, 2088, 102]</code> <code>attentionMask</code> Tells the model which positions are real tokens vs padding <code>[1, 1, 1, 1]</code> <code>tokenTypeIds</code> Distinguishes sentence A from sentence B in pair tasks <code>[0, 0, 0, 0]</code>"},{"location":"guides/tokenizers/#tips","title":"Tips","text":"<ul> <li>You almost never need to touch tokenizers. The default auto-loading handles standard HuggingFace models out of the box.</li> <li>Tokenizer and model must match. A tokenizer trained on one vocabulary will produce wrong token IDs for a model trained on a different vocabulary. Always use the tokenizer that shipped with your model.</li> <li><code>maxLength</code> matters. Most BERT models use 512 tokens max. CLIP uses 77. Exceeding the model's trained length produces undefined results. The wrappers set this automatically.</li> <li>WordPiece does not pad, BPE does. WordPiece returns only the actual tokens (variable length). BPE pads to <code>maxLength</code> with zeros. Both behaviors match what their respective model families expect.</li> </ul>"},{"location":"reference/api-overview/","title":"API Overview","text":""},{"location":"reference/api-overview/#package-structure","title":"Package structure","text":""},{"location":"reference/api-overview/#inference4j-core","title":"inference4j-core","text":"Package Contents <code>io.github.inference4j</code> Core contracts: <code>InferenceTask</code>, <code>Classifier</code>, <code>Detector</code>, <code>ZeroShotClassifier</code>, <code>ZeroShotInput</code>, <code>AbstractInferenceTask</code>, <code>Tensor</code>, <code>TensorType</code>, <code>InferenceSession</code>, <code>InferenceContext</code> <code>io.github.inference4j.session</code> Session config: <code>SessionConfigurer</code>, <code>SessionOptions</code> <code>io.github.inference4j.model</code> Model resolution: <code>ModelSource</code>, <code>HuggingFaceModelSource</code>, <code>LocalModelSource</code> <code>io.github.inference4j.processing</code> Pre/post-processing: <code>Preprocessor</code>, <code>Postprocessor</code>, <code>OutputOperator</code>, <code>MathOps</code> <code>io.github.inference4j.exception</code> Custom exceptions: <code>ModelLoadException</code>, <code>InferenceException</code> <code>io.github.inference4j.tokenizer</code> <code>Tokenizer</code>, <code>EncodedInput</code>, <code>WordPieceTokenizer</code>, <code>BpeTokenizer</code>, <code>DecodingBpeTokenizer</code>, <code>SentencePieceBpeTokenizer</code>, <code>UnigramTokenizer</code>, <code>TokenDecoder</code> <code>io.github.inference4j.preprocessing.text</code> <code>ModelConfig</code> (HuggingFace config.json parser) <code>io.github.inference4j.preprocessing.image</code> Image transforms pipeline: <code>ImageTransformPipeline</code>, <code>ResizeTransform</code>, <code>CenterCropTransform</code>, <code>ImageLayout</code>, <code>Labels</code> <code>io.github.inference4j.preprocessing.audio</code> <code>AudioTransformPipeline</code>, <code>AudioTransform</code>, <code>AudioData</code>, <code>AudioLoader</code>, <code>AudioWriter</code>, <code>AudioProcessor</code> <code>io.github.inference4j.vision</code> <code>ResNetClassifier</code>, <code>EfficientNetClassifier</code>, <code>YoloV8Detector</code>, <code>Yolo26Detector</code>, <code>CraftTextDetector</code>, <code>ImageEmbedder</code>, <code>ImageAnnotator</code> <code>io.github.inference4j.audio</code> <code>Wav2Vec2Recognizer</code>, <code>SileroVadDetector</code> <code>io.github.inference4j.nlp</code> <code>DistilBertTextClassifier</code>, <code>SentenceTransformerEmbedder</code>, <code>MiniLMSearchReranker</code>, <code>OnnxTextGenerator</code>, <code>FlanT5TextGenerator</code>, <code>BartSummarizer</code>, <code>MarianTranslator</code>, <code>CoeditGrammarCorrector</code>, <code>T5SqlGenerator</code>, <code>TextGenerator</code>, <code>Summarizer</code>, <code>Translator</code>, <code>GrammarCorrector</code>, <code>SqlGenerator</code>, <code>Language</code>, <code>PoolingStrategy</code>, <code>QueryDocumentPair</code> <code>io.github.inference4j.multimodal</code> <code>ClipClassifier</code>, <code>ClipImageEncoder</code>, <code>ClipTextEncoder</code> <code>io.github.inference4j.generation</code> <code>GenerativeTask</code>, <code>GenerationEngine</code>, <code>GenerationResult</code>, <code>GenerativeSession</code>, <code>EncoderDecoderSession</code>, <code>ChatTemplate</code>, <code>GenerativeModel</code> <code>io.github.inference4j.sampling</code> <code>LogitsProcessor</code>, <code>LogitsSampler</code>, <code>CategoricalSampler</code>, <code>GreedySampler</code>"},{"location":"reference/api-overview/#inference4j-genai","title":"inference4j-genai","text":"Package Contents <code>io.github.inference4j.genai</code> <code>AbstractGenerativeTask</code>, <code>ModelSources</code> <code>io.github.inference4j.genai.nlp</code> <code>TextGenerator</code> (onnxruntime-genai backed) <code>io.github.inference4j.genai.audio</code> <code>WhisperSpeechModel</code>, <code>WhisperTask</code> <code>io.github.inference4j.genai.vision</code> <code>VisionLanguageModel</code>, <code>VisionInput</code>"},{"location":"reference/api-overview/#inference4j-runtime","title":"inference4j-runtime","text":"Package Contents <code>io.github.inference4j.routing</code> <code>ModelRouter</code>, <code>Route</code>, <code>RoutingStrategy</code>, <code>WeightedRoutingStrategy</code>, <code>RouterMetrics</code>"},{"location":"reference/api-overview/#inference4j-spring-boot-starter","title":"inference4j-spring-boot-starter","text":"Package Contents <code>io.github.inference4j.autoconfigure</code> Auto-configuration, health indicators, <code>Inference4jProperties</code>"},{"location":"reference/api-overview/#interface-hierarchy","title":"Interface hierarchy","text":"<pre><code>InferenceTask&lt;I, O&gt;                     // run(I) \u2192 O, extends AutoCloseable\n\u251c\u2500\u2500 Classifier&lt;I, C&gt;                    // classify(I) \u2192 List&lt;C&gt;\n\u2502   \u251c\u2500\u2500 ImageClassifier                 // classify(BufferedImage/Path) \u2192 List&lt;Classification&gt;\n\u2502   \u2514\u2500\u2500 TextClassifier                  // classify(String) \u2192 List&lt;TextClassification&gt;\n\u251c\u2500\u2500 ZeroShotClassifier&lt;I, C&gt;            // classify(I, List&lt;String&gt;) \u2192 List&lt;C&gt;, run(ZeroShotInput&lt;I&gt;) \u2192 List&lt;C&gt;\n\u251c\u2500\u2500 Detector&lt;I, D&gt;                      // detect(I) \u2192 List&lt;D&gt;\n\u2502   \u251c\u2500\u2500 ObjectDetector                  // detect(BufferedImage/Path) \u2192 List&lt;Detection&gt;\n\u2502   \u251c\u2500\u2500 TextDetector                    // detect(BufferedImage/Path) \u2192 List&lt;TextRegion&gt;\n\u2502   \u2514\u2500\u2500 VoiceActivityDetector           // detect(Path/float[]) \u2192 List&lt;VoiceSegment&gt;\n\u251c\u2500\u2500 TextEmbedder                        // encode(String) \u2192 float[]\n\u251c\u2500\u2500 ImageEmbedder                       // encode(BufferedImage/Path) \u2192 float[]\n\u251c\u2500\u2500 SearchReranker                      // score(String, String) \u2192 float\n\u251c\u2500\u2500 SpeechRecognizer                    // transcribe(Path) \u2192 Transcription\n\u251c\u2500\u2500 TextGenerator                       // generate(String) \u2192 GenerationResult\n\u251c\u2500\u2500 Summarizer                          // summarize(String) \u2192 String\n\u251c\u2500\u2500 Translator                          // translate(String) \u2192 String\n\u251c\u2500\u2500 GrammarCorrector                    // correct(String) \u2192 String\n\u2514\u2500\u2500 SqlGenerator                        // generateSql(String, String) \u2192 String\n</code></pre>"},{"location":"reference/api-overview/#abstractinferencetask","title":"AbstractInferenceTask","text":"<p>Most task wrappers extend <code>AbstractInferenceTask&lt;I, O&gt;</code>, which enforces a <code>final run()</code> method with three stages:</p> <ol> <li>Preprocess: <code>I \u2192 Map&lt;String, Tensor&gt;</code> (via <code>Preprocessor</code>)</li> <li>Infer: <code>Map&lt;String, Tensor&gt; \u2192 Map&lt;String, Tensor&gt;</code> (via <code>InferenceSession</code>)</li> <li>Postprocess: <code>InferenceContext&lt;I&gt; \u2192 O</code> (via <code>Postprocessor</code>)</li> </ol> <p><code>InferenceContext&lt;I&gt;</code> carries data across stages \u2014 the original input, preprocessed tensors, and output tensors.</p> <p>Exceptions: <code>SileroVadDetector</code> (stateful hidden state), <code>MiniLMSearchReranker</code>, <code>ClipClassifier</code>, and <code>ClipTextEncoder</code> do not extend <code>AbstractInferenceTask</code>.</p>"},{"location":"reference/api-overview/#generative-ai-hierarchy","title":"Generative AI hierarchy","text":"<pre><code>GenerativeTask&lt;I, O&gt;                    // generate(I) \u2192 O, extends AutoCloseable\n\u2514\u2500\u2500 AbstractGenerativeTask&lt;I, O&gt;        // owns the autoregressive loop\n    \u251c\u2500\u2500 TextGenerator                   // generate(String) \u2192 GenerationResult\n    \u251c\u2500\u2500 WhisperSpeechModel              // transcribe(Path) \u2192 Transcription\n    \u2514\u2500\u2500 VisionLanguageModel             // generate(VisionInput) \u2192 GenerationResult\n</code></pre> <p><code>GenerativeTask</code> is the generative counterpart to <code>InferenceTask</code>. While <code>InferenceTask</code> performs a single forward pass, <code>GenerativeTask</code> runs an iterative generate loop backed by onnxruntime-genai. <code>AbstractGenerativeTask</code> owns the generate loop and provides hooks for subclasses: <code>prepareGenerator()</code>, <code>createStream()</code>, <code>parseOutput()</code>, and <code>createParams()</code> (for model-specific generation options like temperature and max length). See Generative AI for details.</p>"},{"location":"reference/api-overview/#result-types","title":"Result types","text":"Type Fields Used by <code>Classification</code> <code>label()</code>, <code>index()</code>, <code>confidence()</code> <code>ResNetClassifier</code>, <code>EfficientNetClassifier</code>, <code>ClipClassifier</code> <code>TextClassification</code> <code>label()</code>, <code>classIndex()</code>, <code>confidence()</code> <code>DistilBertTextClassifier</code> <code>Detection</code> <code>label()</code>, <code>classIndex()</code>, <code>confidence()</code>, <code>box()</code> <code>YoloV8Detector</code>, <code>Yolo26Detector</code> <code>TextRegion</code> <code>box()</code>, <code>confidence()</code> <code>CraftTextDetector</code> <code>BoundingBox</code> <code>x1()</code>, <code>y1()</code>, <code>x2()</code>, <code>y2()</code> Embedded in <code>Detection</code>, <code>TextRegion</code> <code>Transcription</code> <code>text()</code>, <code>segments()</code> <code>Wav2Vec2Recognizer</code>, <code>WhisperSpeechModel</code> <code>VoiceSegment</code> <code>start()</code>, <code>end()</code>, <code>duration()</code>, <code>confidence()</code> <code>SileroVadDetector</code> <code>GenerationResult</code> <code>text()</code>, <code>promptTokens()</code>, <code>generatedTokens()</code>, <code>duration()</code> <code>OnnxTextGenerator</code>, <code>FlanT5TextGenerator</code>, <code>BartSummarizer</code>, <code>MarianTranslator</code>, <code>CoeditGrammarCorrector</code>, <code>TextGenerator</code>, <code>VisionLanguageModel</code> <code>QueryDocumentPair</code> <code>query()</code>, <code>document()</code> <code>MiniLMSearchReranker</code>, <code>SearchReranker</code> <code>VisionInput</code> <code>imagePath()</code>, <code>prompt()</code> <code>VisionLanguageModel</code>"},{"location":"reference/api-overview/#builder-pattern","title":"Builder pattern","text":"<p>All task wrappers follow a consistent builder pattern:</p> <pre><code>try (var task = SomeWrapper.builder()\n        .modelId(\"org/model-name\")              // HuggingFace model ID\n        .modelSource(new LocalModelSource(...)) // optional: custom model source\n        .sessionOptions(opts -&gt; opts.addCoreML()) // optional: ONNX Runtime config\n        // ... task-specific options ...\n        .build()) {\n    task.run(input);\n}\n</code></pre> <p>The <code>.session(InferenceSession)</code> method exists on all builders but is package-private \u2014 the public API uses <code>modelId</code> + <code>modelSource</code> + <code>sessionOptions</code>.</p>"},{"location":"reference/api-overview/#functional-interfaces","title":"Functional interfaces","text":"Interface Signature Description <code>SessionConfigurer</code> <code>void configure(SessionOptions) throws OrtException</code> ONNX Runtime session customization <code>ModelSource</code> <code>Path resolve(String modelId)</code> Model file resolution <code>Preprocessor&lt;I, O&gt;</code> <code>O process(I input)</code> Input transformation <code>Postprocessor&lt;I, O&gt;</code> <code>O process(I input)</code> Output transformation <code>OutputOperator</code> <code>float[] apply(float[] values)</code> Activation function (softmax, sigmoid, identity) <code>ChatTemplate</code> <code>String format(String userMessage)</code> Prompt formatting for generative models"},{"location":"reference/clip-encoders/","title":"CLIP Encoders","text":"<p>Low-level access to CLIP's vision and text encoders for image-text similarity, visual search, and custom retrieval pipelines.</p> <p>For zero-shot classification as a single API, see Visual Search. For direct encoder access, see below.</p>"},{"location":"reference/clip-encoders/#clipimageencoder","title":"ClipImageEncoder","text":"<p>Maps images to 512-dimensional L2-normalized embeddings.</p> <pre><code>try (ClipImageEncoder encoder = ClipImageEncoder.builder().build()) {\n    float[] embedding = encoder.encode(ImageIO.read(Path.of(\"photo.jpg\").toFile()));\n    // 512-dim L2-normalized vector\n}\n</code></pre>"},{"location":"reference/clip-encoders/#batch-encoding","title":"Batch encoding","text":"<pre><code>List&lt;float[]&gt; embeddings = encoder.encodeBatch(images);\n</code></pre>"},{"location":"reference/clip-encoders/#builder-options","title":"Builder options","text":"Option Type Default Description <code>modelId(String)</code> <code>String</code> <code>inference4j/clip-vit-base-patch32</code> HuggingFace model ID <code>modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Where to load the model from <code>sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> Default (CPU) ONNX Runtime session options <code>preprocessor(Preprocessor)</code> <code>Preprocessor</code> CLIP pipeline (224\u00d7224, CLIP normalization) Custom image preprocessor"},{"location":"reference/clip-encoders/#preprocessing","title":"Preprocessing","text":"<ul> <li>Resize to 224\u00d7224, center crop</li> <li>CLIP normalization: mean <code>[0.48145466, 0.4578275, 0.40821073]</code>, std <code>[0.26862954, 0.26130258, 0.27577711]</code></li> <li>NCHW layout: <code>[1, 3, 224, 224]</code></li> </ul>"},{"location":"reference/clip-encoders/#cliptextencoder","title":"ClipTextEncoder","text":"<p>Maps text to 512-dimensional L2-normalized embeddings in the same vector space as <code>ClipImageEncoder</code>.</p> <pre><code>try (ClipTextEncoder encoder = ClipTextEncoder.builder().build()) {\n    float[] embedding = encoder.encode(\"a photo of a cat\");\n    // 512-dim L2-normalized vector\n}\n</code></pre>"},{"location":"reference/clip-encoders/#builder-options_1","title":"Builder options","text":"Option Type Default Description <code>modelId(String)</code> <code>String</code> <code>inference4j/clip-vit-base-patch32</code> HuggingFace model ID <code>modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Where to load the model from <code>sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> Default (CPU) ONNX Runtime session options <code>tokenizer(Tokenizer)</code> <code>Tokenizer</code> Auto-loaded BPE from model directory Custom tokenizer"},{"location":"reference/clip-encoders/#tokenization","title":"Tokenization","text":"<p>Uses byte-level BPE tokenization (<code>BpeTokenizer</code>) with CLIP's vocabulary. The tokenizer is automatically loaded from <code>vocab.json</code> and <code>merges.txt</code> in the model directory. Sequences are wrapped with BOS/EOS tokens and padded to 77 tokens.</p>"},{"location":"reference/clip-encoders/#image-text-similarity","title":"Image-text similarity","text":"<pre><code>try (ClipImageEncoder imageEncoder = ClipImageEncoder.builder().build();\n     ClipTextEncoder textEncoder = ClipTextEncoder.builder().build()) {\n\n    float[] imageEmb = imageEncoder.encode(ImageIO.read(Path.of(\"photo.jpg\").toFile()));\n    float[] textEmb = textEncoder.encode(\"a photo of a sunset\");\n\n    float similarity = MathOps.dotProduct(imageEmb, textEmb);\n    System.out.println(\"Similarity: \" + similarity);\n}\n</code></pre>"},{"location":"reference/clip-encoders/#image-search","title":"Image search","text":"<p>Index a collection of images, then query with text:</p> <pre><code>// Index: encode all images once\nList&lt;float[]&gt; imageEmbeddings = imageEncoder.encodeBatch(images);\n\n// Query: encode the search text\nfloat[] queryEmb = textEncoder.encode(\"a red sports car\");\n\n// Rank by similarity\nint bestIdx = 0;\nfloat bestScore = Float.NEGATIVE_INFINITY;\nfor (int i = 0; i &lt; imageEmbeddings.size(); i++) {\n    float score = MathOps.dotProduct(queryEmb, imageEmbeddings.get(i));\n    if (score &gt; bestScore) {\n        bestScore = score;\n        bestIdx = i;\n    }\n}\n</code></pre>"},{"location":"reference/clip-encoders/#dot-product-helper","title":"Dot product helper","text":"<p>Since both encoders produce L2-normalized vectors, the dot product equals cosine similarity. Use <code>MathOps.dotProduct()</code> from inference4j-core:</p> <pre><code>import io.github.inference4j.processing.MathOps;\n\nfloat similarity = MathOps.dotProduct(imageEmb, textEmb);\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":""},{"location":"reference/configuration/#model-cache","title":"Model cache","text":"<p>Models are downloaded from HuggingFace and cached locally. The cache directory is resolved in this order:</p> Priority Method Example 1 Constructor parameter <code>new HuggingFaceModelSource(Path.of(\"/cache\"))</code> 2 System property <code>-Dinference4j.cache.dir=/path/to/cache</code> 3 Environment variable <code>INFERENCE4J_CACHE_DIR=/path/to/cache</code> 4 Default <code>~/.cache/inference4j/</code>"},{"location":"reference/configuration/#jvm-flags","title":"JVM flags","text":"<p>ONNX Runtime requires native access:</p> <pre><code>--enable-native-access=ALL-UNNAMED\n</code></pre> <p>Or, on the module path:</p> <pre><code>--enable-native-access=com.microsoft.onnxruntime\n</code></pre>"},{"location":"reference/configuration/#system-properties","title":"System properties","text":"Property Description Default <code>inference4j.cache.dir</code> Model cache directory <code>~/.cache/inference4j/</code>"},{"location":"reference/configuration/#environment-variables","title":"Environment variables","text":"Variable Description Default <code>INFERENCE4J_CACHE_DIR</code> Model cache directory <code>~/.cache/inference4j/</code>"},{"location":"reference/configuration/#spring-boot-properties","title":"Spring Boot properties","text":"<p>See the Spring Boot guide for the full list of <code>inference4j.*</code> application properties.</p>"},{"location":"reference/configuration/#onnx-runtime-session-options","title":"ONNX Runtime session options","text":"<p>Session-level configuration is set via <code>.sessionOptions()</code> on each builder:</p> <pre><code>.sessionOptions(opts -&gt; {\n    opts.addCoreML();                                      // execution provider\n    opts.setIntraOpNumThreads(4);                          // parallelism\n    opts.setOptimizationLevel(SessionOptions.OptLevel.ALL_OPT); // graph optimization\n})\n</code></pre> <p>See the Hardware Acceleration guide for execution provider details.</p>"},{"location":"reference/supported-models/","title":"Supported Models","text":"<p>All supported models are hosted under the <code>inference4j</code> HuggingFace organization and are automatically downloaded and cached on first use.</p>"},{"location":"reference/supported-models/#nlp","title":"NLP","text":"Capability Wrapper Default Model ID Size API Text Classification <code>DistilBertTextClassifier</code> <code>inference4j/distilbert-base-uncased-finetuned-sst-2-english</code> ~260 MB <code>TextClassifier</code> Text Embeddings <code>SentenceTransformerEmbedder</code> <code>inference4j/all-MiniLM-L6-v2</code> ~90 MB <code>TextEmbedder</code> Search Reranking <code>MiniLMSearchReranker</code> <code>inference4j/ms-marco-MiniLM-L-6-v2</code> ~90 MB <code>SearchReranker</code> Text Generation <code>OnnxTextGenerator.gpt2()</code> <code>inference4j/gpt2</code> ~500 MB <code>TextGenerator</code> Text Generation <code>OnnxTextGenerator.smolLM2()</code> <code>inference4j/smollm2-360m-instruct</code> ~700 MB <code>TextGenerator</code> Text Generation <code>OnnxTextGenerator.smolLM2_1_7B()</code> <code>inference4j/smollm2-1.7b-instruct</code> ~3.4 GB <code>TextGenerator</code> Text Generation <code>OnnxTextGenerator.tinyLlama()</code> <code>inference4j/tinyllama-1.1b-chat</code> ~2.2 GB <code>TextGenerator</code> Text Generation <code>OnnxTextGenerator.qwen2()</code> <code>inference4j/qwen2.5-1.5b-instruct</code> ~3 GB <code>TextGenerator</code> Text Generation <code>OnnxTextGenerator.gemma2()</code> Gated \u2014 requires manual download ~5 GB <code>TextGenerator</code> Summarization / Translation / Grammar <code>FlanT5TextGenerator.flanT5Small()</code> <code>inference4j/flan-t5-small</code> ~300 MB <code>TextGenerator</code>, <code>Summarizer</code>, <code>Translator</code>, <code>GrammarCorrector</code> Summarization / Translation / Grammar <code>FlanT5TextGenerator.flanT5Base()</code> <code>inference4j/flan-t5-base</code> ~900 MB <code>TextGenerator</code>, <code>Summarizer</code>, <code>Translator</code>, <code>GrammarCorrector</code> Summarization / Translation / Grammar <code>FlanT5TextGenerator.flanT5Large()</code> <code>inference4j/flan-t5-large</code> ~3 GB <code>TextGenerator</code>, <code>Summarizer</code>, <code>Translator</code>, <code>GrammarCorrector</code> Text-to-SQL <code>T5SqlGenerator.t5SmallAwesome()</code> <code>inference4j/t5-small-awesome-text-to-sql</code> ~240 MB <code>TextGenerator</code>, <code>SqlGenerator</code> Text-to-SQL <code>T5SqlGenerator.t5LargeSpider()</code> <code>inference4j/T5-LM-Large-text2sql-spider</code> ~4.6 GB <code>TextGenerator</code>, <code>SqlGenerator</code> Summarization <code>BartSummarizer.distilBartCnn()</code> <code>inference4j/distilbart-cnn-12-6</code> ~1.2 GB <code>TextGenerator</code>, <code>Summarizer</code> Summarization <code>BartSummarizer.bartLargeCnn()</code> <code>inference4j/bart-large-cnn</code> ~1.6 GB <code>TextGenerator</code>, <code>Summarizer</code> Translation <code>MarianTranslator.builder()</code> User-specified (<code>inference4j/opus-mt-*</code>) varies <code>TextGenerator</code>, <code>Translator</code> Grammar Correction <code>CoeditGrammarCorrector.coeditBase()</code> <code>inference4j/coedit-base</code> ~900 MB <code>TextGenerator</code>, <code>GrammarCorrector</code> Grammar Correction <code>CoeditGrammarCorrector.coeditLarge()</code> <code>inference4j/coedit-large</code> ~3 GB <code>TextGenerator</code>, <code>GrammarCorrector</code>"},{"location":"reference/supported-models/#vision","title":"Vision","text":"Capability Wrapper Default Model ID Size API Image Classification <code>ResNetClassifier</code> <code>inference4j/resnet50-v1-7</code> ~100 MB <code>ImageClassifier</code> Image Classification <code>EfficientNetClassifier</code> <code>inference4j/efficientnet-lite4</code> ~50 MB <code>ImageClassifier</code> Object Detection <code>YoloV8Detector</code> <code>inference4j/yolov8n</code> ~25 MB <code>ObjectDetector</code> Object Detection <code>Yolo26Detector</code> <code>inference4j/yolo26n</code> ~25 MB <code>ObjectDetector</code> Text Detection <code>CraftTextDetector</code> <code>inference4j/craft-mlt-25k</code> ~80 MB <code>TextDetector</code>"},{"location":"reference/supported-models/#multimodal","title":"Multimodal","text":"Capability Wrapper Default Model ID Size API Zero-Shot Classification <code>ClipClassifier</code> <code>inference4j/clip-vit-base-patch32</code> ~595 MB <code>ZeroShotClassifier</code> Image Embeddings <code>ClipImageEncoder</code> <code>inference4j/clip-vit-base-patch32</code> ~340 MB <code>ImageEmbedder</code> Text Embeddings (CLIP) <code>ClipTextEncoder</code> <code>inference4j/clip-vit-base-patch32</code> ~255 MB <code>TextEmbedder</code>"},{"location":"reference/supported-models/#audio","title":"Audio","text":"Capability Wrapper Default Model ID Size API Speech-to-Text <code>Wav2Vec2Recognizer</code> <code>inference4j/wav2vec2-base-960h</code> ~370 MB <code>SpeechRecognizer</code> Voice Activity Detection <code>SileroVadDetector</code> <code>inference4j/silero-vad</code> ~2 MB <code>VoiceActivityDetector</code>"},{"location":"reference/supported-models/#generative-ai","title":"Generative AI","text":"<p>Generative models use a separate module (<code>inference4j-genai</code>) and a different builder pattern. See Generative AI for details.</p> Capability Wrapper Model ID Size License Text Generation <code>TextGenerator</code> <code>inference4j/phi-3-mini-4k-instruct</code> ~2.7 GB MIT Text Generation <code>TextGenerator</code> <code>inference4j/deepseek-r1-distill-qwen-1.5b</code> ~1 GB MIT Speech-to-Text / Translation (WIP) <code>WhisperSpeechModel</code> <code>inference4j/whisper-small-genai</code> ~500 MB MIT Vision-Language <code>VisionLanguageModel</code> <code>inference4j/phi-3.5-vision-instruct</code> ~3.3 GB MIT"},{"location":"reference/supported-models/#model-reference","title":"Model reference","text":"<p>A comprehensive view of all supported models, organized by architecture:</p>"},{"location":"reference/supported-models/#encoder-only-single-pass","title":"Encoder-only (single-pass)","text":"Model Tokenizer Wrapper Use Cases DistilBERT SST-2 WordPiece <code>DistilBertTextClassifier</code> Sentiment analysis, text classification all-MiniLM-L6-v2 WordPiece <code>SentenceTransformerEmbedder</code> Semantic search, embeddings MiniLM-L-6 MS MARCO WordPiece <code>MiniLMSearchReranker</code> Search reranking"},{"location":"reference/supported-models/#decoder-only-autoregressive","title":"Decoder-only (autoregressive)","text":"Model Tokenizer Wrapper Use Cases GPT-2 BPE <code>OnnxTextGenerator</code> Text completion SmolLM2-360M BPE <code>OnnxTextGenerator</code> Chat, instruction following TinyLlama-1.1B SentencePiece BPE <code>OnnxTextGenerator</code> Chat, instruction following Qwen2.5-1.5B BPE <code>OnnxTextGenerator</code> Chat, instruction following Gemma 2-2B SentencePiece BPE <code>OnnxTextGenerator</code> Chat, instruction following"},{"location":"reference/supported-models/#encoder-decoder-autoregressive","title":"Encoder-decoder (autoregressive)","text":"Model Tokenizer Wrapper Use Cases Flan-T5 (Small / Base / Large) SentencePiece Unigram <code>FlanT5TextGenerator</code> Summarization, translation, grammar T5-small-awesome-text-to-sql SentencePiece Unigram <code>T5SqlGenerator</code> Text-to-SQL (lightweight) T5-LM-Large-text2sql-spider SentencePiece Unigram <code>T5SqlGenerator</code> Text-to-SQL (high accuracy) DistilBART CNN 12-6 BPE <code>BartSummarizer</code> Summarization BART Large CNN BPE <code>BartSummarizer</code> Summarization MarianMT (opus-mt-*) SentencePiece BPE <code>MarianTranslator</code> Translation (fixed language pair) CoEdIT (Base / Large) SentencePiece Unigram <code>CoeditGrammarCorrector</code> Grammar correction"},{"location":"reference/supported-models/#vision_1","title":"Vision","text":"Model Tokenizer Wrapper Use Cases ResNet-50 N/A <code>ResNetClassifier</code> Image classification EfficientNet-Lite4 N/A <code>EfficientNetClassifier</code> Image classification YOLOv8n N/A <code>YoloV8Detector</code> Object detection YOLO26n N/A <code>Yolo26Detector</code> Object detection CRAFT N/A <code>CraftTextDetector</code> Text detection in images"},{"location":"reference/supported-models/#multimodal_1","title":"Multimodal","text":"Model Tokenizer Wrapper Use Cases CLIP ViT-B/32 BPE <code>ClipClassifier</code> Zero-shot image classification"},{"location":"reference/supported-models/#audio_1","title":"Audio","text":"Model Tokenizer Wrapper Use Cases Wav2Vec2 CTC <code>Wav2Vec2Recognizer</code> Speech-to-text Silero VAD N/A <code>SileroVadDetector</code> Voice activity detection"},{"location":"reference/supported-models/#model-compatibility","title":"Model compatibility","text":""},{"location":"reference/supported-models/#yolov8-yolo11","title":"YOLOv8 / YOLO11","text":"<p><code>YoloV8Detector</code> is compatible with both YOLOv8 and YOLO11 models \u2014 they share the same output layout (<code>[1, 4+C, N]</code>). It is not compatible with YOLOv5 (different layout with objectness column) or YOLO26 (NMS-free architecture).</p>"},{"location":"reference/supported-models/#efficientnet-variants","title":"EfficientNet variants","text":"<p><code>EfficientNetClassifier</code> is tested against EfficientNet-Lite4 (TensorFlow origin, softmax built-in). For PyTorch-exported EfficientNet models that output raw logits, override with <code>.outputOperator(OutputOperator.softmax())</code>.</p>"},{"location":"reference/supported-models/#custom-models","title":"Custom models","text":"<p>Any ONNX-exported model works with the appropriate wrapper, as long as it follows the expected input/output layout. See the Custom Models guide for details.</p>"},{"location":"reference/supported-models/#cache","title":"Cache","text":"<p>Models are cached in <code>~/.cache/inference4j/</code> by default. Customize with:</p> <ul> <li>System property: <code>-Dinference4j.cache.dir=/path/to/cache</code></li> <li>Environment variable: <code>INFERENCE4J_CACHE_DIR=/path/to/cache</code></li> </ul> <p>See Configuration for all options.</p>"},{"location":"reference/supported-models/#planned-models","title":"Planned models","text":"Domain Model Status Text TrOCR (text recognition) Planned \u2014 enabled by encoder-decoder infrastructure <p>See the Roadmap for details.</p>"},{"location":"use-cases/grammar-correction/","title":"Grammar Correction","text":"<p>Fix grammatical errors in text using CoEdIT or Flan-T5 encoder-decoder models.</p>"},{"location":"use-cases/grammar-correction/#quick-example","title":"Quick example","text":"<pre><code>try (var corrector = CoeditGrammarCorrector.coeditBase().build()) {\n    String corrected = corrector.correct(\"She don't likes swimming.\");\n    System.out.println(corrected); // She doesn't like swimming.\n}\n</code></pre>"},{"location":"use-cases/grammar-correction/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.nlp.CoeditGrammarCorrector;\n\npublic class GrammarCorrection {\n    public static void main(String[] args) {\n        try (var corrector = CoeditGrammarCorrector.coeditBase()\n                .maxNewTokens(200)\n                .build()) {\n\n            String[] sentences = {\n                \"She don't likes swimming.\",\n                \"Me and him went to the store yesterday.\",\n                \"The informations is very useful for we.\"\n            };\n\n            for (String sentence : sentences) {\n                GenerationResult result = corrector.correct(sentence,\n                        token -&gt; System.out.print(token));\n                System.out.println();\n                System.out.printf(\"  \u2192 %d tokens in %,d ms%n\",\n                        result.generatedTokens(), result.duration().toMillis());\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"use-cases/grammar-correction/#using-flan-t5-as-an-alternative","title":"Using Flan-T5 as an alternative","text":"<p><code>FlanT5TextGenerator</code> can also correct grammar. It implements the same <code>GrammarCorrector</code> interface:</p> <pre><code>import io.github.inference4j.nlp.FlanT5TextGenerator;\nimport io.github.inference4j.nlp.GrammarCorrector;\n\n// Both implement GrammarCorrector \u2014 swap freely\nGrammarCorrector corrector = FlanT5TextGenerator.flanT5Base()\n        .maxNewTokens(200)\n        .build();\n</code></pre>"},{"location":"use-cases/grammar-correction/#model-presets","title":"Model presets","text":""},{"location":"use-cases/grammar-correction/#coeditgrammarcorrector","title":"CoeditGrammarCorrector","text":"Preset Model Parameters Size <code>CoeditGrammarCorrector.coeditBase()</code> CoEdIT Base 250M ~900 MB <code>CoeditGrammarCorrector.coeditLarge()</code> CoEdIT Large 780M ~3 GB"},{"location":"use-cases/grammar-correction/#flant5textgenerator","title":"FlanT5TextGenerator","text":"Preset Model Parameters Size <code>FlanT5TextGenerator.flanT5Small()</code> Flan-T5 Small 77M ~300 MB <code>FlanT5TextGenerator.flanT5Base()</code> Flan-T5 Base 250M ~900 MB <code>FlanT5TextGenerator.flanT5Large()</code> Flan-T5 Large 780M ~3 GB"},{"location":"use-cases/grammar-correction/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> Preset-dependent HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizerProvider(TokenizerProvider)</code> <code>TokenizerProvider</code> <code>SentencePieceBpeTokenizer</code> Tokenizer construction strategy <code>.maxNewTokens(int)</code> <code>int</code> <code>256</code> Maximum tokens to generate <code>.temperature(float)</code> <code>float</code> <code>0.0</code> Sampling temperature <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(float)</code> <code>float</code> <code>0.0</code> (disabled) Nucleus sampling <code>.eosTokenId(int)</code> <code>int</code> Auto-detected End-of-sequence token ID <code>.addedToken(String)</code> <code>String</code> \u2014 Register a special token for atomic encoding"},{"location":"use-cases/grammar-correction/#result-type","title":"Result type","text":"<p><code>GenerationResult</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The corrected text <code>promptTokens()</code> <code>int</code> Number of tokens in the input <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time <p>The convenience method <code>correct(text)</code> returns the corrected text as a plain <code>String</code>.</p>"},{"location":"use-cases/grammar-correction/#tips","title":"Tips","text":"<ul> <li>CoEdIT is specifically trained for grammar correction (using the \"Fix grammatical errors\" instruction internally). It produces more reliable corrections than general-purpose models.</li> <li>Flan-T5 is a general-purpose model that also handles summarization, translation, and SQL generation. Use it when you need multiple tasks from a single model.</li> <li>Use greedy decoding (default <code>temperature=0</code>) for grammar correction \u2014 sampling introduces random variations.</li> <li>CoEdIT automatically prepends the instruction prefix <code>\"Fix grammatical errors in this sentence: \"</code> \u2014 just pass the raw text to <code>correct()</code>.</li> <li>For batch correction, reuse the same instance \u2014 each call to <code>correct()</code> runs an independent generation.</li> </ul>"},{"location":"use-cases/image-classification/","title":"Image Classification","text":"<p>Classify images using ResNet or EfficientNet models. Pass an image path or <code>BufferedImage</code>, get back labeled predictions with confidence scores.</p>"},{"location":"use-cases/image-classification/#quick-example","title":"Quick example","text":"<pre><code>try (var classifier = ResNetClassifier.builder().build()) {\n    List&lt;Classification&gt; results = classifier.classify(Path.of(\"cat.jpg\"));\n    // [Classification[label=tabby cat, confidence=0.87], ...]\n}\n</code></pre>"},{"location":"use-cases/image-classification/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.vision.ResNetClassifier;\nimport io.github.inference4j.vision.Classification;\nimport java.nio.file.Path;\n\npublic class ImageClassification {\n    public static void main(String[] args) {\n        try (var classifier = ResNetClassifier.builder().build()) {\n            List&lt;Classification&gt; results = classifier.classify(Path.of(\"cat.jpg\"), 5);\n\n            for (Classification c : results) {\n                System.out.printf(\"%-30s %.2f%%%n\", c.label(), c.confidence() * 100);\n            }\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/image-classification/#available-models","title":"Available models","text":""},{"location":"use-cases/image-classification/#resnet","title":"ResNet","text":"<p>The default choice for image classification. Uses ImageNet normalization and softmax output.</p> <pre><code>try (var classifier = ResNetClassifier.builder().build()) {\n    classifier.classify(Path.of(\"image.jpg\"));\n}\n</code></pre>"},{"location":"use-cases/image-classification/#efficientnet","title":"EfficientNet","text":"<p>An alternative architecture with built-in softmax. Good for models exported from TensorFlow.</p> <pre><code>try (var classifier = EfficientNetClassifier.builder().build()) {\n    classifier.classify(Path.of(\"image.jpg\"));\n}\n</code></pre> <p>Note</p> <p>EfficientNet-Lite4 (the default) has softmax built into the model. If you use a PyTorch-exported EfficientNet that outputs raw logits, override with <code>.outputOperator(OutputOperator.softmax())</code>.</p>"},{"location":"use-cases/image-classification/#resnet-builder-options","title":"ResNet builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/resnet50-v1-7</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.labels(Labels)</code> <code>Labels</code> auto-loaded from <code>labels.txt</code> Classification labels <code>.outputOperator(OutputOperator)</code> <code>OutputOperator</code> <code>softmax()</code> Output activation function <code>.defaultTopK(int)</code> <code>int</code> <code>5</code> Default number of top predictions"},{"location":"use-cases/image-classification/#efficientnet-builder-options","title":"EfficientNet builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/efficientnet-lite4</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.labels(Labels)</code> <code>Labels</code> auto-loaded from <code>labels.txt</code> Classification labels <code>.outputOperator(OutputOperator)</code> <code>OutputOperator</code> <code>identity()</code> Output activation (softmax built-in) <code>.defaultTopK(int)</code> <code>int</code> <code>5</code> Default number of top predictions"},{"location":"use-cases/image-classification/#result-type","title":"Result type","text":"<p><code>Classification</code> is a record with:</p> Field Type Description <code>label()</code> <code>String</code> ImageNet class label (e.g., <code>tabby cat</code>) <code>index()</code> <code>int</code> Class index (0\u2013999 for ImageNet) <code>confidence()</code> <code>float</code> Confidence score (0.0 to 1.0)"},{"location":"use-cases/image-classification/#hardware-acceleration","title":"Hardware acceleration","text":"<p>Image classification benefits significantly from hardware acceleration:</p> Model CPU CoreML Speedup ResNet-50 37 ms 10 ms 3.7x <pre><code>try (var classifier = ResNetClassifier.builder()\n        .sessionOptions(opts -&gt; opts.addCoreML())\n        .build()) {\n    classifier.classify(Path.of(\"cat.jpg\"));\n}\n</code></pre> <p>See the Hardware Acceleration guide for details.</p>"},{"location":"use-cases/image-classification/#tips","title":"Tips","text":"<ul> <li>Use <code>classify(image, topK)</code> to control how many predictions are returned.</li> <li>Both classifiers accept <code>Path</code> or <code>BufferedImage</code> as input.</li> <li>Input images are automatically resized and normalized \u2014 no manual preprocessing needed.</li> <li>Input size is auto-detected from the model (ResNet: 224x224, EfficientNet-Lite4: 280x280).</li> </ul>"},{"location":"use-cases/object-detection/","title":"Object Detection","text":"<p>Detect and locate objects in images using YOLO models. inference4j supports both NMS-based (YOLOv8, YOLO11) and NMS-free (YOLO26) architectures.</p>"},{"location":"use-cases/object-detection/#quick-example","title":"Quick example","text":"<pre><code>try (var detector = YoloV8Detector.builder().build()) {\n    List&lt;Detection&gt; detections = detector.detect(Path.of(\"street.jpg\"));\n    // [Detection[label=car, confidence=0.94, box=BoundingBox[...]], ...]\n}\n</code></pre>"},{"location":"use-cases/object-detection/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.vision.YoloV8Detector;\nimport io.github.inference4j.vision.Detection;\nimport java.nio.file.Path;\n\npublic class ObjectDetection {\n    public static void main(String[] args) {\n        try (var detector = YoloV8Detector.builder().build()) {\n            List&lt;Detection&gt; detections = detector.detect(Path.of(\"street.jpg\"));\n\n            for (Detection d : detections) {\n                System.out.printf(\"%-12s (%.0f%%) at [%.0f, %.0f, %.0f, %.0f]%n\",\n                    d.label(), d.confidence() * 100,\n                    d.box().x1(), d.box().y1(), d.box().x2(), d.box().y2());\n            }\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/object-detection/#available-models","title":"Available models","text":""},{"location":"use-cases/object-detection/#yolov8-yolo11","title":"YOLOv8 / YOLO11","text":"<p>NMS-based detection. The <code>YoloV8Detector</code> is compatible with both YOLOv8 and YOLO11 models (same output layout).</p> <pre><code>try (var detector = YoloV8Detector.builder().build()) {\n    detector.detect(Path.of(\"image.jpg\"));\n}\n</code></pre>"},{"location":"use-cases/object-detection/#yolo26","title":"YOLO26","text":"<p>NMS-free detection. The architecture outputs 300 deduplicated proposals directly \u2014 no post-processing NMS needed.</p> <pre><code>try (var detector = Yolo26Detector.builder().build()) {\n    detector.detect(Path.of(\"image.jpg\"));\n}\n</code></pre> <p>Note</p> <p>YOLO26 accepts an <code>iouThreshold</code> parameter for API compatibility, but it is ignored \u2014 the architecture handles deduplication internally.</p>"},{"location":"use-cases/object-detection/#yolov8-builder-options","title":"YoloV8 builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/yolov8n</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.labels(Labels)</code> <code>Labels</code> COCO (80 classes) Detection class labels <code>.inputName(String)</code> <code>String</code> auto-detected Input tensor name <code>.inputSize(int)</code> <code>int</code> <code>640</code> Model input resolution <code>.confidenceThreshold(float)</code> <code>float</code> <code>0.25</code> Minimum detection confidence <code>.iouThreshold(float)</code> <code>float</code> <code>0.45</code> NMS IoU threshold"},{"location":"use-cases/object-detection/#yolo26-builder-options","title":"Yolo26 builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/yolo26n</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.labels(Labels)</code> <code>Labels</code> COCO (80 classes) Detection class labels <code>.inputName(String)</code> <code>String</code> auto-detected Input tensor name <code>.inputSize(int)</code> <code>int</code> <code>640</code> Model input resolution <code>.confidenceThreshold(float)</code> <code>float</code> <code>0.5</code> Minimum detection confidence"},{"location":"use-cases/object-detection/#result-type","title":"Result type","text":"<p><code>Detection</code> is a record with:</p> Field Type Description <code>label()</code> <code>String</code> Class label (e.g., <code>car</code>, <code>person</code>) <code>classIndex()</code> <code>int</code> Class index in the label set <code>confidence()</code> <code>float</code> Detection confidence (0.0 to 1.0) <code>box()</code> <code>BoundingBox</code> Bounding box coordinates <p><code>BoundingBox</code> provides:</p> Field Type Description <code>x1()</code> <code>float</code> Left coordinate <code>y1()</code> <code>float</code> Top coordinate <code>x2()</code> <code>float</code> Right coordinate <code>y2()</code> <code>float</code> Bottom coordinate <p>Coordinates are in the original image's pixel space (not normalized).</p>"},{"location":"use-cases/object-detection/#custom-thresholds","title":"Custom thresholds","text":"<p>Override thresholds per-call:</p> <pre><code>try (var detector = YoloV8Detector.builder().build()) {\n    // Stricter confidence, tighter NMS\n    List&lt;Detection&gt; detections = detector.detect(\n        Path.of(\"image.jpg\"), 0.5f, 0.3f);\n}\n</code></pre>"},{"location":"use-cases/object-detection/#tips","title":"Tips","text":"<ul> <li>YOLOv8 vs YOLO26: YOLOv8 is the safer default \u2014 widely tested and well-understood. YOLO26 is NMS-free (potentially faster) but newer.</li> <li>Both detectors accept <code>Path</code> or <code>BufferedImage</code> as input.</li> <li>Default labels are COCO (80 classes). Override with <code>.labels(Labels.of(yourLabels))</code> for custom-trained models.</li> <li>YOLOv8 uses letterbox preprocessing (preserves aspect ratio, pads with gray). YOLO26 uses direct resize.</li> </ul>"},{"location":"use-cases/semantic-search/","title":"Semantic Search","text":"<p>Build a semantic search pipeline using text embeddings and cross-encoder reranking. inference4j provides two complementary models: <code>SentenceTransformerEmbedder</code> for fast retrieval and <code>MiniLMSearchReranker</code> for precision reranking.</p>"},{"location":"use-cases/semantic-search/#quick-example-embeddings","title":"Quick example \u2014 embeddings","text":"<pre><code>try (var embedder = SentenceTransformerEmbedder.builder()\n        .modelId(\"inference4j/all-MiniLM-L6-v2\").build()) {\n    float[] embedding = embedder.encode(\"Hello, world!\");\n}\n</code></pre>"},{"location":"use-cases/semantic-search/#quick-example-reranking","title":"Quick example \u2014 reranking","text":"<pre><code>try (var reranker = MiniLMSearchReranker.builder().build()) {\n    float score = reranker.score(\"What is Java?\", \"Java is a programming language.\");\n}\n</code></pre>"},{"location":"use-cases/semantic-search/#full-search-pipeline","title":"Full search pipeline","text":"<p>A typical semantic search pipeline uses embeddings for fast candidate retrieval, then a cross-encoder reranker for precision scoring of the top results.</p> <pre><code>import io.github.inference4j.nlp.SentenceTransformerEmbedder;\nimport io.github.inference4j.nlp.MiniLMSearchReranker;\n\npublic class SemanticSearch {\n    public static void main(String[] args) {\n        String query = \"How do I handle errors in Java?\";\n        List&lt;String&gt; documents = List.of(\n            \"Java uses try-catch blocks for exception handling.\",\n            \"Python decorators are a powerful feature.\",\n            \"Error handling in Java includes checked and unchecked exceptions.\",\n            \"The Java Stream API provides functional operations on collections.\"\n        );\n\n        // Stage 1: Embed and retrieve candidates by cosine similarity\n        try (var embedder = SentenceTransformerEmbedder.builder()\n                .modelId(\"inference4j/all-MiniLM-L6-v2\").build()) {\n\n            float[] queryEmbedding = embedder.encode(query);\n            List&lt;float[]&gt; docEmbeddings = embedder.encodeBatch(documents);\n\n            // Rank by cosine similarity (your own similarity function)\n            // ...\n        }\n\n        // Stage 2: Rerank top candidates with cross-encoder\n        try (var reranker = MiniLMSearchReranker.builder().build()) {\n            float[] scores = reranker.scoreBatch(query, documents);\n\n            for (int i = 0; i &lt; documents.size(); i++) {\n                System.out.printf(\"%.4f  %s%n\", scores[i], documents.get(i));\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"use-cases/semantic-search/#embedder-builder-options","title":"Embedder builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/all-MiniLM-L6-v2</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizer(Tokenizer)</code> <code>Tokenizer</code> auto-loaded <code>WordPieceTokenizer</code> Custom tokenizer <code>.poolingStrategy(PoolingStrategy)</code> <code>PoolingStrategy</code> <code>MEAN</code> Pooling method: <code>CLS</code>, <code>MEAN</code>, or <code>MAX</code> <code>.maxLength(int)</code> <code>int</code> <code>512</code> Maximum token sequence length"},{"location":"use-cases/semantic-search/#reranker-builder-options","title":"Reranker builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/ms-marco-MiniLM-L-6-v2</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizer(Tokenizer)</code> <code>Tokenizer</code> auto-loaded <code>WordPieceTokenizer</code> Custom tokenizer <code>.maxLength(int)</code> <code>int</code> <code>512</code> Maximum token sequence length"},{"location":"use-cases/semantic-search/#result-types","title":"Result types","text":""},{"location":"use-cases/semantic-search/#embeddings","title":"Embeddings","text":"<p><code>encode()</code> returns a <code>float[]</code> \u2014 a dense vector representation of the input text. Use cosine similarity to compare embeddings:</p> <pre><code>static double cosineSimilarity(float[] a, float[] b) {\n    double dot = 0, normA = 0, normB = 0;\n    for (int i = 0; i &lt; a.length; i++) {\n        dot += a[i] * b[i];\n        normA += a[i] * a[i];\n        normB += b[i] * b[i];\n    }\n    return dot / (Math.sqrt(normA) * Math.sqrt(normB));\n}\n</code></pre>"},{"location":"use-cases/semantic-search/#reranking-scores","title":"Reranking scores","text":"<p><code>score()</code> returns a <code>float</code> in the range [0, 1] \u2014 the sigmoid-normalized relevance score for a query-document pair. Higher means more relevant.</p> <p><code>scoreBatch()</code> returns a <code>float[]</code> \u2014 one score per document, scored against the same query.</p>"},{"location":"use-cases/semantic-search/#pooling-strategies","title":"Pooling strategies","text":"<p>The embedder supports three pooling strategies for converting token-level representations into a single sentence embedding:</p> Strategy Description <code>MEAN</code> Average of all token embeddings (default, best for most tasks) <code>CLS</code> Uses only the <code>[CLS]</code> token embedding <code>MAX</code> Element-wise maximum across all token embeddings"},{"location":"use-cases/semantic-search/#tips","title":"Tips","text":"<ul> <li>Two-stage pipeline: Use embeddings for fast top-K retrieval (cheap cosine similarity), then rerank the top candidates with the cross-encoder (expensive but more accurate).</li> <li>Batch encoding: Use <code>encodeBatch()</code> when encoding multiple texts \u2014 more efficient than calling <code>encode()</code> in a loop.</li> <li>Embedding dimension depends on the model: all-MiniLM-L6-v2 produces 384-dimensional vectors.</li> </ul>"},{"location":"use-cases/sentiment-analysis/","title":"Sentiment Analysis","text":"<p>Classify text as positive or negative (or any custom label set) using a fine-tuned DistilBERT model.</p>"},{"location":"use-cases/sentiment-analysis/#quick-example","title":"Quick example","text":"<pre><code>try (var classifier = DistilBertTextClassifier.builder().build()) {\n    List&lt;TextClassification&gt; results = classifier.classify(\"This movie was fantastic!\");\n    // [TextClassification[label=POSITIVE, confidence=0.9998]]\n}\n</code></pre>"},{"location":"use-cases/sentiment-analysis/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.nlp.DistilBertTextClassifier;\nimport io.github.inference4j.nlp.TextClassification;\nimport java.util.List;\n\npublic class SentimentAnalysis {\n    public static void main(String[] args) {\n        try (var classifier = DistilBertTextClassifier.builder().build()) {\n            List&lt;String&gt; reviews = List.of(\n                \"This movie was fantastic!\",\n                \"Terrible experience, would not recommend.\",\n                \"It was okay, nothing special.\"\n            );\n\n            for (String review : reviews) {\n                List&lt;TextClassification&gt; results = classifier.classify(review);\n                TextClassification top = results.get(0);\n                System.out.printf(\"%-45s \u2192 %s (%.2f%%)%n\",\n                    review, top.label(), top.confidence() * 100);\n            }\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/sentiment-analysis/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/distilbert-base-uncased-finetuned-sst-2-english</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizer(Tokenizer)</code> <code>Tokenizer</code> auto-loaded <code>WordPieceTokenizer</code> Custom tokenizer <code>.config(ModelConfig)</code> <code>ModelConfig</code> auto-loaded from <code>config.json</code> Model config with labels <code>.outputOperator(OutputOperator)</code> <code>OutputOperator</code> auto-detected (softmax or sigmoid) Output activation <code>.maxLength(int)</code> <code>int</code> <code>512</code> Maximum token sequence length"},{"location":"use-cases/sentiment-analysis/#result-type","title":"Result type","text":"<p><code>TextClassification</code> is a record with:</p> Field Type Description <code>label()</code> <code>String</code> Classification label (e.g., <code>POSITIVE</code>) <code>classIndex()</code> <code>int</code> Numeric class index <code>confidence()</code> <code>float</code> Confidence score (0.0 to 1.0)"},{"location":"use-cases/sentiment-analysis/#using-custom-models","title":"Using custom models","text":"<p>Any HuggingFace text classification model exported to ONNX will work, as long as it includes <code>vocab.txt</code> and <code>config.json</code> with <code>id2label</code> mappings.</p> <pre><code>try (var classifier = DistilBertTextClassifier.builder()\n        .modelId(\"your-org/your-model\")\n        .build()) {\n    classifier.classify(\"Some text\");\n}\n</code></pre> <p>The output activation (softmax vs sigmoid) is auto-detected from <code>config.json</code>:</p> <ul> <li><code>problem_type: \"multi_label_classification\"</code> \u2192 sigmoid</li> <li>Everything else \u2192 softmax</li> </ul>"},{"location":"use-cases/sentiment-analysis/#tips","title":"Tips","text":"<ul> <li>The default model is fine-tuned on SST-2 (movie reviews). For other domains (product reviews, support tickets), use a model fine-tuned on relevant data.</li> <li>Use <code>.classify(text, topK)</code> to limit the number of returned classifications.</li> <li>For multi-label classification (where multiple labels can be true simultaneously), use a model with <code>problem_type: \"multi_label_classification\"</code> in its <code>config.json</code>.</li> </ul>"},{"location":"use-cases/speech-to-text/","title":"Speech-to-Text","text":"<p>Transcribe audio files using Wav2Vec2, a non-autoregressive CTC model that converts speech to text in a single forward pass.</p>"},{"location":"use-cases/speech-to-text/#quick-example","title":"Quick example","text":"<pre><code>try (var recognizer = Wav2Vec2Recognizer.builder().build()) {\n    System.out.println(recognizer.transcribe(Path.of(\"audio.wav\")).text());\n}\n</code></pre>"},{"location":"use-cases/speech-to-text/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.audio.Wav2Vec2Recognizer;\nimport io.github.inference4j.audio.Transcription;\nimport java.nio.file.Path;\n\npublic class SpeechToText {\n    public static void main(String[] args) {\n        try (var recognizer = Wav2Vec2Recognizer.builder().build()) {\n            Transcription result = recognizer.transcribe(Path.of(\"speech.wav\"));\n            System.out.println(result.text());\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/speech-to-text/#from-raw-audio-data","title":"From raw audio data","text":"<p>If you already have audio samples as a float array:</p> <pre><code>try (var recognizer = Wav2Vec2Recognizer.builder().build()) {\n    float[] audioData = loadAudioSamples(); // your audio loading logic\n    Transcription result = recognizer.transcribe(audioData, 16000);\n    System.out.println(result.text());\n}\n</code></pre>"},{"location":"use-cases/speech-to-text/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/wav2vec2-base-960h</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.vocabulary(Vocabulary)</code> <code>Vocabulary</code> auto-loaded from <code>vocab.json</code> CTC vocabulary <code>.inputName(String)</code> <code>String</code> auto-detected Input tensor name <code>.sampleRate(int)</code> <code>int</code> <code>16000</code> Target sample rate (Hz) <code>.blankIndex(int)</code> <code>int</code> <code>0</code> CTC blank token index <code>.wordDelimiter(String)</code> <code>String</code> <code>\"\\|\"</code> Word separator token in vocabulary"},{"location":"use-cases/speech-to-text/#result-type","title":"Result type","text":"<p><code>Transcription</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The transcribed text <code>segments()</code> <code>List&lt;Segment&gt;</code> Timed segments (empty for Wav2Vec2)"},{"location":"use-cases/speech-to-text/#audio-requirements","title":"Audio requirements","text":"<ul> <li>Format: WAV files (loaded automatically from <code>Path</code>)</li> <li>Sample rate: Audio is automatically resampled to the model's target rate (16kHz by default)</li> <li>Channels: Mono (stereo is downmixed automatically)</li> <li>Duration: No hard limit, but very long files will use more memory</li> </ul>"},{"location":"use-cases/speech-to-text/#how-it-works","title":"How it works","text":"<p>Wav2Vec2 is a non-autoregressive model \u2014 it processes the entire audio waveform in a single forward pass and produces character-level predictions using CTC (Connectionist Temporal Classification) decoding.</p> <p>The pipeline:</p> <ol> <li>Load and normalize audio from WAV file</li> <li>Resample to 16kHz if needed</li> <li>Run a single forward pass through the model</li> <li>Apply CTC greedy decoding to convert logits to characters</li> <li>Join characters into words using the word delimiter</li> </ol>"},{"location":"use-cases/speech-to-text/#tips","title":"Tips","text":"<ul> <li>The default model (<code>wav2vec2-base-960h</code>) is trained on English LibriSpeech data. For other languages, use an appropriate fine-tuned model.</li> <li>Wav2Vec2 works best with clean speech. For noisy audio, consider preprocessing with VAD to extract speech segments first \u2014 see Voice Activity Detection.</li> <li>This is a CTC model (single-pass), not an autoregressive model. It's fast but may be less accurate on complex audio. For multilingual support or translation, see Whisper.</li> </ul>"},{"location":"use-cases/summarization/","title":"Text Summarization","text":"<p>Summarize long articles and documents into concise text using BART or Flan-T5 encoder-decoder models.</p>"},{"location":"use-cases/summarization/#quick-example","title":"Quick example","text":"<pre><code>try (var summarizer = BartSummarizer.distilBartCnn().build()) {\n    String summary = summarizer.summarize(\"Long article text here...\");\n    System.out.println(summary);\n}\n</code></pre>"},{"location":"use-cases/summarization/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.nlp.BartSummarizer;\n\npublic class Summarization {\n    public static void main(String[] args) {\n        try (var summarizer = BartSummarizer.distilBartCnn()\n                .maxNewTokens(150)\n                .build()) {\n\n            String article = \"\"\"\n                The Amazon rainforest, often referred to as the \"lungs of the Earth\",\n                produces about 20% of the world's oxygen. Spanning across nine countries\n                in South America, it is the largest tropical rainforest in the world,\n                covering approximately 5.5 million square kilometers. The forest is home\n                to an estimated 10% of all species on Earth, including over 40,000 plant\n                species, 1,300 bird species, and 3,000 types of fish. Deforestation\n                remains a critical threat, with an estimated 17% of the forest lost in\n                the last 50 years due to logging, agriculture, and urban expansion.\n                \"\"\";\n\n            GenerationResult result = summarizer.summarize(article, token -&gt; System.out.print(token));\n            System.out.println();\n            System.out.printf(\"%d tokens in %,d ms%n\",\n                    result.generatedTokens(), result.duration().toMillis());\n        }\n    }\n}\n</code></pre>"},{"location":"use-cases/summarization/#using-flan-t5-as-an-alternative","title":"Using Flan-T5 as an alternative","text":"<p><code>FlanT5TextGenerator</code> can also summarize text. It uses a different architecture but implements the same <code>Summarizer</code> interface:</p> <pre><code>import io.github.inference4j.nlp.FlanT5TextGenerator;\nimport io.github.inference4j.nlp.Summarizer;\n\n// Both implement Summarizer \u2014 swap freely\nSummarizer summarizer = FlanT5TextGenerator.flanT5Base()\n        .maxNewTokens(150)\n        .build();\n</code></pre>"},{"location":"use-cases/summarization/#model-presets","title":"Model presets","text":""},{"location":"use-cases/summarization/#bartsummarizer","title":"BartSummarizer","text":"Preset Model Parameters Size <code>BartSummarizer.distilBartCnn()</code> DistilBART CNN 12-6 306M ~1.2 GB <code>BartSummarizer.bartLargeCnn()</code> BART Large CNN 406M ~1.6 GB"},{"location":"use-cases/summarization/#flant5textgenerator","title":"FlanT5TextGenerator","text":"Preset Model Parameters Size <code>FlanT5TextGenerator.flanT5Small()</code> Flan-T5 Small 77M ~300 MB <code>FlanT5TextGenerator.flanT5Base()</code> Flan-T5 Base 250M ~900 MB <code>FlanT5TextGenerator.flanT5Large()</code> Flan-T5 Large 780M ~3 GB"},{"location":"use-cases/summarization/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> Preset-dependent HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizerProvider(TokenizerProvider)</code> <code>TokenizerProvider</code> Preset-dependent Tokenizer construction strategy <code>.maxNewTokens(int)</code> <code>int</code> <code>256</code> Maximum tokens to generate <code>.temperature(float)</code> <code>float</code> <code>0.0</code> Sampling temperature (higher = more random) <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(float)</code> <code>float</code> <code>0.0</code> (disabled) Nucleus sampling <code>.eosTokenId(int)</code> <code>int</code> Auto-detected End-of-sequence token ID <code>.addedToken(String)</code> <code>String</code> \u2014 Register a special token for atomic encoding"},{"location":"use-cases/summarization/#result-type","title":"Result type","text":"<p>Both <code>summarize(text, tokenListener)</code> and <code>generate(text, tokenListener)</code> return a <code>GenerationResult</code> record:</p> Field Type Description <code>text()</code> <code>String</code> The generated summary <code>promptTokens()</code> <code>int</code> Number of tokens in the input <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time <p>The convenience method <code>summarize(text)</code> returns the summary as a plain <code>String</code>.</p>"},{"location":"use-cases/summarization/#tips","title":"Tips","text":"<ul> <li>DistilBART CNN is purpose-built for summarization and produces the best summaries. Use it when summarization is your only task.</li> <li>Flan-T5 is a general-purpose model that also handles translation, grammar correction, and SQL generation. Use it when you need multiple tasks from a single model.</li> <li>Lower <code>maxNewTokens</code> for shorter summaries \u2014 the model will still produce coherent output.</li> <li>Use streaming (<code>summarize(text, token -&gt; ...)</code>) for long inputs where generation takes several seconds.</li> <li>Reuse instances across calls \u2014 each one holds the model and tokenizer in memory.</li> </ul>"},{"location":"use-cases/text-detection/","title":"Text Detection","text":"<p>Detect text regions in images using the CRAFT model. Returns bounding boxes around detected text \u2014 useful as the first stage of an OCR pipeline.</p>"},{"location":"use-cases/text-detection/#quick-example","title":"Quick example","text":"<pre><code>try (var detector = CraftTextDetector.builder().build()) {\n    List&lt;TextRegion&gt; regions = detector.detect(Path.of(\"document.jpg\"));\n}\n</code></pre>"},{"location":"use-cases/text-detection/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.vision.CraftTextDetector;\nimport io.github.inference4j.vision.TextRegion;\nimport java.nio.file.Path;\n\npublic class TextDetection {\n    public static void main(String[] args) {\n        try (var detector = CraftTextDetector.builder().build()) {\n            List&lt;TextRegion&gt; regions = detector.detect(Path.of(\"document.jpg\"));\n\n            for (TextRegion r : regions) {\n                System.out.printf(\"[%.1f, %.1f, %.1f, %.1f] (confidence=%.2f)%n\",\n                    r.box().x1(), r.box().y1(),\n                    r.box().x2(), r.box().y2(),\n                    r.confidence());\n            }\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/text-detection/#custom-thresholds","title":"Custom thresholds","text":"<p>The default thresholds (0.7, 0.4) are tuned for clean document scans. For real-world images (photos, screenshots), use lower thresholds:</p> <pre><code>try (var detector = CraftTextDetector.builder().build()) {\n    List&lt;TextRegion&gt; regions = detector.detect(\n        Path.of(\"photo.jpg\"), 0.4f, 0.3f);\n}\n</code></pre>"},{"location":"use-cases/text-detection/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/craft-mlt-25k</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.inputName(String)</code> <code>String</code> auto-detected Input tensor name <code>.targetSize(int)</code> <code>int</code> <code>1280</code> Long-side resize target <code>.textThreshold(float)</code> <code>float</code> <code>0.7</code> Region score threshold <code>.lowTextThreshold(float)</code> <code>float</code> <code>0.4</code> Affinity score threshold <code>.minComponentArea(int)</code> <code>int</code> <code>10</code> Minimum region pixel area"},{"location":"use-cases/text-detection/#result-type","title":"Result type","text":"<p><code>TextRegion</code> is a record with:</p> Field Type Description <code>box()</code> <code>BoundingBox</code> Bounding box around the detected text <code>confidence()</code> <code>float</code> Detection confidence (0.0 to 1.0)"},{"location":"use-cases/text-detection/#how-craft-works","title":"How CRAFT works","text":"<p>CRAFT (Character Region Awareness for Text detection) produces two pixel-level heatmaps:</p> <ol> <li>Region score \u2014 highlights character centers</li> <li>Affinity score \u2014 highlights spacing between characters (links characters into words)</li> </ol> <p>Connected components on the thresholded heatmaps produce text region bounding boxes. Unlike YOLO-style detectors, CRAFT doesn't use NMS \u2014 the connected component approach naturally handles overlapping text.</p>"},{"location":"use-cases/text-detection/#threshold-tuning","title":"Threshold tuning","text":"Scenario <code>textThreshold</code> <code>lowTextThreshold</code> Clean document scans <code>0.7</code> (default) <code>0.4</code> (default) Real-world photos <code>0.4</code> <code>0.3</code> Sparse text (signs, labels) <code>0.3</code> <code>0.2</code> <p>Lower thresholds detect more text but may produce false positives. Higher thresholds are more precise but may miss faint or small text.</p>"},{"location":"use-cases/text-detection/#hardware-acceleration","title":"Hardware acceleration","text":"<p>CRAFT benefits significantly from hardware acceleration:</p> Backend Latency Speedup CPU 831 ms \u2014 CoreML 153 ms 5.4x <pre><code>try (var detector = CraftTextDetector.builder()\n        .sessionOptions(opts -&gt; opts.addCoreML())\n        .build()) {\n    detector.detect(Path.of(\"document.jpg\"));\n}\n</code></pre>"},{"location":"use-cases/text-detection/#tips","title":"Tips","text":"<ul> <li>Both <code>Path</code> and <code>BufferedImage</code> inputs are supported.</li> <li>Images are automatically resized (preserving aspect ratio) to fit within <code>targetSize</code>. Dimensions are rounded to multiples of 32 (VGG16 backbone requirement).</li> <li>CRAFT detects text locations, not text content. Pair it with a text recognition model (like TrOCR) for full OCR.</li> <li>Coordinates are in the original image's pixel space.</li> </ul>"},{"location":"use-cases/text-to-sql/","title":"Text-to-SQL","text":"<p>Generate SQL queries from natural language questions using T5-based text-to-SQL models.</p>"},{"location":"use-cases/text-to-sql/#quick-example","title":"Quick example","text":"<pre><code>try (var sqlGen = T5SqlGenerator.t5SmallAwesome().build()) {\n    String sql = sqlGen.generateSql(\n            \"How many employees are in the engineering department?\",\n            \"CREATE TABLE employees (id INT, name VARCHAR, department VARCHAR, salary INT)\");\n    System.out.println(sql);\n    // SELECT COUNT(*) FROM employees AS T1 JOIN departments AS T2\n    //   ON T1.department = T2.id WHERE T2.name = 'Engineer'\n}\n</code></pre>"},{"location":"use-cases/text-to-sql/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.nlp.T5SqlGenerator;\n\npublic class TextToSql {\n    public static void main(String[] args) {\n        String schema = \"CREATE TABLE employees (id INT, name VARCHAR, department VARCHAR, salary INT); \" +\n                         \"CREATE TABLE departments (id INT, name VARCHAR, location VARCHAR)\";\n\n        try (var sqlGen = T5SqlGenerator.t5SmallAwesome()\n                .maxNewTokens(200)\n                .build()) {\n\n            String[] questions = {\n                \"What is the average salary by department?\",\n                \"List all employees in New York\",\n                \"Which department has the most employees?\"\n            };\n\n            for (String question : questions) {\n                GenerationResult result = sqlGen.generateSql(question, schema,\n                        token -&gt; System.out.print(token));\n                System.out.println();\n                System.out.printf(\"  \u2192 %d tokens in %,d ms%n\",\n                        result.generatedTokens(), result.duration().toMillis());\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"use-cases/text-to-sql/#model-presets","title":"Model presets","text":"Preset Model Parameters Size Schema format <code>T5SqlGenerator.t5SmallAwesome()</code> T5-small-awesome-text-to-sql 60M ~240 MB <code>CREATE TABLE</code> DDL <code>T5SqlGenerator.t5LargeSpider()</code> T5-LM-Large-text2sql-spider 0.8B ~4.6 GB Spider format with <code>[SEP]</code>"},{"location":"use-cases/text-to-sql/#choosing-a-preset","title":"Choosing a preset","text":"<p>T5-small-awesome is recommended for most use cases. It's fast, lightweight, and handles standard SQL patterns well. Schema is provided as familiar <code>CREATE TABLE</code> statements.</p> <p>T5-large-spider produces higher accuracy on complex queries (JOINs, subqueries, GROUP BY with HAVING). It uses a specialized schema format designed for the Spider benchmark with quoted identifiers and <code>[SEP]</code> table delimiters \u2014 ideal for integration with JDBC metadata.</p>"},{"location":"use-cases/text-to-sql/#schema-formats","title":"Schema formats","text":""},{"location":"use-cases/text-to-sql/#t5-small-awesome-create-table-ddl","title":"T5-small-awesome (CREATE TABLE DDL)","text":"<pre><code>String schema = \"CREATE TABLE employees (id INT, name VARCHAR, salary INT); \"\n              + \"CREATE TABLE departments (id INT, name VARCHAR)\";\nsqlGen.generateSql(\"What is the average salary?\", schema);\n</code></pre>"},{"location":"use-cases/text-to-sql/#t5-large-spider-spider-format","title":"T5-large-spider (Spider format)","text":"<pre><code>String schema = \"\\\"employees\\\" \\\"id\\\" int, \\\"name\\\" varchar, \\\"salary\\\" int, \"\n              + \"foreign_key: primary key: \\\"id\\\" \"\n              + \"[SEP] \"\n              + \"\\\"departments\\\" \\\"id\\\" int, \\\"name\\\" varchar, \"\n              + \"foreign_key: primary key: \\\"id\\\"\";\nsqlGen.generateSql(\"What is the average salary?\", schema);\n</code></pre>"},{"location":"use-cases/text-to-sql/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> Preset-dependent HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizerProvider(TokenizerProvider)</code> <code>TokenizerProvider</code> <code>UnigramTokenizer</code> Tokenizer construction strategy <code>.promptFormatter(BiFunction)</code> <code>BiFunction&lt;String, String, String&gt;</code> Preset-dependent Combines (query, schema) into model prompt <code>.maxNewTokens(int)</code> <code>int</code> <code>256</code> Maximum tokens to generate <code>.temperature(float)</code> <code>float</code> <code>0.0</code> Sampling temperature <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(float)</code> <code>float</code> <code>0.0</code> (disabled) Nucleus sampling <code>.eosTokenId(int)</code> <code>int</code> Auto-detected End-of-sequence token ID"},{"location":"use-cases/text-to-sql/#result-type","title":"Result type","text":"<p><code>GenerationResult</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The generated SQL query <code>promptTokens()</code> <code>int</code> Number of tokens in the input <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time <p>The convenience method <code>generateSql(query, schema)</code> returns the SQL as a plain <code>String</code>.</p>"},{"location":"use-cases/text-to-sql/#tips","title":"Tips","text":"<ul> <li>Use greedy decoding (default <code>temperature=0</code>) for SQL generation \u2014 deterministic output is what you want.</li> <li>Always validate and sanitize generated SQL before executing it against a real database.</li> <li>Include all relevant tables in the schema, even if the query only touches one \u2014 the model uses the full schema to resolve column references.</li> <li>For the T5-large-spider model, the schema format can be generated programmatically from JDBC <code>DatabaseMetaData</code>.</li> </ul>"},{"location":"use-cases/translation/","title":"Machine Translation","text":"<p>Translate text between languages using MarianMT (fixed language pairs) or Flan-T5 (flexible, any-to-any).</p>"},{"location":"use-cases/translation/#quick-example","title":"Quick example","text":"MarianMT (fixed pair)Flan-T5 (flexible) <pre><code>try (var translator = MarianTranslator.builder()\n        .modelId(\"inference4j/opus-mt-en-fr\")\n        .build()) {\n    String french = translator.translate(\"The weather is beautiful today.\");\n    System.out.println(french); // Le temps est beau aujourd'hui.\n}\n</code></pre> <pre><code>try (var translator = FlanT5TextGenerator.flanT5Base().build()) {\n    String french = translator.translate(\"The weather is beautiful today.\",\n            Language.EN, Language.FR);\n    System.out.println(french);\n}\n</code></pre>"},{"location":"use-cases/translation/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.generation.GenerationResult;\nimport io.github.inference4j.nlp.MarianTranslator;\n\npublic class Translation {\n    public static void main(String[] args) {\n        try (var translator = MarianTranslator.builder()\n                .modelId(\"inference4j/opus-mt-en-de\")\n                .maxNewTokens(200)\n                .build()) {\n\n            GenerationResult result = translator.translate(\n                    \"Machine learning is transforming how we build software.\",\n                    token -&gt; System.out.print(token));\n\n            System.out.println();\n            System.out.printf(\"%d tokens in %,d ms%n\",\n                    result.generatedTokens(), result.duration().toMillis());\n        }\n    }\n}\n</code></pre>"},{"location":"use-cases/translation/#flexible-translation-with-flan-t5","title":"Flexible translation with Flan-T5","text":"<p><code>FlanT5TextGenerator</code> implements the <code>Translator</code> interface and can translate between any pair of languages using a single model:</p> <pre><code>import io.github.inference4j.nlp.FlanT5TextGenerator;\nimport io.github.inference4j.nlp.Language;\n\ntry (var translator = FlanT5TextGenerator.flanT5Base()\n        .maxNewTokens(200)\n        .build()) {\n\n    // English to French\n    String french = translator.translate(\"Hello, how are you?\",\n            Language.EN, Language.FR);\n\n    // English to German\n    String german = translator.translate(\"Hello, how are you?\",\n            Language.EN, Language.DE);\n\n    // French to Spanish\n    String spanish = translator.translate(\"Bonjour, comment allez-vous?\",\n            Language.FR, Language.ES);\n}\n</code></pre>"},{"location":"use-cases/translation/#supported-languages","title":"Supported languages","text":"<p>The <code>Language</code> enum provides constants for the most widely spoken languages. More languages will be added in future releases.</p> Constant Language <code>EN</code> English <code>FR</code> French <code>DE</code> German <code>ES</code> Spanish <code>PT</code> Portuguese <code>PT_BR</code> Brazilian Portuguese <code>IT</code> Italian <code>NL</code> Dutch <code>CA</code> Catalan <code>SV</code> Swedish <code>DA</code> Danish <code>NO</code> Norwegian <code>FI</code> Finnish <code>PL</code> Polish <code>CS</code> Czech <code>HR</code> Croatian <code>RO</code> Romanian <code>RU</code> Russian <code>UK</code> Ukrainian <code>TR</code> Turkish <code>JA</code> Japanese <code>KO</code> Korean <code>AR</code> Arabic <code>ZH_CN</code> Chinese Simplified <code>ZH_TW</code> Chinese Traditional <code>HI</code> Hindi <p>Each constant provides <code>displayName()</code> (e.g., <code>\"Brazilian Portuguese\"</code>) and <code>isoCode()</code> (e.g., <code>\"pt-br\"</code>).</p>"},{"location":"use-cases/translation/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> \u2014 (required for MarianMT) HuggingFace model ID (e.g., <code>inference4j/opus-mt-en-fr</code>) <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.tokenizerProvider(TokenizerProvider)</code> <code>TokenizerProvider</code> <code>SentencePieceBpeTokenizer</code> Tokenizer construction strategy <code>.maxNewTokens(int)</code> <code>int</code> <code>256</code> Maximum tokens to generate <code>.temperature(float)</code> <code>float</code> <code>0.0</code> Sampling temperature <code>.topK(int)</code> <code>int</code> <code>0</code> (disabled) Top-K sampling <code>.topP(float)</code> <code>float</code> <code>0.0</code> (disabled) Nucleus sampling <code>.eosTokenId(int)</code> <code>int</code> Auto-detected End-of-sequence token ID <code>.addedToken(String)</code> <code>String</code> \u2014 Register a special token for atomic encoding"},{"location":"use-cases/translation/#result-type","title":"Result type","text":"<p><code>GenerationResult</code> is a record with:</p> Field Type Description <code>text()</code> <code>String</code> The translated text <code>promptTokens()</code> <code>int</code> Number of tokens in the input <code>generatedTokens()</code> <code>int</code> Number of tokens generated <code>duration()</code> <code>Duration</code> Wall-clock generation time <p>The convenience method <code>translate(text)</code> returns the translation as a plain <code>String</code>.</p>"},{"location":"use-cases/translation/#using-your-own-marianmt-model","title":"Using your own MarianMT model","text":"<p>The pre-exported models under <code>inference4j/opus-mt-*</code> work out of the box. If you want to use a different MarianMT language pair (e.g., <code>Helsinki-NLP/opus-mt-en-ja</code>), you'll need to export it yourself.</p> <p><code>MarianTranslator</code> expects the model directory to contain:</p> File Description <code>encoder_model.onnx</code> Encoder ONNX model <code>decoder_model.onnx</code> Decoder ONNX model <code>decoder_with_past_model.onnx</code> Decoder with KV cache <code>config.json</code> Model configuration <code>tokenizer.json</code> HuggingFace fast tokenizer format <p>MarianMT models require tokenizer conversion</p> <p>MarianMT models on HuggingFace ship with SentencePiece files (<code>source.spm</code>, <code>target.spm</code>) instead of <code>tokenizer.json</code>. You must build <code>tokenizer.json</code> using the model's <code>vocab.json</code> for vocabulary IDs and <code>source.spm</code> for BPE merges.</p> <p>This is important because MarianMT merges source and target SentencePiece vocabularies into a shared <code>vocab.json</code> with ~65K entries. The raw <code>SentencePieceExtractor</code> produces SPM-internal IDs (0\u201331999) which differ from the model's actual IDs, so you must use <code>vocab.json</code> for the vocabulary mapping and only extract BPE merges from the SPM model.</p> <pre><code>import json\nfrom huggingface_hub import hf_hub_download\nfrom optimum.exporters.onnx import main_export\nfrom transformers.convert_slow_tokenizer import SentencePieceExtractor\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\n\nmodel_id = \"Helsinki-NLP/opus-mt-en-ja\"\n\n# 1. Export ONNX models\nmain_export(\n    model_name_or_path=model_id,\n    output=\"my-model/\",\n    task=\"text2text-generation-with-past\",\n)\n\n# 2. Build tokenizer.json from vocab.json + source.spm merges\nwith open(\"my-model/vocab.json\") as f:\n    model_vocab = json.load(f)\n\nextractor = SentencePieceExtractor(\"my-model/source.spm\")\n_, merges = extractor.extract(None)\n\ntokenizer = Tokenizer(BPE(model_vocab, merges, unk_token=\"&lt;unk&gt;\"))\ntokenizer.save(\"my-model/tokenizer.json\")\n</code></pre> <p>Only standard <code>opus-mt-*</code> models are supported. The newer <code>opus-mt-tc-big-*</code> variants require target language prefixes (e.g., <code>&gt;&gt;por&lt;&lt;</code>) which <code>MarianTranslator</code> does not handle.</p>"},{"location":"use-cases/translation/#tips","title":"Tips","text":"<ul> <li>MarianMT models are specialized for a single language pair (e.g., <code>opus-mt-en-fr</code> for English\u2192French). They produce higher quality translations for their specific pair but require a separate model per direction.</li> <li>Flan-T5 handles any language pair with a single model, making it more flexible but generally lower quality than a dedicated pair-specific model.</li> <li>For bidirectional translation, you need two MarianMT models (e.g., <code>opus-mt-en-fr</code> and <code>opus-mt-fr-en</code>) \u2014 or use Flan-T5 which handles both directions.</li> <li>Use greedy decoding (default <code>temperature=0</code>) for translation \u2014 sampling adds noise without improving quality.</li> </ul>"},{"location":"use-cases/visual-search/","title":"Visual Search","text":"<p>Classify images using arbitrary text labels \u2014 no training required \u2014 powered by CLIP.</p>"},{"location":"use-cases/visual-search/#quick-start","title":"Quick start","text":"<pre><code>try (ClipClassifier classifier = ClipClassifier.builder().build()) {\n    List&lt;Classification&gt; results = classifier.classify(\n            Path.of(\"photo.jpg\"), List.of(\"cat\", \"dog\", \"bird\", \"car\", \"airplane\"));\n    System.out.println(results.get(0).label());      // \"cat\"\n    System.out.println(results.get(0).confidence());  // 0.92\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/visual-search/#zero-shot-classification","title":"Zero-shot classification","text":"<p>Unlike traditional image classifiers that are trained on a fixed set of labels, CLIP classifies images against any labels you provide at each call. Just pass the labels you need \u2014 no retraining, no fine-tuning, and no need to rebuild the classifier for different label sets:</p> <pre><code>try (ClipClassifier classifier = ClipClassifier.builder().build()) {\n    // Emotion detection\n    classifier.classify(image, List.of(\n            \"a photo of a happy person\", \"a photo of a sad person\",\n            \"a photo of an angry person\", \"a photo of a surprised person\"));\n\n    // Product categorization\n    classifier.classify(image, List.of(\n            \"a product photo of electronics\", \"a product photo of clothing\",\n            \"a product photo of furniture\", \"a product photo of food\"));\n\n    // Scene classification\n    classifier.classify(image, List.of(\n            \"a landscape photo of a beach\", \"a landscape photo of a mountain\",\n            \"a landscape photo of a city\", \"a landscape photo of a forest\"));\n}\n</code></pre>"},{"location":"use-cases/visual-search/#how-it-works","title":"How it works","text":"<p>CLIP uses two separate encoders \u2014 one for images, one for text \u2014 trained so that matching image-text pairs produce similar embeddings. <code>ClipClassifier</code> wraps both encoders:</p> <pre><code>flowchart TD\n    Image[\"Image\"]\n    Labels[\"Candidate labels&lt;br&gt;&lt;i&gt;'a photo of a cat', 'a photo of a dog', ...&lt;/i&gt;\"]\n\n    Image --&gt; IE[\"ClipImageEncoder\"]\n    Labels --&gt; TE[\"ClipTextEncoder\"]\n\n    IE --&gt; IEmb[\"Image embedding&lt;br&gt;[512-dim]\"]\n    TE --&gt; LEmb[\"Label embeddings&lt;br&gt;[512-dim] x N\"]\n\n    IEmb --&gt; Sim[\"Dot-product similarity\"]\n    LEmb --&gt; Sim\n\n    Sim --&gt; SM[\"Softmax\"]\n    SM --&gt; Result[\"List&amp;lt;Classification&amp;gt;\"]</code></pre>"},{"location":"use-cases/visual-search/#builder-options","title":"Builder options","text":"Option Type Default Description <code>modelId(String)</code> <code>String</code> <code>inference4j/clip-vit-base-patch32</code> HuggingFace model ID <code>modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Where to load the model from <code>sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> Default (CPU) ONNX Runtime session options"},{"location":"use-cases/visual-search/#prompt-tips","title":"Prompt tips","text":"<p>CLIP was trained on natural language captions, so passing full prompt text as labels produces better results than bare nouns. The label text is passed directly to the text encoder \u2014 format it however works best for your use case:</p> Use case Label examples Why General objects <code>\"a photo of a cat\"</code>, <code>\"a photo of a dog\"</code> Matches CLIP training data format Fine-grained <code>\"a photo of a tabby cat, a type of pet\"</code> Adds context for disambiguation Scenes <code>\"a beach landscape\"</code>, <code>\"a mountain landscape\"</code> Descriptive captions Actions <code>\"a photo of a person running\"</code> Activity descriptions Styles <code>\"an impressionist style painting\"</code> Art style descriptions"},{"location":"use-cases/visual-search/#api-methods","title":"API methods","text":"<pre><code>// Primary API \u2014 classify with candidate labels\nList&lt;Classification&gt; classify(BufferedImage image, List&lt;String&gt; candidateLabels);\nList&lt;Classification&gt; classify(BufferedImage image, List&lt;String&gt; candidateLabels, int topK);\n\n// Path convenience overloads\nList&lt;Classification&gt; classify(Path imagePath, List&lt;String&gt; candidateLabels);\nList&lt;Classification&gt; classify(Path imagePath, List&lt;String&gt; candidateLabels, int topK);\n\n// InferenceTask compatibility\nList&lt;Classification&gt; run(ZeroShotInput&lt;BufferedImage&gt; input);\n</code></pre>"},{"location":"use-cases/visual-search/#advanced-direct-encoder-access","title":"Advanced: direct encoder access","text":"<p>For use cases beyond classification \u2014 image search, image-text similarity, or custom pipelines \u2014 use <code>ClipImageEncoder</code> and <code>ClipTextEncoder</code> directly. See the CLIP Encoders reference.</p>"},{"location":"use-cases/visual-search/#alternative-models","title":"Alternative models","text":"<p>The default model is <code>inference4j/clip-vit-base-patch32</code> (ViT-B/32) \u2014 the smallest and fastest variant. You can use other CLIP-compatible models by exporting them to ONNX with the same input/output layout and pointing to them via <code>.modelId()</code> or <code>.modelSource()</code>.</p> <p>Possible variants (not yet tested with inference4j):</p> Model Source Embedding dim Notes <code>openai/clip-vit-base-patch16</code> OpenAI 512 16\u00d716 patches \u2014 better quality, ~2\u00d7 slower <code>openai/clip-vit-large-patch14</code> OpenAI 768 Best quality from OpenAI, significantly larger <code>laion/CLIP-ViT-B-32-laion2B-s34B-b79K</code> OpenCLIP 512 Trained on LAION-2B, often outperforms OpenAI's original <code>laion/CLIP-ViT-L-14-laion2B-s32B-b82K</code> OpenCLIP 768 Large variant trained on LAION-2B <code>google/siglip-base-patch16-224</code> Google 768 SigLIP \u2014 improved training objective, strong zero-shot performance <p>Note</p> <p>Models with different embedding dimensions (e.g., 768 instead of 512) will work \u2014 the wrappers don't assume a fixed size. However, you must use the same model for both image and text encoding since embeddings are only comparable within the same model's vector space.</p>"},{"location":"use-cases/visual-search/#model-details","title":"Model details","text":"Property Value Architecture ViT-B/32 (vision) + Transformer (text) Embedding dimensions 512 Image input 224\u00d7224 RGB, CLIP-normalized Text input BPE tokenized, max 77 tokens Default model <code>inference4j/clip-vit-base-patch32</code> Model size ~340 MB (vision) + ~255 MB (text)"},{"location":"use-cases/voice-activity-detection/","title":"Voice Activity Detection","text":"<p>Detect speech segments in audio using Silero VAD. Returns timestamped segments with confidence scores \u2014 useful for preprocessing audio before transcription, or for detecting when someone is speaking.</p>"},{"location":"use-cases/voice-activity-detection/#quick-example","title":"Quick example","text":"<pre><code>try (var vad = SileroVadDetector.builder().build()) {\n    List&lt;VoiceSegment&gt; segments = vad.detect(Path.of(\"meeting.wav\"));\n    // [VoiceSegment[start=0.50, end=3.20], VoiceSegment[start=5.10, end=8.75]]\n}\n</code></pre>"},{"location":"use-cases/voice-activity-detection/#full-example","title":"Full example","text":"<pre><code>import io.github.inference4j.audio.SileroVadDetector;\nimport io.github.inference4j.audio.VoiceSegment;\nimport java.nio.file.Path;\n\npublic class VoiceActivityDetection {\n    public static void main(String[] args) {\n        try (var vad = SileroVadDetector.builder()\n                .threshold(0.7f)\n                .minSpeechDuration(0.3f)\n                .minSilenceDuration(0.15f)\n                .build()) {\n\n            List&lt;VoiceSegment&gt; segments = vad.detect(Path.of(\"meeting.wav\"));\n\n            for (VoiceSegment seg : segments) {\n                System.out.printf(\"%.2fs - %.2fs (duration: %.2fs, confidence: %.2f)%n\",\n                    seg.start(), seg.end(), seg.duration(), seg.confidence());\n            }\n        }\n    }\n}\n</code></pre> Screenshot from showcase app"},{"location":"use-cases/voice-activity-detection/#per-frame-probabilities","title":"Per-frame probabilities","text":"<p>For visualization or custom segmentation logic, extract raw speech probabilities:</p> <pre><code>try (var vad = SileroVadDetector.builder().build()) {\n    float[] probabilities = vad.probabilities(Path.of(\"audio.wav\"));\n    // One probability per frame (32ms window at 16kHz)\n}\n</code></pre>"},{"location":"use-cases/voice-activity-detection/#from-raw-audio-data","title":"From raw audio data","text":"<pre><code>try (var vad = SileroVadDetector.builder().build()) {\n    float[] audioData = loadAudioSamples();\n    List&lt;VoiceSegment&gt; segments = vad.detect(audioData, 16000);\n}\n</code></pre>"},{"location":"use-cases/voice-activity-detection/#builder-options","title":"Builder options","text":"Method Type Default Description <code>.modelId(String)</code> <code>String</code> <code>inference4j/silero-vad</code> HuggingFace model ID <code>.modelSource(ModelSource)</code> <code>ModelSource</code> <code>HuggingFaceModelSource</code> Model resolution strategy <code>.sessionOptions(SessionConfigurer)</code> <code>SessionConfigurer</code> default ONNX Runtime session config <code>.sampleRate(int)</code> <code>int</code> <code>16000</code> Target sample rate (Hz) <code>.windowSizeSamples(int)</code> <code>int</code> <code>512</code> Frame window size (512 = 32ms at 16kHz) <code>.threshold(float)</code> <code>float</code> <code>0.5</code> Speech probability threshold <code>.minSpeechDuration(float)</code> <code>float</code> <code>0.25</code> Minimum speech segment duration (seconds) <code>.minSilenceDuration(float)</code> <code>float</code> <code>0.1</code> Minimum silence gap between segments (seconds)"},{"location":"use-cases/voice-activity-detection/#result-type","title":"Result type","text":"<p><code>VoiceSegment</code> is a record with:</p> Field Type Description <code>start()</code> <code>float</code> Segment start time in seconds <code>end()</code> <code>float</code> Segment end time in seconds <code>duration()</code> <code>float</code> Segment duration in seconds <code>confidence()</code> <code>float</code> Average speech probability for the segment"},{"location":"use-cases/voice-activity-detection/#tuning-thresholds","title":"Tuning thresholds","text":"<p>The default thresholds work well for clean speech. Adjust for your use case:</p> Scenario <code>threshold</code> <code>minSpeechDuration</code> <code>minSilenceDuration</code> Clean speech (default) <code>0.5</code> <code>0.25</code> <code>0.1</code> Noisy environment <code>0.7</code> <code>0.3</code> <code>0.15</code> Short utterances (commands) <code>0.5</code> <code>0.1</code> <code>0.05</code> Long-form speech (podcasts) <code>0.5</code> <code>0.5</code> <code>0.3</code>"},{"location":"use-cases/voice-activity-detection/#combining-with-speech-to-text","title":"Combining with speech-to-text","text":"<p>Use VAD to segment audio before transcription for better accuracy:</p> <pre><code>try (var vad = SileroVadDetector.builder().build();\n     var recognizer = Wav2Vec2Recognizer.builder().build()) {\n\n    List&lt;VoiceSegment&gt; segments = vad.detect(Path.of(\"meeting.wav\"));\n\n    for (VoiceSegment segment : segments) {\n        // Extract segment audio and transcribe\n        System.out.printf(\"[%.1fs-%.1fs] %s%n\",\n            segment.start(), segment.end(), \"...\");\n    }\n}\n</code></pre>"},{"location":"use-cases/voice-activity-detection/#tips","title":"Tips","text":"<ul> <li>Silero VAD is a stateful model \u2014 it maintains hidden state across frames for context. This is handled internally.</li> <li>The model supports both 16kHz and 8kHz sample rates.</li> <li>Use <code>probabilities()</code> to inspect per-frame speech likelihood for debugging or visualization.</li> <li>For real-time applications, the 32ms window size (512 samples at 16kHz) provides low-latency detection.</li> </ul>"}]}